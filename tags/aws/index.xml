<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on vim, git, aws and other three-letter words</title>
    <link>https://serebrov.github.io/tags/aws/</link>
    <description>Recent content in aws on vim, git, aws and other three-letter words</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://serebrov.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Config - Unexpected Charges and Data Analysis</title>
      <link>https://serebrov.github.io/html/2019-10-08-aws-config-charges.html</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2019-10-08-aws-config-charges.html</guid>
      <description>I started seeing an increased charge in billing for AWS Config service in one of the accounts, it increased from around $5 to $100 per month. And I didn&amp;rsquo;t even remember if I enabled and configured it.
I could not get any details from AWS Cost Explorer besides that charges are in the same region where our app is running.
The confusing part was a note in the AWS Config management console:</description>
    </item>
    
    <item>
      <title>CloudFront Setup to Route Requests to Services Based on Request Path</title>
      <link>https://serebrov.github.io/html/2019-06-16-multi-origin-cloudfront-setup.html</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2019-06-16-multi-origin-cloudfront-setup.html</guid>
      <description>AWS CloudFront allows to have multiple origins for the distribution and, along with lambda@edge functions, that makes it possible to use CloudFront as an entry point to route the requests to different services based on the request path.
For example:
 www.myapp.com -&amp;gt; unbounce.com (landing pages) www.myapp.com/app -&amp;gt; single page app hosted on S3 www.myapp.com/blog -&amp;gt; wordpress blog  CloudFront Setup Structure CloudFront distribution can have one or more origins (sources to serve the data from) and one or more behaviors (rules defining how to cache the data based on the request path).</description>
    </item>
    
    <item>
      <title>SSH Tunnels (How to Access AWS RDS Locally Without Exposing it to Internet)</title>
      <link>https://serebrov.github.io/html/ssh-tunnels-how-to-access-aws-rds-locally-without-exposing-it-to-internet.html/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/ssh-tunnels-how-to-access-aws-rds-locally-without-exposing-it-to-internet.html/</guid>
      <description>Using SSH tunnels, it is possible to access remote resources that are not exposed to the Internet through the intermediate hosts or expose your local services to the Internet.
Setup To make SSH commands shorter and easier to use, edit the ~/.ssh/config and add the configuration for the hosts you are going to connect.
The configuration defines default ssh options, so instead of the command like this ssh ec2-user@ec2-55-222-55-55.compute-1.amazonaws.com -i ~/.</description>
    </item>
    
    <item>
      <title>AWS error - Default subnet in us-east-1f not found</title>
      <link>https://serebrov.github.io/html/2017-06-28-aws-default-subnet-in-us-east-1f-not-found.html</link>
      <pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2017-06-28-aws-default-subnet-in-us-east-1f-not-found.html</guid>
      <description>I suddenly started getting the Default subnet in us-east-1f not found error during the ElasticBeanstalk environment update.
Failed to deploy application. Updating load balancer named: awseb-e-t-AWSEBLoa-XXXXXXXXXXXXX failed Reason: Default subnet not found in us-east-1f Service:AmazonCloudFormation, Message:Stack named &amp;#39;awseb-e-xxxxxxxxxx-stack&amp;#39; aborted operation. Current state: &amp;#39;UPDATE_ROLLBACK_IN_PROGRESS&amp;#39; Reason: The following resource(s) failed to create: [AWSEBUpdateWaitConditionHandleralanC]. The following resource(s) failed to update: [AWSEBLoadBalancer]. And the similar one when trying to create the new environment:
Creating load balancer failed Reason: Default subnet in us-east-1f not found Created CloudWatch alarm named: awseb-e-tet63me2mx-stack-AWSEBCWLAllErrorsCountAlarm-3XCPMJ1ZGJ18 Stack named &amp;#39;awseb-e-tet63me2mx-stack&amp;#39; aborted operation.</description>
    </item>
    
    <item>
      <title>AWS PostgreSQL RDS - remaining connection slots are reserved error</title>
      <link>https://serebrov.github.io/html/2015-09-22-aws-postgresql-max-connections.html</link>
      <pubDate>Tue, 22 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-09-22-aws-postgresql-max-connections.html</guid>
      <description>Today I had a problem with PostgreSQL connection, both my application and psql tool returned an error:
FATAL: remaining connection slots are reserved for non-replication superuser connections The PostgreSQL server was running on the db.t1.micro RDS instance and the &amp;lsquo;Current activity&amp;rsquo; column showed &amp;lsquo;22 connections&amp;rsquo; and a red line which should represent a connection limit was far away from the 22 value.
Here is how it looked:
.
And this connection information is actually misleading - it shows 22 connections and it looks like around 30% consumed.</description>
    </item>
    
    <item>
      <title>How to set up Drone CI on EC2 instance via Elastic Beanstalk</title>
      <link>https://serebrov.github.io/html/2015-07-05-elastic-beanstalk-drone-ci-setup.html</link>
      <pubDate>Sun, 05 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-07-05-elastic-beanstalk-drone-ci-setup.html</guid>
      <description>Drone CI is a Continuous Integration platform. It uses Docker containers to run tests for your application hosted on github.
It not complex to set up the automatic testing for your application and run Drone CI on EC2 instance using Elastic Beanstalk. It is even not necessary to have a dedicated EC2 instance for CI system, for example, I run it on the staging server.
Drone CI setup First you&amp;rsquo;ll need to create a drone configuration file, .</description>
    </item>
    
    <item>
      <title>CloudWatch Logs - how to log data from multiple instances to the single stream</title>
      <link>https://serebrov.github.io/html/2015-06-17-cloudwatch-logs-single-stream.html</link>
      <pubDate>Wed, 20 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-06-17-cloudwatch-logs-single-stream.html</guid>
      <description>After using CloudWatch Logs for some time I found that it is very inconvenient to have one stream per instance. The Logs UI is really complex to use - I need to remember instance names, open the log group I need and then go into each instance logs one-by-one to check them.
A more convenient alternative is to use one stream like error_log for all instances.
Update: logging to the same stream from multiple sources is not recommended and may cause duplicate records (although in my case this is fine).</description>
    </item>
    
    <item>
      <title>Elastic Beanstalk - how to setup CloudWatch Logs</title>
      <link>https://serebrov.github.io/html/2015-05-20-cloudwatch-setup.html</link>
      <pubDate>Wed, 20 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-05-20-cloudwatch-setup.html</guid>
      <description>CloudWatch Logs is an AWS service to collect and monitor system and application logs. On the top level setup is this:
 install CloudWatch agent to collect logs data and send to CloudWatch Logs service define log metric filters to extract useful data, like number of all errors or information about some specific events create alarms for metrics to get notifications about logs make sure that the instance role has permissions to push logs to CloudWatch (see comments for details about this issue)  All the configuration can be done using the Elastic Beanstalk config.</description>
    </item>
    
    <item>
      <title>Elastic Beanstalk - python application server structure and celery installation</title>
      <link>https://serebrov.github.io/html/2015-04-02-elastic-beanstalk-python.html</link>
      <pubDate>Thu, 02 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-04-02-elastic-beanstalk-python.html</guid>
      <description>Elastic beanstalk python application is deployed under /opt/python/. The application is running under Apache web server.
Source folder structure is this:
bin httpdlaunch - a tool script to set environment variables and launch httpd bundle - dir with app source code, used during updates current - symlink to the recent source code version under bundle app - application sources env - shell script with environment variables (passed from EB environment settings) etc - supervisord config log - supervisord logs run - virtual environments Apache logs, deployment logs and system messages log are under /var/log.</description>
    </item>
    
    <item>
      <title>Amazon DynamoDB - how to add global secondary index</title>
      <link>https://serebrov.github.io/html/2015-01-25-aws-add-secondary-index.html</link>
      <pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-01-25-aws-add-secondary-index.html</guid>
      <description>Note: this post is outdated, because it is already possible to add a secondary index to the existing table (it was not possible in earlier DynamoDB versions).
At the moment it is not possible to add a secondary index into the existing table. This feature is announced but not yet available.
So the only way is to create a new table and migrate the existing data to it. This can be done using Amazon EMR.</description>
    </item>
    
    <item>
      <title>Local Amazon DynamoDB - tools, dump/restore and testing</title>
      <link>https://serebrov.github.io/html/2015-02-01-dynamodb-local.html</link>
      <pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-02-01-dynamodb-local.html</guid>
      <description>Setup Download and extract dynamodb local to some folder.
Launch it (-sharedDb allows us to connect to the same database with other tools):
$ java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb By default it will be running on the port 8000 and will create the db file in the same directory where it was launched.
Without the -sharedDB parameter the DB file name depends on connection parameters, the name is {aws_access_key_id}_{region_name}.db. So different clients can use different databases.</description>
    </item>
    
    <item>
      <title>Amazon DynamoDB, EMR and Hive notes</title>
      <link>https://serebrov.github.io/html/2015-01-24-aws-dynamodb-emr-hive.html</link>
      <pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2015-01-24-aws-dynamodb-emr-hive.html</guid>
      <description>First you need the EMR cluster running and you should have ssh connection to the master instance like described in the getting started tutorial.
Now it is possible to run Hive commands in few following ways:
 Connect via ssh, launch hive and run commands interactively Create a script file with commands, upload it to S3 and launch as a ERM &amp;lsquo;Hive program&amp;rsquo; step Run it from Hue web-interface (see below)  Connect to Hue Hue is a Hadoop web interface.</description>
    </item>
    
    <item>
      <title>AWS - Deployment via OpsWorks from the command line</title>
      <link>https://serebrov.github.io/html/2014-12-30-aws-opsworks-cmd-deployment.html</link>
      <pubDate>Tue, 30 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2014-12-30-aws-opsworks-cmd-deployment.html</guid>
      <description>Below is a simple python script which performs application deployment using OpsWorks API library (boto). Script performs following steps
 Execute &amp;lsquo;update_custom_cookbooks&amp;rsquo; deployment command and wait for successful completion (or stop with an error) Execute &amp;lsquo;deploy&amp;rsquo; command and wait for completion  At the top there are aws configuration parameters (aws_access_key, aws_secret_key) - these can be left empty if the script is launched on the AWS instance which has IAM role assigned.</description>
    </item>
    
    <item>
      <title>AWS OpsWorks - setup mongodb ebs volume backups</title>
      <link>https://serebrov.github.io/html/2014-12-30-aws-ebs-mongo-backups.html</link>
      <pubDate>Tue, 30 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2014-12-30-aws-ebs-mongo-backups.html</guid>
      <description>I described how to setup mongodb on EC2 using OpsWroks and here is how to setup mongo data backups.
In my case all mongo data is stored on the same EBS volume so I just need to make a volume snapshot.
The relevant part from the mongodb docs:
 Backup with --journal The journal file allows for roll forward recovery. The journal files are located in the dbpath directory so will be snapshotted at the same time as the database files.</description>
    </item>
    
    <item>
      <title>Amazon OpsWorks - node.js app with MongoDB setup</title>
      <link>https://serebrov.github.io/html/2014-12-19-aws-opsworks-mongo-and-nodejs.html</link>
      <pubDate>Fri, 19 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2014-12-19-aws-opsworks-mongo-and-nodejs.html</guid>
      <description>Amazon OpsWorks provides a way to manage AWS resources using Chef recipes.
Here I describe a simple setup of the single-instance node.js app with single-node MongoDB server. It is similar to the php application + mysql setup described in the OpsWorks Getting Started guide.
The OpsWorks setup includes:
 Stack - a container for the deployment process we will setup Two Layers - node.js app and MongoDB Two Instances - one EC2 instance for node app and another for mongo One Application - this is the code we will deploy  MongoDb setup is based on this blog post.</description>
    </item>
    
    <item>
      <title>Elastic Beanstalk - cron command and RDS DB access</title>
      <link>https://serebrov.github.io/html/2013-10-22-elastic-beanstalk-cron-and-db.html</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2013-10-22-elastic-beanstalk-cron-and-db.html</guid>
      <description>Problem I have a console command in php which needs an access to DB. The command need to be launched via cron.
The DB connection string looks like like this
&#39;connectionString&#39; =&amp;gt; &#39;mysql:host=&#39;.$_SERVER[&#39;RDS_HOSTNAME&#39;].&#39;;port=&#39;.$_SERVER[&#39;RDS_PORT&#39;].&#39;;dbname=&#39;.$_SERVER[&#39;RDS_DB_NAME&#39;],  where RDS_xxx parameters come from environment variables.
The problem is that cron launches the command with a clean environment (there are no RDS_xx variables). So the command fails to access the database.
Solution Solution is to set the required environment variables before launching the command and this can be done with &amp;lsquo;/opt/elasticbeanstal/support/envvars&amp;rsquo; script:</description>
    </item>
    
    <item>
      <title>Elastic Beanstalk - deploy from different machines / by different users (or how to get rid of absolute paths in configs)</title>
      <link>https://serebrov.github.io/html/2013-09-11-elastic-beanstalk-configs.html</link>
      <pubDate>Wed, 11 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2013-09-11-elastic-beanstalk-configs.html</guid>
      <description>By default Elastic Beanstalk console tool (eb) adds config files to .gitignore. If there are manual changes to EB configs it can be complex to manually sync these changes between different machines / different users. Of cause it is possible to add config files to git repository but there are also several parameters in the main config which are absolute paths to local files. This way it makes configs not useful for other users (except for the case when different users have exactly the same files layout).</description>
    </item>
    
    <item>
      <title>Amazon NoSQL Solutions</title>
      <link>https://serebrov.github.io/html/2013-06-11-amazon-nosql-review.html</link>
      <pubDate>Tue, 11 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://serebrov.github.io/html/2013-06-11-amazon-nosql-review.html</guid>
      <description>Amazon provides following NoSQL storage options:
 [SimpleDB] (http://aws.amazon.com/simpledb/) - Amazon SimpleDB is a highly available and flexible non-relational data store that offloads the work of database administration. Developers simply store and query data items via web services requests and Amazon SimpleDB does the rest. [DynamoDB] (http://aws.amazon.com/dynamodb/) - Amazon DynamoDB is a fully-managed, high performance, NoSQL database service that is easy to set up, operate, and scale.  Amazon&amp;rsquo;s review of big data solutions Amazon&amp;rsquo;s review Big Data on AWS mentions only DynamoDB and Elastic Map Reduce (based on Hadoop) as tools for big data management.</description>
    </item>
    
  </channel>
</rss>