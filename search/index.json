[{"content":"","href":"/","title":"Home"},{"content":"","href":"/posts/","title":"Posts"},{"content":"","href":"/archive/","title":"Archive"},{"content":"","href":"/tags/cmd/","title":"cmd"},{"content":"","href":"/tags/git/","title":"git"},{"content":"To use kdiff3 as your diff tool and merge tool in git, run the following commands:\ngit config --global mergetool.kdiff3.cmd \u0026#39;kdiff3 \u0026#34;$BASE\u0026#34; \u0026#34;$LOCAL\u0026#34; \u0026#34;$REMOTE\u0026#34; -o \u0026#34;$MERGED\u0026#34;\u0026#39; git config --global merge.tool kdiff3 git config --global difftool.kdiff3.cmd \u0026#39;kdiff3 \u0026#34;$LOCAL\u0026#34; \u0026#34;$REMOTE\u0026#34;\u0026#39; git config --global diff.tool kdiff3 Alternatively, edit the ~/.gitconfig and add settings there:\n[mergetool \u0026#34;kdiff3\u0026#34;] cmd = kdiff3 $LOCAL $REMOTE $BASE -o $MERGED [diff \u0026#34;kdiff3\u0026#34;] cmd = kdiff3 $LOCAL $REMOTE [merge] tool = kdiff3 [diff] tool = kdiff3 ","href":"/html/2023-06-11-git-use-kdiff3-as-difftool.html","title":"git - use kdiff3 as a diff/merge tool"},{"content":"There are several tools that can be used to format the large json file.\nPrettier (if you have node.js and npx installed):\nnpx run prettier input_json.json \u0026gt; formatted_json.json With python:\ncat input_json.json | python -m json.tool \u0026gt; formatted_json.json # json.tool uses 4 spaces as indent by default, we can change it: cat input_json.json | python -m json.tool --indent 2 \u0026gt; formatted_json.json With jq:\njq \u0026#39;.\u0026#39; input_json.json \u0026gt; formatted_json.json Reminder: to open a large file in vim / nvim it is better to run it with -u NONE: nvim -u NONE formatted_json.json.\n","href":"/html/2023-06-11-format-json-command-line.html","title":"How to format large JSON file in command line"},{"content":"The extension storage is not displayed under \u0026ldquo;Application\u0026rdquo; tab in Chrome DevTools, but it is possible to access the extension storage using javascript console, here is how:\nHow to inspect extension chrome.storage in Chrome DevTools\nit is possible to access it via javascript console:\nOpen some web page, open Chrome DevTools In the javascript console, select the extension context (the drop down with \u0026ldquo;top\u0026rdquo; in it) Use chrome.storage.local to access the localstorage The chrome.storage.local is a StorageArea object and it has methods to get and set values.\nWe can also print the entire storage by passing null to get method with chrome.storage.local.get(null).then((data) =\u0026gt; console.log(data)).\nThe demo:\n","href":"/html/2023-06-13-chrome-extension-inspect-storage.html","title":"How to inspect extension `chrome.storage` in Chrome DevTools"},{"content":"","href":"/tags/json/","title":"json"},{"content":"","href":"/tags/","title":"Tags"},{"content":"To cherry pick a range of commits to another branch, we can use the START^..END commit range syntax, where START is the first commit in the range and END is the last commit:\ngit cherry-pick START^..END References How to cherry-pick a range of commits and merge them into another branch?\n","href":"/html/2021-09-13-git-cherry-pick-a-range-of-commits.html","title":"git - cherry-pick a range of commits"},{"content":"Git allows joining unrelated repositories via remotes which, in turn, allows moving files and change history between them.\nSome cases when this might be needed:\nExtract part of a big repository into a separate repository, preserving change history Splitting big repository to a set of smaller repositories Merge smaller repository into a bigger one (merge in library from the external repository) Preserving the history has an important effect: after we do the change, for example, extract a part of a bigger repository into a separate repository, we can continue moving changes between them (merge updates from the big repository to the small one and back).\nBelow, I am assuming the first case (extract part of the big repository), other cases can be implemented with similar technique.\nSetup Assuming we have a source (upstream) repository and we want to extract a top-level /lib folder into a separate repository, first, create a destination repository:\nmkdir dest-repo cd dest-repo git init Add a main repository as remote and checkout master branch:\ngit remote add -f upstream git@github.com:myaccount/main.git git checkout -b upstream_master upstream/master git subtree split --prefix=lib/ -b upstream_lib What we do above is: add the source (upstream) repository as a remote, checkout its master branch to the destination repository and then use git subtree split to create upstream_master_lib branch containing only /lib folder with all the change history.\nNow we can merge the upstream files and history from the /lib folder to the target master branch:\ngit checkout master git merge upstream_lib git push origin HEAD Update from upstream Pull recent upstream repository master branch and re-split it to a new branch:\ngit checkout upstream_master git pull git subtree split --prefix=lib/ -b upstream_lib_YYYY.MM.DD Merge the update:\ngit checkout master git merge upstream_lib_YYYY.MM.DD git push origin HEAD ","href":"/html/2021-09-13-git-move-history-to-another-repository.html","title":"git - how to move files with history to another repository"},{"content":"The shortlog -ns will show number of commits by author:\ngit shortlog -ns 280 Author One 46 Author Two 25 authorthree 14 au 4 x 3 Autor One 1 ide user x ","href":"/html/2021-09-13-git-number-of-commits-by-author.html","title":"git - show number of commits by author"},{"content":"Simple: Change Last Commit Message To change the last commit message, use commit with --amend flag:\nCareful: `commit --amend` will rewirte history, do not use on public branches. $ git commit --amend It will open an editor and change the commit message, changes will be applied after saving the file and closing the editor.\nAdvanced: Change Any Commit Message or Multiple Commit Messages Besides other things, interactive rebase allows editing commit messages:\ngit rebase -i HEAD~3 pick SHA-A commit message pick SHA-B commit message pick SHA-C commit message Do not change commit messages just yet, only change pick in the first column to r (or full reword):\npick SHA-A commit message r SHA-B commit message r SHA-C commit message Here we schedule commit message update for SHA-B and SHA-C commits. After the file is saved, git will open the editor once more for each of the marked commits where we can edit the commit message.\nAlternative: Cherry Pick Commits and Update Messages This might be useful if we want to pick some commits from the branch and update commit messages:\n# create the new branch git checkout -b new-branch # for every commit to pick git cherry-pick commit-sha git commit --amend So we cherry pick needed commits and then edit commit messages with commit --amend.\n","href":"/html/2021-09-13-git-update-commit-message.html","title":"git - update commit message"},{"content":"","href":"/tags/kbd/","title":"kbd"},{"content":"Mitosis Keyboard First Impressions\nI\u0026rsquo;ve received my Mitosis a few days ago and I like it a lot so far:\nThe size: perfect for the 36 keys layout I use The shape: very comfortable, lowered outer columns feel great for pinkies I wished the inner column, for the index finger would not be shifted up as it is, it causes a bit of extension to use Y and T (not a big deal though) I also think the bottom thumb row is too far, so I am using the top row, having the bottom one without keycaps, so it doesn\u0026rsquo;t get in a way Wireless: probably this was one of main reasons to get it, it is really nice to freely move both halves without having to care about wires An interesting decision here is to use the radio channel instead of bluetooth which gives much longer battery lifetime It requires a receiver to be connected to the computer, but that is not a big deal, also it is cool to realize that your keyboard (QMK) is actually running inside that small board Layout: not sure which layout was there initially, it was QWERTY while the default layout in QMK configuratior is Maltron (I think), anyway, I changed it to my layout, similar to what I have on Moonlander Open source: here is the announcement post, hardware, receiver firmware and keyboard firmware, QMK How it looks:\nand the receiver:\nGetting it to work didn\u0026rsquo;t take much time, although I first had an issue with the cable (the first one I tried didn\u0026rsquo;t work, probably it was just not a data cable) and then I had batteries upside down and one of them didn\u0026rsquo;t hold in place.\nI wish there was some quick start guide, something like this:\nUnscrew PCBs from the case (inner screws) Put batteries into the left and right halves, they go between top and bottom plates, right bottom corner of the right half and left bottom corner of the left half Note that battery position is different in left and right halves Don\u0026rsquo;t screw the PCB back yet, first make sure it works Find a micro-USB to USB-A/USB-C cable, it should be the data cable (so some cables may not work) Connect the receiver to a computer Try pressing keys on both halves, it should already work Screw the PCB back into the case Put on keycaps (it goes without keycaps) If it doesn\u0026rsquo;t work:\nMake sure to use a data cable for the receiver Make sure the batteries are inserted with the correct polarity There are + and - markings although I had to put my batteries upside down (+ battery side facing - marking), not sure if markings are misplaced or I am interpreting them incorrectly Flashing:\nGo to the QMK configuratior, change the layout as you like Download json keymap (at least, for the backup, it can be loaded back later) Compile and download the firmware (there are buttons in the configurator) and then flash it with the QMK Toolbox Press the button with R near it on the receiver to reset it and go to the flashing mode Alternatively, clone qmk and flash from the command line. Setup guide is here, after everything is set up, new firmware can be compiled and flushed with qmk flash -kb mitosis -km mitosis_mini (where mitosis_mini is the custom keymap name, there are also several alternative keymaps in the QMK source).\nThe keymap itself, can remain in json format, here is what I have so far:\nconfig.h - defines to auto shift timeout and enables tapping force hold custom.c - led colors for layers (although, I have a problem with green light) keymap.json - the keymap made with QMK configurator rules.mk - enables auto shift Compared to the ZSA configurator, the QMK Configurator provides less features, so I had to switch to the source code to enable auto shift.\nI still have a few questions, maybe someone could answer these for me:\nThe battery in the right side didn\u0026rsquo;t hold in the right half, I put aluminum foil under it to keep it in place. Seems to work good now, but is it safe to do and to go like this in the long term? Maybe there is a better way to fix this?\nHere is how the battery with foil looks.\nThe are + / - markers on the pcb, but I had to put batteries upside down to make it work (so battery\u0026rsquo;s plus is facing the plate marked with minus). Same on both sides. Am I interpreting these markings wrong? (it works, so not very important, I am just curious)\nThe green light doesn\u0026rsquo;t seem to work in the indicator led. Is this a defective led or a soldering defect? How to \u0026ldquo;debug\u0026rdquo; it? The blue and red lights work separately and together, producing magenta, but the green doesn\u0026rsquo;t work both on its own when combined with red or blue, it just stays off.\nWhat do I do with several boards that I like a lot? I now have Ergodox EZ, Moonlander and Mitosis. Should I cycle between them daily or weekly?\n","href":"/html/2020-11-22-mitosis-keyboard-first-impressions.html","title":"Mitosis Keyboard First Impressions"},{"content":"After reading this, this and this and this, I\u0026rsquo;ve got Oculus Quest willing to try it for work. I am a software developer and spend 8-10 hours per day before the laptop. My main machine at the moment is Mac Book Pro, I also own Thinkpad with Linux.\nThis way, in terms of software, ImmersedVR seems to be the only contender and only compatible headsets are Oculus Go and Oculus Quest. I\u0026rsquo;ve go Quest as it has a higher resolution.\nAfter reading all the posts above my expectations of the image quality were quite high, I was hoping to get something similar to what I have with regular physical computer screens. In (virtual) reality it appeared to be not that good. The text is crisp enough to read, but the overall image has some moving artifacts and general experience is somewhat close (I think) to what I\u0026rsquo;ve seen long ago on first 14-15 inch CRT monitors. The video in this post is pretty accurate - you can see these moving artifacts (should I call it moire? or distortion?), especially on the edges of screens. I don\u0026rsquo;t know if that is a resolution that is not high enough or a refresh rate not high enough, maybe both. There is eye strain too, after using it for 2-3 hours and taking the headset off, you can feel like your eyes begging to stay closed for a while to get some rest.\nAnyway, this is a future of screens as I see and want it where you have a wearable (headset, glasses) that create a virtual screen or several screens of any size you want. It is way more compact and easier than having a physical setup. I also like to move often from place to place, work outside, sit, or stay, so I rarely use a stationary workplace that I have at home with 27-inch monitor. Usually, I just take the laptop and sit on a coach with it. Having a VR screen is a big addition to the portable setup that you can take anywhere with you.\nAlthough the image quality is less than what I expected, it is not that bad, and I was able to work in VR for the most of the day, I am just saying that it is still not a perfect experience. I would say it is definitely something that I can use right now, at least for the part of the day, and I am looking forward to higher resolution devices that would improve the quality of the image.\nGood parts:\nFreedom and portability: you can sit (or lay or stay) wherever you want and position the screen anywhere, for example, I can lay in a deck chair like this with a big screen hanging above me in the air. This is something you can do physically, but that would be quite an effort (and you can not take it with you on a beach). Easy to add more monitors (up to 5 virtual monitors in a paid version of ImmersedVR), again you can reposition them easily in a way that is hard to do or impossible in a physical setup. Using oculus controller instead of the mouse is OK (better than I expected), you can scroll with the stick and click with a trigger or a button (this is where some training is needed to avoid moving the pointer up when you pull the trigger). Nobody could say if you are working or watching a movie :) Less good:\nImage quality, as mentioned above, is not perfect. It is not convenient to drink coffee, hope future headsets will be smaller and closer to glasses in size. A bit of practice is needed to adjust both headset and the virtual screen to make it look sharp enough. Regarding the headset adjustment, correct position on the head is very important. I also own PSVR and I must say it is way easier to put it on - there are no sticky straps, and it is very easy to adjust it properly.\nSome specific notes about working in Quest:\nTouch typing is essential, at least unless they implement virtual keyboard that works with hands tracking (this is not a problem for me, as I do touch typing). The guardian system is very useful, and stationary area (vs roomscale area) is good for cases when you are sitting in place - it makes a circle around you, so no need to draw circles manually with the controller. Another good thing is that you move out of the guarded area, you start seeing the outside world through the external cameras. The hole around the nose is useful to peek out of the headset to grab something or check where the keyboard is. Headset weight, at least for me, this is not a big concern, I was worried it would be too heavy, but in fact, I was able to have the headset on for quite a long time without any problem. And notes about ImmersedVR:\nIt looks better in 1440x900 resolution (on Mac: \u0026ldquo;scaled\u0026rdquo; display mode, \u0026ldquo;lager text\u0026rdquo; option). Different virtual environments are good, although I ended up using \u0026ldquo;void\u0026rdquo; where everything is just dark around you. Immersed settings can be invoked with stick click, this is also useful to check the headset battery level, there\u0026rsquo;s an indicator at the bottom bar. The laptop screen sometimes goes dark after exiting immersed VR, increase/decrease brightness to fix it (fn + F1/F2 on Mac). It is easier to use the side button on the controller to position the screen rather than the icons to move and scale. While holding the side button, you can move the screen and using the stick you can scale it. If you messed up the screen position, open immersed settings (stick click) and select settings - monitors - reset, it will get back to the center of the view. Besides Immersed VR, there are also Virtual Desktop and vSpatial, but both only support Windows, so software choice (Immersed VR) was out of the question. One thing I didn\u0026rsquo;t find how to do (maybe it\u0026rsquo;s not possible) in Immersed VR is the ability to move the virtual screen horizontally (left / right) or vertically without rotation. It seems that I can only move it up and down or left-right, while it rotates around the point of view at the same time, while sometimes I just want to move it a bit left or right straight, without rotation.\nWhat else would be cool to have:\nA virtual keyboard that works with hands tracking Some hotkey to enable external cameras, the same as in the guardian setup mode, or when you walk off the guardian area. Looks like this is not possible at the moment. Update: this is possible with Oculus\u0026rsquo;s pass-through mode. Besides work, it is an amazing gaming and entertainment platform. I like Beat Saber on PSVR a lot and immediately got a copy for Quest, others that look quite interesting are FitXR and eleven table tennis, as well as many other awesome games and apps (don\u0026rsquo;t forget netflix on a big screen).\nSummarizing, I can say that I satisfied with the overall experience, even so, it didn\u0026rsquo;t match all of my expectations.\nAlso posted on reddit.\n","href":"/html/2020-05-03-oculus-quest-for-work.html","title":"Oculus Quest for Work: First Impressions"},{"content":"","href":"/tags/vr/","title":"vr"},{"content":"","href":"/tags/vue/","title":"vue"},{"content":"It can be useful to have more than one configuration file, for example, to build several code bundles.\nThe config file to use can be set with VUE_CLI_SERVICE_CONFIG_PATH environment variable:\n# Build using vue.config.public.js CONF=`realpath vue.config.public.js` VUE_CLI_SERVICE_CONFIG_PATH=$CONF npm run build -- --mode production # build using vue.config.js npm run build -- --mode production Where npm run build is defined in package.json as vue-cli-service build.\nIt is also possible to create several bundles using vue cli multi-page mode, but in this case we will have big common js and css \u0026ldquo;vendors\u0026rdquo; package.\nThe js bundle is solvable with custom chanks settings, but it still leaves big css shared package (I didn\u0026rsquo;t find the solution for that).\nThe separate vue.config.js file had an advantage of building a completely independent bundle.\nThe alternative, check the comments is to have separate configs and copy them over, but this is less convenient than using the environment variable.\nNote: I didn\u0026rsquo;t find any mention on VUE_CLI_SERVICE_CONFIG_PATH in the documentation, found it looking at the source code.\n","href":"/html/2020-05-03-vue-cli-multiple-configs.html","title":"Vue.js Cli: How to Use Multiple vue.config.js Configs"},{"content":"A collection of links to touch typing tutors and games.\nOnline Typing Tutors keybr.com - Typing practice, supports Qwerty, Dvorak, Colemak and Workman layouts keyhero.com - Typing test and lessons speedcoder.net - Online Typing Practice for Programmers The Typing Cat - Free online keyboard typing tutor TypingClub - Learn Touch Typing Free typing.io - Typing Practice for Programmers monkeytype.com - a minimalistic, customizable typing test, featuring many test modes, open source Ratatype — Online typing tutor and typing lessons Typing Scout - Faster typing means more spare time :) Solotyping - Touch Typing Course | Online Typing Test | Learn to Type Faster 10FastFingers.com - Typing Test, Competitions, Practice \u0026amp; Typing Games TyperA - the original typing test TypingStudy - Touch Typing Practice Online Ngram Type - Practice typing bigrams, trigrams, tetragrams and top words Online Games Nitro Type - Competitive Typing Game | Race Your Friends TypeRacer - Free typing game and competition Клавогонки - racing game ZType – Typing Game - Type to Shoot Hacker Typer - just start typing anything to look smart git-invaders ★ GitHub Game Off 2012 Desktop Games Epistory Typing chronicles - a beautiful atmospheric 3D action/adventure typing game Nanotale Typing Chronicles - one more typing advengure game The Typing of the Dead (windows game) - Wikipedia, Steam Desktop Software Fonttype (windows) - Look front. Type looking front. Frontype!. Onscreen keyboard to learn typing (instead of tutors). Interesting idea, I didn\u0026rsquo;t try this myself GNU Typist (cross-platform) - typing tutor Keyboard Memo Keyboard with finger positions (source: http://frontype.com/keyboarding/400px-Touch-typing.png):\nShows which key should be pressed with which finger (middle pink is for index fingers, green for middle fingers and so on).\nGrey keys: I usually use pinky here, except for the space bar).\nSpace bar: Press with left thumb after typing a letter with right hand and press with the right thumb after typing a letter with left hand.\n","href":"/html/2020-01-21-touch-typing.html","title":"Touch Typing Tutors and Games"},{"content":"","href":"/tags/typing/","title":"typing"},{"content":"","href":"/tags/aws/","title":"aws"},{"content":"I started seeing an increased charge in billing for AWS Config service in one of the accounts, it increased from around $5 to $100 per month. And I didn\u0026rsquo;t even remember if I enabled and configured it.\nI could not get any details from AWS Cost Explorer besides that charges are in the same region where our app is running.\nThe confusing part was a note in the AWS Config management console:\nWhy am I seeing this page? You can now use Config Rules to check configurations recorded by AWS Config. These steps will update your settings and permissions to use Config Rules. To use AWS Config without Config Rules, click here. When you are ready to try Config Rules, update your settings by referring to our documentation.\nSo it even looked like we didn\u0026rsquo;t configure AWS Config at all and the questions I had were:\nWhy are we charged for AWS Config?\nCan we disable it?\nHow can we access the results of AWS Config? (We were charged for something, which might be useful - how to access this data?)\nThe AWS Config pricing page was also a bit misleading - what caught my eye was the table with rule evaluation prices (and I didn\u0026rsquo;t have any rules defined).\nOnly after reading through the examples, I found that they also use some \u0026ldquo;configuration items\u0026rdquo; to calculate the price, not only rules evaluations. The thing I missed was at the very top: You pay $0.003 per configuration item recorded in your AWS account per AWS Region. (I jumped over this part initially right into the rules pricing table).\nIt appears, that AWS Config was enabled and some automation in our account caused the large number of configuration items (changes to the configuration) to be recorded. The guide on how to get and query the collected data (its on S3) can be found here.\nThe procedure is following:\nGo to AWS Config settings, check the bucket name Go to S3, find that bucket and go into it to get the path to the logs The path will be something like this: s3://aws-config-bucket-123456789123/AWSLogs/123456789123/Config/us-east-1 Go to AWS Athena, create external table and query the data The table can be created using from query editor. First, create the database:\nCREATE DATABASE s3data Then, create the table for AWS Config data:\nCREATE EXTERNAL TABLE awsconfig ( fileversion string, configSnapshotId string, configurationitems ARRAY \u0026lt; STRUCT \u0026lt; configurationItemVersion : STRING, configurationItemCaptureTime : STRING, configurationStateId : BIGINT, awsAccountId : STRING, configurationItemStatus : STRING, resourceType : STRING, resourceId : STRING, resourceName : STRING, ARN : STRING, awsRegion : STRING, availabilityZone : STRING, configurationStateMd5Hash : STRING, resourceCreationTime : STRING \u0026gt; \u0026gt; ) ROW FORMAT SERDE \u0026#39;org.apache.hive.hcatalog.data.JsonSerDe\u0026#39; LOCATION \u0026#39;s3://aws-config-bucket-123456789123/AWSLogs/123456789123/Config/us-east-1/\u0026#39; Replace the LOCATION path with actual path on S3. Now we can query the awsconfig table to see how many and which items were created, for example:\nSELECT result.configurationitemcapturetime, count(result.configurationitemcapturetime) AS NumberOfChanges FROM ( SELECT regexp_replace( configurationItem.configurationItemCaptureTime, \u0026#39;(.+)(T.+)\u0026#39;, \u0026#39;$1\u0026#39; ) AS configurationitemcapturetime FROM s3data.awsconfig CROSS JOIN UNNEST(configurationitems) AS t(configurationItem) WHERE \u0026#34;$path\u0026#34; LIKE \u0026#39;%ConfigHistory%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026gt;= \u0026#39;2019-10-01T%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026lt;= \u0026#39;2019-10-10T%\u0026#39; ) result GROUP BY result.configurationitemcapturetime ORDER BY result.configurationitemcapturetime The above will show number of configuration items created per day. The configurationitems field is a JSON object, so we UNNEST and join it to access the columns inside JSON.\ncapturetime NumberOfChanges 1 2019-10-01 4279 2 2019-10-02 4254 3 2019-10-03 4307 4 2019-10-04 3765 5 2019-10-05 134 6 2019-10-06 7 7 2019-10-07 9 8 2019-10-08 2 This is how to get a data sample:\nSELECT configurationItem.configurationItemVersion, configurationItem.configurationItemCaptureTime as fullTime, configurationItem.configurationStateId, configurationItem.awsAccountId, configurationItem.configurationItemStatus, configurationItem.resourceType, configurationItem.resourceId, configurationItem.resourceName, configurationItem.aRN, configurationItem.awsRegion, configurationItem.availabilityZone, configurationItem.configurationStateMd5Hash, configurationItem.resourceCreationTime FROM s3data.awsconfig CROSS JOIN UNNEST(configurationitems) AS t(configurationItem) WHERE \u0026#34;$path\u0026#34; LIKE \u0026#39;%ConfigHistory%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026gt;= \u0026#39;2019-10-01T%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026lt;= \u0026#39;2019-10-10T%\u0026#39; capturetime ... configurationitemstatus resourcetype ... resourcename ... 1 2019-10-07 ResourceDeleted AWS::RDS::DBSnapshot rds:db-wordpress-2019-09-28-10-23 2 2019-10-07 ResourceDeleted AWS::RDS::DBSnapshot rds:db-wordpress-2019-09-29-10-24 3 2019-10-07 ResourceDiscovered AWS::RDS::DBSnapshot rds:db-wordpress-2019-10-07-10-23 4 2019-10-07 ResourceDeleted AWS::RDS::DBSnapshot rds:db-production-2019-09-28-10-18 5 2019-10-07 OK AWS::RDS::DBInstance db-production 6 2019-10-07 OK AWS::RDS::DBInstance db-wordpress 7 2019-10-07 OK AWS::RDS::DBInstance db-production 8 2019-10-07 OK AWS::RDS::DBInstance db-wordpress And this query fetches data grouped by resource type, name and status:\nSELECT configurationItem.resourceType, configurationItem.resourceName, configurationItem.configurationItemStatus, COUNT(configurationItem.resourceId) AS NumberOfChanges FROM s3data.awsconfig CROSS JOIN UNNEST(configurationitems) AS t(configurationItem) WHERE \u0026#34;$path\u0026#34; LIKE \u0026#39;%ConfigHistory%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026gt;= \u0026#39;2019-10-07T%\u0026#39; AND configurationItem.configurationItemCaptureTime \u0026lt;= \u0026#39;2019-10-08T%\u0026#39; GROUP BY configurationItem.resourceType , configurationItem.resourceName , configurationItem.configurationItemStatus ORDER BY NumberOfChanges DESC resourcetype resourcename configurationitemstatus NumberOfChanges 1 AWS::RDS::DBInstance db-wordpress OK 2 2 AWS::RDS::DBInstance db-production OK 2 3 AWS::RDS::DBSnapshot rds:db-production-2019-10-07-10-18 ResourceDiscovered 1 4 AWS::RDS::DBSnapshot rds:db-wordpress-2019-10-07-10-23 ResourceDiscovered 1 5 AWS::RDS::DBSnapshot rds:db-production-2019-09-28-10-18 ResourceDeleted 1 6 AWS::RDS::DBSnapshot rds:db-wordpress-2019-09-29-10-24 ResourceDeleted 1 7 AWS::RDS::DBSnapshot rds:db-wordpress-2019-09-28-10-23 ResourceDeleted 1 Using these queries, it should be possible to get an idea of what is happening, in my case it was misbehaving ElasticBeanstalk environment with bad health that was constantly re-creating all the resources.\n","href":"/html/2019-10-08-aws-config-charges.html","title":"AWS Config - Unexpected Charges and Data Analysis"},{"content":"When using project management system (Jira, Redmine, Github issues, etc) it is useful to add the issue number into commit message that makes is easier to understand which issue the commit belongs to and often allows the project management system to display related commits.\nFor same reasons, it is also useful to include the issue number into branch name, such as 123-branch-description or feature/PROJECT-123-branch-description.\nThe process of adding the issue number into commit message can be automated with git prepare-commit-msg hook (shell script).\nBelow are few examples of hook scripts.\nBranch Named as 123-branch-description In the simplest case we have branches named as 123-branch-description, where 123 is the issue id:\n#!/bin/bash # This hook works for branches named such as \u0026#34;123-description\u0026#34; and will add \u0026#34;[#123]\u0026#34; to the commit message. # get current branch branchName=`git rev-parse --abbrev-ref HEAD` # search issue id in the branch name, such a \u0026#34;123-description\u0026#34; or \u0026#34;XXX-123-description\u0026#34; issueId=$(echo $branchName | sed -nE \u0026#39;s,([A-Z]?-?[0-9]+)-.+,\\1,p\u0026#39;) # only prepare commit message if pattern matched and issue id was found if [[ ! -z $issueId ]]; then # $1 is the name of the file containing the commit message # sed -i.bak -e \u0026#34;1s/^/\\n\\n[$issueId]\\n/\u0026#34; $1 echo -e \u0026#34;[#$issueId] \u0026#34;\u0026#34;$(cat $1)\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # echo -e \u0026#34;[$issueId]\\n\u0026#34;\u0026#34;$(cat $1)\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # sed -i.bak -e \u0026#34;1s/^/$TRIMMED /\u0026#34; $1 fi Branch Named as feature/PROJECT-123-branch-description This version is useful for gitflow and Jira (where we should use PROJECT-123 format to display commits in the issue):\n#!/bin/bash # This hook works for branches named such as \u0026#34;feature/ABC-123-description\u0026#34; and will add \u0026#34;[ABC-123]\u0026#34; to the commit message. # get current branch branchName=`git rev-parse --abbrev-ref HEAD` # search jira issue id in a pattern such a \u0026#34;feature/ABC-123-description\u0026#34; jiraId=$(echo $branchName | sed -nE \u0026#39;s,[a-z]+/([A-Z]+-[0-9]+)-.+,\\1,p\u0026#39;) # only prepare commit message if pattern matched and jiraId was found if [[ ! -z $jiraId ]]; then # $1 is the name of the file containing the commit message # sed -i.bak -e \u0026#34;1s/^/\\n\\n[$jiraId]\\n/\u0026#34; $1 echo -e \u0026#34;[$jiraId] \u0026#34;\u0026#34;$(cat $1)\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # echo -e \u0026#34;[$jiraId]\\n\u0026#34;\u0026#34;$(cat $1)\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # sed -i.bak -e \u0026#34;1s/^/$TRIMMED /\u0026#34; $1 fi Cross-Repository Version This is useful when project uses multiple repositories on GitHub and issues tracked in one of the project (or there is a separate project for issues).\nIn this case, we aslo need to add full repository name to the commit message.\nThe hook below puts short issue number at the beginning of the commit message and adds full issue id (organization/repository/#issueId) at the end.\n#!/bin/bash # This hook works for branches named such as \u0026#34;123-description\u0026#34; and will add \u0026#34;[#123]\u0026#34; to the commit message. # get current branch branchName=`git rev-parse --abbrev-ref HEAD` # search issue id in the branch name, such a \u0026#34;123-description\u0026#34; or \u0026#34;XXX-123-description\u0026#34; issueId=$(echo $branchName | sed -nE \u0026#39;s,([A-Z]?-?[0-9]+)-.+,\\1,p\u0026#39;) # only prepare commit message if pattern matched and issue id was found if [[ ! -z $issueId ]]; then # $1 is the name of the file containing the commit message # We prepend issue number in the beginning (so we can easily see it in history) # And also we append the full issue id (with organization/repository#) prefix, # so this commit hook works for other repositories (frontend) too. echo -e \u0026#34;[#$issueId] \u0026#34;\u0026#34;$(cat $1)\u0026#34;\u0026#34;\\n[organization/repository#$issueId]\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # sed -i.bak -e \u0026#34;1s/^/\\n\\n[$issueId]\\n/\u0026#34; $1 # echo -e \u0026#34;[$issueId]\\n\u0026#34;\u0026#34;$(cat $1)\u0026#34; \u0026gt; \u0026#34;$1\u0026#34; # sed -i.bak -e \u0026#34;1s/^/$TRIMMED /\u0026#34; $1 fi Installing the Hook Create the .git/hooks/prepare-commit-msg script Copy the content from the snippet above Make the file executable chmod +x .git/hooks/prepare-commit-msg Try to commit something, check that the issue number is automatically added Note: when you do the commit --amend, the issue number will be added for the second time - you need to remove it manually.\n","href":"/html/2019-06-16-git-hook-to-add-issue-number-to-commit-message.html","title":"Git Hook to Add Issue Number to Commit Message"},{"content":"AWS CloudFront allows to have multiple origins for the distribution and, along with lambda@edge functions, that makes it possible to use CloudFront as an entry point to route the requests to different services based on the request path.\nFor example:\nwww.myapp.com -\u0026gt; unbounce.com (landing pages) www.myapp.com/app -\u0026gt; single page app hosted on S3 www.myapp.com/blog -\u0026gt; wordpress blog CloudFront Setup Structure CloudFront distribution is an entry point which we assign a root domain name (www.myapp.com).\nThe distribution can have one or more origins (these are the our services - landing page, S3 app, wordpress, etc).\nAnd each origins has behaviors - rules defining how CloudFront will work for the specific request path:\nAllowed protocols (HTTP, HTTPS) and HTTP methods Caching settings Lambda@Edge functions to add custom logic to the request or response processing. For example, we can have a distribution for a single-page app hosted on S3 (origin is S3 bucket) with one behavior defining caching rules.\nOr we can have a distribution for a wordpress site with one origin (wordpress server) and several behaviors defining caching rules for static and dynamic content.\nIn a simple case that is enough: if we have multiple services (single-page app and wordpress blog), we can create separate distributions for them and use different sub-domains to route the requests (app.myapp.com and blog.myapp.com).\nBut it is also possible to merge different services under the same distribution and same domain name (myapp.com/app and myapp.com/blog) - this is the multi-origin setup described below.\nMulti-Origin CloudFront Setup If we want to have a single domain name used for all services (frontend app, landing pages, wordpress blog, etc), we can achieve that with CloudFront by having multiple origins for each service and multiple behaviors to specify which request paths should be forwarded to these origins.\nThe example setup for this post includes following origins:\nwww.myapp.com -\u0026gt; unbounce.com, landing pages www.myapp.com/app -\u0026gt; single-page application, S3 bucket www.myapp.com/blog -\u0026gt; wordpress running on ElasticBeanstalk (origin points to Elastic Load Balancer) And behaviors for our distribution will route requests to different origins based on the request path:\napp* -\u0026gt; S3 origin, S3-app.myapp.com blog/wp-login.php -\u0026gt; wordpress origin, wordpress-elb, no caching blog/wp-admin/* -\u0026gt; wordpress origin, wordpress-elb, no caching blog/wp-content/upload, origin: S3-wordpress.myapp.com blog*, origin: wordpress-elb / -\u0026gt; unbounce origin, unbounce-www.myapp.com The app* behavior serves the single-page application from the S3 origin. The app is accessible from the sub-path (myapp.com/app) and should be placed in the sub-folder on S3 (the sub-path is preserved in the request to origin).\nIn the case of single page web application, there is a single entry point, usually index.html that is loaded on first request and further navigation accross app pages is handled by application code in browser.\nTo implement this logic in CloudFront, we use Lambda@Edge functions attached to origin request and response events:\nOrigin Request: CloudFrontSubdirectoryIndex - handles root requests (www.myapp.com/app) and forwards them to index.html Origin Response: CloudFrontDefaultIndexForOrigin - catches 404 / 403 requests (www.myapp.com/app/some-path) and forwards them to index.html In general, this setup is flexible enough to handle various use cases.\nAdvantages of CloudFront for routing are:\nThe distributed solution, which is very fast for end users No need in single entry point component for app services, requests are routed by CloudFront All services are under the same subdomain, so we avoid additional CORS complexity More details about the setup for specific services and lambda functions code are below.\nSingle Page App Setup The single page app setup is quite simple with CloudFront: the app source is copied to S3 bucket and then served via CloudFront (the origin is S3 bucket, with one default behavior).\nHere is an example shell script to deploy the SPA application to S3 (which also sends Slack notification about the deployment):\n#!/usr/bin/env bash SLACK_HOOK=https://hooks.slack.com/services/XXX/YYY/aaabbbccc GIT_BRANCH=$(git rev-parse --abbrev-ref HEAD) if [[ $APP_ENV == \u0026#34;develop\u0026#34; ]]; then CLOUDFRONT_DISTRIBUTION=EEE88888888888 AWS_BUCKET=dev.myapp.com elif [[ $APP_ENV == \u0026#34;production\u0026#34; ]]; then CLOUDFRONT_DISTRIBUTION=EEE9999999999 AWS_BUCKET=app.myapp.com else echo \u0026#34;Unknown environment ${APP_ENV}\u0026#34; exit 1 fi echo \u0026#34;Deploying ${GIT_BRANCH} to ${AWS_BUCKET} with options ${AWS_CLI_OPTS}\u0026#34; aws s3 sync --delete dist/ s3://${AWS_BUCKET}/app/ ${AWS_CLI_OPTS} # Invalidate app on CloudFront aws cloudfront create-invalidation --distribution-id ${CLOUDFRONT_DISTRIBUTION} --paths /app/* ${AWS_CLI_OPTS} if [ $? -eq 0 ]; then echo \u0026#39;Deployment: OK\u0026#39; curl -s -X POST $SLACK_HOOK \\ -H \u0026#34;content-type:application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;text\u0026#34;:\u0026#34;[frontend] Branch `\u0026#39;\u0026#34;${GIT_BRANCH}\u0026#34;\u0026#39;` was successfully deployed to `\u0026#39;\u0026#34;${AWS_BUCKET}\u0026#34;\u0026#39;` bucket.\u0026#34;}\u0026#39; else echo \u0026#39;Deployment: FAILED\u0026#39; exit $? curl -s -X POST $SLACK_HOOK \\ -H \u0026#34;content-type:application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;text\u0026#34;:\u0026#34;[frontend] Branch `\u0026#39;\u0026#34;${GIT_BRANCH}\u0026#34;\u0026#39;` deployment to `\u0026#39;\u0026#34;${AWS_BUCKET}\u0026#34;\u0026#39;` bucket FAILED.\u0026#34;}\u0026#39; fi I\u0026rsquo;ve used similar setup for Angular and Vue applications, it should work well for other frameworks too.\nIn a single-origin setup (no other services, and the distribution only used for single page app), it is also useful to:\nSet default object for distribution to index.html Add 404 and 403 error handlers for distribution and reply 200 OK and with index.html (so all requests are sent to the single page app even if there are no real files on S3) Unfortunately, these settings are on the distribution level, but in multi-origin setup we would need them for behaviors represeting specific request paths. For example, we need to redirect requests to index.html for www.myapp.com/app, but not for www.myapp.com/blog.\nIn a multi-origin setup this logic is handled by Lambda@Edge functions: CloudFrontSubdirectoryIndex and CloudFrontDefaultIndexForOrigin\nWordPress Setup WordPress can be installed on a standalone EC2 instance, hosted externally or launched using ElasticBeanstalk, in any case the CloudFront setup will be similar:\nOne origin that points to the wordpress server (wordpress-elb in my case, ElasticBeanstalk load balancer) Several behaviors define caching rules for the static and dynamic content: blog/wp-login.php -\u0026gt; wordpress origin, wordpress-elb, no caching blog/wp-admin/* -\u0026gt; wordpress origin, wordpress-elb, no caching blog/wp-content/upload, origin: S3-wordpress.myapp.com blog*, origin: wordpress-elb Here we forward requests to /wp-login.php and wp-admin/* without caching.\nWordPress media files are hosted on S3 (for example, using WP Media Offload plugin) and other requests also go to the load balancer, but with caching enabled.\nFor the blog* behaviors we also have the CloudFrontRedirectToTrailingSlash lambda function attached to the \u0026ldquo;Viewer Request\u0026rdquo; event that forwards www.myapp.com/blog requests to www.myapp.com/blog/.\nHere are some useful resources describing the WordPress setup options on AWS:\nDeploying WordPresswith AWS Elastic Beanstalk (PDF) How to accelerate your WordPress site with Amazon CloudFront: https://aws.amazon.com/blogs/startups/how-to-accelerate-your-wordpress-site-with-amazon-cloudfront/ https://d0.awsstatic.com/whitepapers/wordpress-best-practices-on-aws.pdf How to accelerate your WordPress site with Amazon CloudFront | AWS Startups Blog: https://aws.amazon.com/blogs/startups/how-to-accelerate-your-wordpress-site-with-amazon-cloudfront/ Hosting WordPress on AWS: https://github.com/aws-samples/aws-refarch-wordpress Elastic Beanstalk Setup for PHP App\nThe whole wordpress setup (along with the wordpress code and plugins) is added to the git repository and deployed to ElasticBeanstalk as php application.\nThe (standard) Elastic Beanstalk setup for php is a set of configurations and scripts that automatically deploys and runs the application on the EC2 instance:\nThe app is deployed to /var/www/html and handled by apache Apache logs are under /var/log/httpd (logs are also sent to CloudWatch) System log is /var/log/messages Note: along with the application code it is useful to also deploy .git folder (don\u0026rsquo;t add it to .ebignore), so git status on the server inside the /var/www/html will show if there are any modifications done directly on the server by wordpress or plugins.\nWordpress plugins often modify local files, so it is useful to check the behavior periodically (especially after installing some plugins or doing configuration changes) to see if there is something that needs to be saved to repostiory. This is useful to check if there are any local changes to the files after changing wordpress settings (if there are changes, we need to copy them back to local machine, commit to git and redeploy).\nUnbounce Setup Unbounce.com is a service for landing pages and there are two (official) ways to use it: point CNAME (such as www.myapp.com) to unbounce.com or use a wordpress plugin - in this case the \u0026ldquo;Host\u0026rdquo; header should contain the domain name (www.myapp.com).\nWe can use the latter to make unbounce work with CloudFront: we setup the www.myapp.com domain to point to CloudFront and we configure CloudFront to forward the Host header to unbounce.\nThis is how unbounce and CloudFront can be configured to work together:\nAdd www.myapp.com domain on unbounce.com Get the unbounce URL, like ab44444888888.unbouncepages.com In DNS settings, point www CNAME to ab44444888888.unbouncepages.com Wait until unbounce shows \u0026ldquo;Working and Secure\u0026rdquo; for the domain Edit (or create) CloudFront distribution, set alternative domain to www.myapp.com (or see https://serverfault.com/a/888776/527019, there is also a method based on lambda function) Create the CloudFront origin, point it to unbounce URL (ab44444888888.unbouncepages.com) Create default behavior (*) for the origin above, disable caching (set \u0026ldquo;Cache Based on Selected Request Headers\u0026rdquo; to \u0026ldquo;All\u0026rdquo;) Change DNS settings, point www subdomain to CloudFront distribution (d33xxx8888xxxx.cloudfront.net) - replace the unbounce domain with CloudFront domain Wait for DNS changes to propagate, now the www.myapp.com root URL should point to CloudFront So we first point CNAME to unbounce.com, wait until unbounce thinks that everything is good and then reconfigure the CNAME to point to CloudFront (and CloudFront forwards the custom domain name via Host header).\nLambda Code: CloudFrontDefaultIndexForOrigin This is the function to handle index documents in subfolders.\nFor multi-origin setup, we use CloudFront distribution to route the requests to multiple origins depending on the request path.\nIn this case the application on S3 also needs to be in the sub-folder (my-bucket/app). And it means that we lose the ability to redirect failing requests to index.html (this only works in the root folder), so if we go to the url www.myapp.com/app/login, it will end up with a 404 error.\nIn a single-origin CloudFront distribution this issue can be solved by setting the default object for the distribution to index.html and also adding \u0026ldquo;Error pages\u0026rdquo; for 404 and 403 that would also point to index.html.\nFor a \u0026ldquo;mutli-origin\u0026rdquo; setup a solution is to add lambda@edge function for the \u0026ldquo;Origin Response\u0026rdquo; event and replace 403 / 404 errors with index.html content.\nThe Lambda function code (source) looks like this:\n\u0026#39;use strict\u0026#39;; // Source - https://andrewlock.net/using-lambda-at-edge-to-handle-angular-client-side-routing-with-s3-and-cloudfront/ const http = require(\u0026#39;https\u0026#39;); const indexPage = \u0026#39;index.html\u0026#39;; exports.handler = async (event, context, callback) =\u0026gt; { const cf = event.Records[0].cf; const request = cf.request; const response = cf.response; const statusCode = response.status; // Only replace 403 and 404 requests typically received // when loading a page for a SPA that uses client-side routing const doReplace = request.method === \u0026#39;GET\u0026#39; \u0026amp;\u0026amp; (statusCode == \u0026#39;403\u0026#39; || statusCode == \u0026#39;404\u0026#39;); const result = doReplace ? await generateResponseAndLog(cf, request, indexPage, response) : response; callback(null, result); }; async function generateResponseAndLog(cf, request, indexPage, originalResponse){ const domain = cf.config.distributionDomainName; const appPath = getAppPath(request.uri); const indexPath = `/${appPath}/${indexPage}`; const response = await generateResponse(domain, indexPath, request, originalResponse); // console.log(\u0026#39;response: \u0026#39; + JSON.stringify(response)); return response; } async function generateResponse(domain, path, request, originalResponse){ try { // Load HTML index from the CloudFront cache const s3Response = await httpGet({ hostname: domain, path: path }); const headers = s3Response.headers || { \u0026#39;content-type\u0026#39;: [{ value: \u0026#39;text/html;charset=UTF-8\u0026#39; }] }; const responseHeaders = wrapAndFilterHeaders(headers, originalResponse.headers || {}) // Debug headers to see the original requested URL vs the index file request. // responseHeaders[\u0026#39;x-lambda-request-uri\u0026#39;] = [{value: request.uri}] // responseHeaders[\u0026#39;x-lambda-hostname\u0026#39;] = [{value: domain}] // responseHeaders[\u0026#39;x-lambda-path\u0026#39;] = [{value: path}] // responseHeaders[\u0026#39;x-lambda-response-status\u0026#39;] = [{value: String(s3Response.status)}] return { status: \u0026#39;200\u0026#39;, headers: responseHeaders, body: s3Response.body }; } catch (error) { console.log(error) return { status: \u0026#39;500\u0026#39;, headers:{ \u0026#39;content-type\u0026#39;: [{ value: \u0026#39;text/plain\u0026#39; }] }, body: \u0026#39;An error occurred loading the page\u0026#39; }; } } function httpGet(params) { return new Promise((resolve, reject) =\u0026gt; { http.get(params, (resp) =\u0026gt; { console.log(`Fetching ${params.hostname}${params.path}, status code : ${resp.statusCode}`); let result = { status: resp.statusCode, headers: resp.headers, body: \u0026#39;\u0026#39; }; resp.on(\u0026#39;data\u0026#39;, (chunk) =\u0026gt; { result.body += chunk; }); resp.on(\u0026#39;end\u0026#39;, () =\u0026gt; { resolve(result); }); }).on(\u0026#39;error\u0026#39;, (err) =\u0026gt; { console.log(`Couldn\u0026#39;t fetch ${params.hostname}${params.path} : ${err.message}`); reject(err, null); }); }); } // Get the app path segment e.g. candidates.app, employers.client etc function getAppPath(path){ if(!path){ return \u0026#39;\u0026#39;; } if(path[0] === \u0026#39;/\u0026#39;){ path = path.slice(1); } const segments = path.split(\u0026#39;/\u0026#39;); // will always have at least one segment (may be empty) return segments[0]; } // Cloudfront requires header values to be wrapped in an array function wrapAndFilterHeaders(headers, originalHeaders){ const allowedHeaders = [ \u0026#39;content-type\u0026#39;, \u0026#39;content-length\u0026#39;, \u0026#39;last-modified\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;etag\u0026#39; ]; const responseHeaders = originalHeaders; if(!headers){ return responseHeaders; } for(var propName in headers) { // only include allowed headers if(allowedHeaders.includes(propName.toLowerCase())){ var header = headers[propName]; if (Array.isArray(header)){ // assume already \u0026#39;wrapped\u0026#39; format responseHeaders[propName] = header; } else { // fix to required format responseHeaders[propName] = [{ value: header }]; } } } return responseHeaders; } Lambda Code: CloudFrontRedirectToTrailingSlash This function is used for wordpress setup to redirect www.myapp.com/blog requests to www.myapp.com/blog/ (add trailing slash which triggers index.php loading from the folder).\nThe Lambda function code (source):\nconst path = require(\u0026#39;path\u0026#39;); exports.handler = async (event) =\u0026gt; { const { request } = event.Records[0].cf; const { uri } = request; const extension = path.extname(uri); if (extension \u0026amp;\u0026amp; extension.length \u0026gt; 0) { return request; } const last_character = uri.slice(-1); if (last_character === \u0026#34;/\u0026#34;) { return request; } const newUri = `${uri}/`; console.log(`Rewriting ${uri} to ${newUri}...`); request.uri = newUri; return request; }; The function is attached to \u0026ldquo;Viewer Request\u0026rdquo; event.\nLambda Code: CloudFrontSubdirectoryIndex This function is attached to the \u0026ldquo;Origin Request\u0026rdquo; event and it handles requests like www.myapp.com/app/ (with trailing slash at the end).\nBy default this request returns 200 OK with application/x-directory content type and empty body.\nThe Lambda function adds index.html to these requests (so the request becomes www.myapp.com/app/index.html).\nThe lambda function code (source):\n\u0026#39;use strict\u0026#39;; // Source: https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/ // // Handles requests like www.myapp.com/app/ (with trailing slash) and adds index.html to them // By default this request returns 200 OK with application/x-directory content type exports.handler = (event, context, callback) =\u0026gt; { // Extract the request from the CloudFront event that is sent to Lambda@Edge var request = event.Records[0].cf.request; // Extract the URI from the request var olduri = request.uri; // Match any \u0026#39;/\u0026#39; that occurs at the end of a URI. Replace it with a default index var newuri = olduri.replace(/\\/$/, \u0026#39;\\/index.html\u0026#39;); // Log the URI as received by CloudFront and the new URI to be used to fetch from origin console.log(\u0026#34;Old URI: \u0026#34; + olduri); console.log(\u0026#34;New URI: \u0026#34; + newuri); // Replace the received URI with the URI that includes the index page request.uri = newuri; // Return to CloudFront return callback(null, request); }; Lambda Code: CloudFrontRedirectApp This function can be useful to transfer the application from single-origin setup to multi-rigin. In this case, the app.myapp.com requests should be forwared to www.myapp.com/app.\nNote: in the next section there is another solution for the same problem, based on S3 static website hosting feature.\nWith Lambda function, the redirect can be done by attaching a function to the old CloudFront distribution behavior, the \u0026ldquo;Viewer Request\u0026rdquo; event:\nconst path = require(\u0026#39;path\u0026#39;); exports.handler = async (event) =\u0026gt; { const { request } = event.Records[0].cf; const { uri } = request; const newUri = `https://www.myapp.com/app${uri}`; console.log(`Rewriting ${uri} to ${newUri}...`); let responseHeaders = { location: [{ key: \u0026#39;Location\u0026#39;, value: newUri, }], } // Debug headers to see the original requested URL vs the index file request. responseHeaders[\u0026#39;x-lambda-request-uri\u0026#39;] = [{value: request.uri}] responseHeaders[\u0026#39;x-lambda-redirect-url\u0026#39;] = [{value: newUri}] /* * Generate HTTP redirect response with 302 status code and Location header. */ const response = { status: \u0026#39;302\u0026#39;, statusDescription: \u0026#39;Found\u0026#39;, headers: responseHeaders, }; return response; }; Another Way to Redirect Requests to the new Location with S3 Static Website This is another solution to the problem described above: redirect requests from the old location (such as app.myapp.com to new www.myapp.com/www).\nThis method is described in AWS docs.\nThe initial setup is this:\nThe app.myapp.com points to S3 bucket (root), there is CloudFront distribution using it as origin The www.myapp.com/app points to S3 bucket (/app subfolder), there is (another) CloudFront distribution using it as origin The redirect can be configured like this:\nCreate S3 bucket app.myapp.com-old.app (can be empty) Enable static website hosting for this bucket Select \u0026ldquo;Redirect requests\u0026rdquo; option Set \u0026ldquo;Target bucket or domain\u0026rdquo; to www.myapp.com/app Set \u0026ldquo;Protocol\u0026rdquo; to https Copy public URL for the bucket (develop.myapp.com-old.app.s3-website-us-east-1.amazonaws.com) Edit the CloudFront distribution for app.myapp.com, change origin source to public URL for the new bucket Now all requests to app.myapp.com should be redirected to www.myapp.com/app.\n","href":"/html/2019-06-16-multi-origin-cloudfront-setup.html","title":"Multi-Origin CloudFront Setup to Route Requests to Services Based on Request Path"},{"content":"Sometimes it is simpler to keep the package on github, for example, if you have a fork of a published package with some private changes. So you can avoid cluttering npm registry with similar packages, creating confusing for other people.\nNPM supports installing dependencies from github, but it is also good to have versioning for your package so you can use it exactly as other packages, develop it independently and upgrade the dependency for the main project in a controlled way.\nPreparation Add build branch to keep the last published version of the package:\ngit checkout -b build git push -u origin HEAD The build branch can include some files that are normally excluded from git control, for example, results of the webpack build.\nWorkflow The workflow I use is this:\nDo the changes as usually, create branches / Pull Requests, merge them to master Prepare release: Checkout build branch: git checkout build Merge master into it git merge master Run commands to generate build (if any), like npm run build Add build files to git: git add buiid/. and git commit -m \u0026quot;Rebuild\u0026quot; Push changes: git push origin HEAD Tag the new release git tag 3.1.1 -a, the -a flag means annotated tag, git will also ask for description Add the list of changes to annotation (I mostly use titles of pull requests along with PR id, like \u0026ldquo;Fix search, #19\u0026rdquo;, the PR id will turn into a link in github UI Push the tags: git push origin --tags Note: don\u0026rsquo;t forget about semver, basically: increase last number for fixes, second for new features, first for breaking changes.\nNow, the new release will appear on github under \u0026ldquo;releases\u0026rdquo;, for example https://github.com/serebrov/emoji-mart-vue/releases.\nUse the Package Include package from github, in package.json:\n\u0026#34;dependencies\u0026#34;: { \u0026#34;emoji-mart-vue\u0026#34;: \u0026#34;github:serebrov/emoji-mart-vue#3.1.1\u0026#34;, } Note that we specify the tag (#3.1.1 in the example above), so we can work on the package and release new versions (create new tags) and update the package version used by the main project when we want / need that.\n","href":"/html/2019-02-16-manage-npm-packages-on-github.html","title":"Managing NPM packages on github"},{"content":"","href":"/tags/node.js/","title":"node.js"},{"content":"","href":"/tags/npm/","title":"npm"},{"content":"","href":"/tags/linux/","title":"linux"},{"content":"The asciinema is a good and simple to use tool to record a screencast from the terminal session.\nAnd asciicast2gif allows to convert the recording to gif animation.\nvirtualenv -p python3 venv source venv/bin/activate pip install asciinema $ asciinema asciinema: recording asciicast to demo.cast asciinema: press \u0026lt;ctrl-d\u0026gt; or type \u0026#34;exit\u0026#34; when you\u0026#39;re done ... $ \u0026lt;ctrl-d\u0026gt; asciinema: recording finished asciinema: asciicast saved to demo.cast And convert it to gif:\ndocker run --rm -v $PWD:/data asciinema/asciicast2gif -t solarized-dark demo.cast demo.gif The result looks like this:\nAnd the recording of the recording process :)\n","href":"/html/2018-11-29-linux-terminal-screencast.html","title":"Recording Linux Terminal Session to GIF with asciinema"},{"content":"To get a very convenient full-screen console debugger for python, install ipdb and pdbpp packages.\nThen use __import__('ipdb').set_trace() to start the debugger and enter sticky to switch to the full-screen mode.\nBoth packages can be installed with pip:\nvirtualenv -p python3 venv source venv/bin/activate pip install ipdb pip install pdbpp The ipdb package improves the standard (pdb) debugger by adding syntax highlight and code completion. And the pdbpp adds the \u0026ldquo;sticky\u0026rdquo; mode, so the debugger can be run in a full-screen mode, in terminal:\nThis way we get an interactive debugging environment where we can execute the code, inspect variables and experiment with the live application state.\nIt works well for command-line scripts and web applications, the only thing you need is to start the application from terminal in a non-daemon mode, so the debugger can break the execution and switch to the interactive mode.\nIt also works if the app is started inside the docker container (same here - run in foreground mode, \u0026ldquo;docker run \u0026hellip;\u0026rdquo; or \u0026ldquo;docker-compose run \u0026hellip;\u0026rdquo; without -d flag).\n","href":"/html/2018-11-28-python-debugging-with-ipdb-pdbpp.html","title":"Debugging Python With ipdb and pdbpp"},{"content":"","href":"/tags/python/","title":"python"},{"content":"There are two ways recommended in pep-8 to format the blocks with long parameter lists in Python:\n# Arguments start on the next line foo = long_function_name( var_one, var_two, var_three, var_four) Another way is:\n# Arguments start on the same line foo = long_function_name(var_one, var_two, var_three, var_four) I always prefer the first option and the other one is problematic for a few reasons.\nFor example, if you have two blocks with such indentation, the indent will be jumping:\n# Two functions formatted in such way don\u0026#39;t look good, indent is jumping foo = long_function_name(var_one, var_two, var_three, var_four) bar = another_func(var_one, var_two, var_three, var_four) Another reason is that the indentation will be broken if you, for example, rename the function or make it a class method:\nfoo = long_function_name(var_one, var_two, var_three, var_four) # After renaming the function (or the `foo` variable), we need to fix the indentation on following lines foo = renamed_long_function_name(var_one, var_two, var_three, var_four) foo = self.long_function_name(var_one, var_two, var_three, var_four) Now, besides the rename / refactoring we need to also fix the indentation in following lines.\nWhile the first way of formatting works good in these cases:\n# Two functions near each other, indentation is not jumping foo = long_function_name( var_one, var_two, var_three, var_four) bar = another_func( var_one, var_two, var_three, var_four) # Rename / move to class, arguments formatting doesn\u0026#39;t need to be changed foo = long_function_name( var_one, var_two, var_three, var_four) foo = renamed_long_function_name( var_one, var_two, var_three, var_four) foo = self.long_function_name( var_one, var_two, var_three, var_four) Along with more convenience during the development, this also results in better diffs, so it will make the code review process easier for a reviewer.\n","href":"/html/2018-07-27-python-parameter-blocks-formatting.html","title":"Formatting Parameter Blocks in Python"},{"content":"","href":"/tags/disqus/","title":"disqus"},{"content":"It is possible to format and have syntax highlighting for code in Disqus comments. To do that, wrap the code into \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; tags (see the example comment to this post).\nI didn\u0026rsquo;t know about this feature and, acutally, I think this is an UI flaw.\nIt would be great to see the formatting help link or popup when you are editing the comment (and also the preview feature would be really nice to have).\nIt is also possible to specify the language, for example \u0026lt;code class=\u0026quot;javascript\u0026quot;\u0026gt;.\nSee how this code was formatted in the comment to this post:\n\u0026lt;pre\u0026gt;\u0026lt;code class=\u0026#34;javascript\u0026#34;\u0026gt; const verifyUser = function(username, password, callback) { dataBase.verifyUser(username, password, (error, userInfo) =\u0026gt; { if (error) return callback(error); getRoles(username, userInfo, callback); } } const getRoles = function(username, userInfo, callback) { dataBase.getRoles(username, (error, roles) =\u0026gt; { if (error) return callback(error) logAccess(username, userInfo, roles, callback); }) } const logAccess = function(username, userInfo, roles, callback) { dataBase.logAccess(username, (error) =\u0026gt; { if (error) return callback(error); callback(null, userInfo, roles); }) } \u0026lt;/pre\u0026gt;\u0026lt;/code\u0026gt; More information in the Disqus help - Syntax highlighting.\nRelated topics:\nAdding Images and Videos What HTML tags are allowed within comments? Mentions ","href":"/html/2018-01-19-disqus-code-in-comments.html","title":"Disqus - code formatting and highlighting in comments"},{"content":"","href":"/tags/js/","title":"js"},{"content":"There is no \u0026ldquo;callback hell\u0026rdquo; in Javascript, it is just a bad programming style. The infamous JavaScript \u0026ldquo;callback hell\u0026rdquo; can easily be fixed by un-nesting all the callbacks into separate functions.\nHere is an example:\nconst verifyUser = function(username, password, callback) { dataBase.verifyUser(username, password, function(error, userInfo) { if (error) { callback(error); } else { dataBase.getRoles(username, function(error, roles) { if (error) { callback(error); } else { dataBase.logAccess(username, function(error) { if (error) { callback(error); } else { callback(null, userInfo, roles); } }) } }) } }) }; The same code with separate functions instead of inline callbacks:\nconst verifyUser = function(username, password, callback) { dataBase.verifyUser(username, password, function(error, userInfo) { if (error) return callback(error); getRoles(username, userInfo, callback); } } const getRoles = function(username, userInfo, callback) { dataBase.getRoles(username, function(error, roles) { if (error) return callback(error); logAccess(username, userInfo, roles, callback); }) } const logAccess = function(username, userInfo, roles, callback) { dataBase.logAccess(username, function(error) { if (error) return callback(error); callback(null, userInfo, roles); }) } It is much more readable, manageable and reusable than the approach with inline functions. In fact, there is no \u0026ldquo;callback hell\u0026rdquo; in here - there are simply no nested callbacks in this code anymore.\nThis way, the \u0026ldquo;callback hell\u0026rdquo; is simply a bad style of writing code that is easy to avoid.\nLibraries like step, async, promises or async/await all provide a way to manage the asynchronous code. These methods may be less verbose or more convenient than \u0026ldquo;regular\u0026rdquo; callback functions, but callbacks can work too.\n","href":"/html/2018-01-19-there-is-no-callback-hell.html","title":"There Is No Callback Hell In JavaScript"},{"content":"","href":"/tags/ssh/","title":"ssh"},{"content":"Using SSH tunnels, it is possible to access remote resources that are not exposed to the Internet through the intermediate hosts or expose your local services to the Internet.\nSetup To make SSH commands shorter and easier to use, edit the ~/.ssh/config and add the configuration for the hosts you are going to connect.\nThe configuration defines default ssh options, so instead of the command like this ssh ec2-user@ec2-55-222-55-55.compute-1.amazonaws.com -i ~/.ssh/my_key.pem, we can just use ssh my-remote-host.\nAn example config:\nHost my-remote-host HostName ec2-55-222-55-55.compute-1.amazonaws.com StrictHostKeyChecking no User ec2-user IdentityFile ~/.ssh/my_key.pem Access Remote Hidden Resource The SSH command to access the remote hidden resource locally through the intermediate accessible host is ssh -L:\nlocal_port=5532 accessible_host=my-remote-host hidden_host=hidden_host.amazonaws.com hidden_port=5432 ssh ${accessible_host} -L ${local_port}:${hidden_host}:${hidden_port} With the command above we connect to the my-remote-host that has access to the hidden_host:hidden_port and make the hidden resource available locally:\nlocalhost:5532 =====\u0026gt; my-remote-host =====\u0026gt; hidden_host.amazonaws.com:5432 Expose Local Resource To the Internet The SSH command to expose the local resource through the intermediate host is ssh -R:\nremote_port=8181 local_port=8888 ssh my-remote-host -R *:${remote_port}:localhost:${local_port} With the command above we connect to the my-remote-host and instruct it to accept connections to the 8181 port and forward them to the localhost:8888.\nThe *:8181 that remote host will forward connections to any network interface (by default it will use only 127.0.0.1).\nlocalhost:8888 \u0026lt;===== my-remote-host \u0026lt;===== my-remote-host:8181 You also need to make sure that firewall on my-remote-host allows connections to the 8181 port.\nExample: Access RDS Database Through the EC2 Instance It is good idea to make RDS databases not available from the Internet, so they can only be accessed from the EC2 instances where applications are running.\nOn the other hand, during the development, it is convenient to have the database accessible from your local machine. It is easy to do it, running the following command:\nssh my-aws-host -L 5532:my-rds-host-name.cdiofumqrcpr.us-east-1.rds.amazonaws.com:5432 Here my-aws-host is the EC2 instance that has DB access and my-rds-host-name.cdiofumqrcpr...:5432 is the RDS host name and port.\nAfter that, you can use the localhost:5532 on your local machine to connect to the remote database, for example with psql:\npsql postgresql://my_db_user@localhost:5432/my_db_name Or dump the database with pg_dump:\npg_dump -Fc -v --dbname=postgresql://my_db_user@localhost:5432/my_db_name -f my_db_name$(date --iso-8601).pq Note: both commands above don\u0026rsquo;t specify the database password, to make it work, the password can be specified in the ~/.pgpass file (so it will not be present in the shell history or visible on the screen when the command is executed).\nThe ~/.pgpass looks like this:\nlocalhost:5432:*:my_db_user_one:XXXYYY localhost:5432:*:my_db_user_two:XXXYYY Example: Connect to Redis on AWS Similarly to the PostgreSQL example above, we can create an ssh tunnel to the EC2 instance that has Redis access and expose Redis locally:\nssh my-ec2-instance -L 6379:id.wssxxx.0001.appp1.cache.amazonaws.com:6379 After that, we can use redis-cli or any other tool on the local machine to connect to redis on 6379 port.\nExample: Expose Local Server Through the EC2 Instance Sometimes it can be necessary to make locally running app available through the Internet. There are several ways to do that:\nIf you have the \u0026ldquo;real\u0026rdquo; IP address (you may need to ask your Internet provider to setup a real IP for you), you only need to make sure that your firewall allows connections to the port your app is running on Use service like http://ngrok.com which will do the forwarding from the Internet to your local machine Use SSH tunnel to the EC2 instance (or any other machine that\u0026rsquo;s accessible from the Internet) Let\u0026rsquo;s say there is an app running locally on 8888 port (localhost:8888) and we want to make it available via the EC2 instance ec2-55-222-55-55.compute-1.amazonaws.com:8181.\nTo use the SSH tunnel, first, make sure there is a GatewayPorts yes option in the sshd config on the server. Ssh to the instance, open the /etc/sshd_config:\nsudo vim /etc/sshd_config Find and change or add GatewayPorts yes option, save the config. Restart sshd\nsudo service sshd restart Second, check and change, if necessary, the security group configuration for the EC2 instance and allow access to the port 8181.\nNow, from your local machine run\nssh my-aws-host -R *:8181:localhost:8888 Now you should be able to access your local app via ec2-55-222-55-55.compute-1.amazonaws.com:8181.\nTroubleshooting If the connection fails, check the following:\nssh connection and tunnel established successfully - check the ssh output sometimes, the connection is working, but the tunnel isn\u0026rsquo;t, in the ssh output you can see something like \u0026ldquo;Port forwarding is disabled to avoid man-in-the-middle attacks.\u0026rdquo; and the instructions on what to do you don\u0026rsquo;t have anything running locally on the specified port check with nc -v 127.0.0.1 5433 to see if there is a connection additionally check with netstat -l | grep 5433 and lsof -i :5433 Useful searches for other issues are \u0026ldquo;ssh tunnel aws rds problem\u0026rdquo; and \u0026ldquo;ssh tunnel aws rds connection error\u0026rdquo;.\nReferences The Black Magic Of SSH / SSH Can Do That?\nHow to make requests from an external server to localhost\nConnect to MongoDB on aws Server From Another Server\n","href":"/html/2018-01-11-aws-ec2-ssh-tunnel.html","title":"SSH Tunnels (How to Access AWS RDS Locally Without Exposing it to Internet)"},{"content":"I suddenly started getting the Default subnet in us-east-1f not found error during the ElasticBeanstalk environment update.\nFailed to deploy application. Updating load balancer named: awseb-e-t-AWSEBLoa-XXXXXXXXXXXXX failed Reason: Default subnet not found in us-east-1f Service:AmazonCloudFormation, Message:Stack named \u0026#39;awseb-e-xxxxxxxxxx-stack\u0026#39; aborted operation. Current state: \u0026#39;UPDATE_ROLLBACK_IN_PROGRESS\u0026#39; Reason: The following resource(s) failed to create: [AWSEBUpdateWaitConditionHandleralanC]. The following resource(s) failed to update: [AWSEBLoadBalancer]. And the similar one when trying to create the new environment:\nCreating load balancer failed Reason: Default subnet in us-east-1f not found Created CloudWatch alarm named: awseb-e-tet63me2mx-stack-AWSEBCWLAllErrorsCountAlarm-3XCPMJ1ZGJ18 Stack named \u0026#39;awseb-e-tet63me2mx-stack\u0026#39; aborted operation. Current state: \u0026#39;CREATE_FAILED\u0026#39; Reason: The following resource(s) failed to create: [AWSEBLoadBalancer]. The reason seems to be that new us-east-1f availablity zone was added, but the subnet for it wasn\u0026rsquo;t configured (not sure why and if it supposed to be configured automatically).\nThe solution is to create the subnet manually:\nOpen VPC - Subnets Click \u0026ldquo;Create Subnet\u0026rdquo; Select the \u0026ldquo;Availability Zone\u0026rdquo; - us-east-1f It is neccessary to also specify the \u0026ldquo;IPv4 CIDR block\u0026rdquo; - in my case I already had 5 subnets with IP blocks: 172.31.0.0/20 172.31.16.0/20 172.31.48.0/20 172.31.32.0/20 172.31.64.0/20 I specified the next block - 172.31.80.0/20 ElasticBeanstalk updates started working after that.\n","href":"/html/2017-06-28-aws-default-subnet-in-us-east-1f-not-found.html","title":"AWS error - Default subnet in us-east-1f not found"},{"content":"The main purpose of this workflow is to have a reliable, but simple to use git workflow. It is simple enough to be used by git beginners and minimizes possibility of mistakes (comparing to advanced flows which use rebase and related git features to achieve clean history).\nThe main idea of this workflow is that we create a new branch for every task and one developer works on this branch until the task is finished. Once done, the work from the branch is merged back to the master branch.\nSpecial branches are master, staging and production:\nmaster - integration branch, it is never modified directly, we only merge finished work from tasks branches staging - branch to test changes before we update production, it is never modified directly, only periodically updated from the master branch production - production state, it is never modified directly, only updated from the staging before we deploy new version to the production ... o ---- o ---- o --- o - ... ----------------------- o ---- production / ... o ---- o ---- o --- o - ... ----------- o - ... - o ------ staging / ... -- o -------------- o --------------- o --- ... ---------- master \\ / \\ / o ---- o ---- o o --- o ----- o (task 1 branch) (task 2 branch) Task branches should be short-living, each task should take a few hours or, at maximum, a few days to complete. So usually should be not necessary to merge updates back from master to the task branch.\nGeneral git guidelines:\nAlways write meaningful commit messages, never leave them empty Commit often Give meaningful names to branches Always check git command output to make sure there are no errors The Simple Git Workflow 1) Select a Task to Work on I assume that you have the task description in some project management like Redmine or Trello.\n2) Check If Working Copy Is Clean Check if there are no uncommited changes:\n$ git status On branch master Your branch is up-to-date with \u0026#39;origin/some_branch\u0026#39;. nothing to commit, working directory clean Note: if there are uncommited changes, you can try to go on with following steps; in the case when switching to another branch is not safe, git will stop with error message (you changes will not be lost). An alternative is to use the git stash command to temporary save your changes into the special stash area, you can restore these changes later with git stash pop.\n3) Switch to the Master Branch and Update the Code $ git checkout master $ git pull remote: Counting objects: 1, done. remote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 1 Unpacking objects: 100% (1/1), done. From github.com:tapway/tapway-data f3ba979..68822fb master -\u0026gt; origin/master Updating f3ba979..68822fb Fast-forward docs/api.md | 5 +++++ 1 file changed, 5 insertions(+) Note: It is not necessary to do git checkout master if you are already on master, but if you are a beginner, it is better to stick to the defined steps to avoid mistakes.\nIf there are no remote code changes, the output will look like this:\n$ git pull Already up-to-date. 4) Create the New Branch for the New Task I prefer XXXX-my-task-description naming convention for branches, where XXXX is a task ID in the project management system and my-task-description is a short task definition. For example, 1234-user-login (a task to add user login page) or 2345-fix-facebook-signup (a task to fix issue with Facebook sign-up).\n$ git checkout -b XXXX-my-task-description Note: it is safe to create the new branch, even if you already have some changes.\nPush the new branch to the server and track (-u) changes between local and remote branches:\n$ git push -u origin HEAD 5) Work on the Task Do some changes, check the code state:\n$ git status Add new / changed files, this will add all the changed and new files:\n$ git add . To add only specific files, use git add file_name.ext command.\nIf there are some files you don\u0026rsquo;t want to add under git control permanently, create or update the .gitignore file and put file names or file patterns to ignore into it.\n6) Commit Your Changes and Push to the Remote Repository Check that changes are staged for commit:\n$ git status On branch my-task Your branch is up-to-date with \u0026#39;origin/my-task\u0026#39;. Changes to be committed: (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: content/2016-07-03-simple-git-workflow.md modified: index.html modified: sitemap.xml Commit the changes:\ngit commit -m \u0026#34;The description of the changes made\u0026#34; At the moment you added your changes to the local repository.\nNow push these changes to the remote repository:\n$ git push origin HEAD 7) Repeat Steps 5-6 as You Work on the Task Do as many commits and pushes as you need while working on the task.\nHow to Merge Finished Work to Master Branch with github.com When the task is done, we need to merge it back to master branch. This can be done using github pull request feature:\nEach feature is developed on the separate branch (as described above) Once branch is finished the developer creates pull request (in the github.com UI): Make sure you committed and pushed all changes to github Make sure the automatic tests suite (you really should have it) has passed successfully Select the branch you work on (dropdown at the top) Click the green Compare, review, create a pull request button (on the left of the branches dropdown) Review changes and if everything looks ok click Create pull request button Note: at this point changes are not merged yet. Now we have a pull request which can be reviewed by other developers and discussed. It is possible to update it with some fixes (just commit some changes on the same branch and push them) When the pull request is approved it can be merged to master branch: Click Merge pull requests button and confirm merge Note: github will not allow to merge the branch if there are conflicts. In this case you need to merge changes from master branch to you current branch, fix the conflicts and then push your branch to the server again. Similar technique can be used not only with github, but with other git servers too (for example, bitbucket).\nHow to Merge Finished Work to Master Branch Manually Once the work is finished on the branch, the change can be reviewed manually.\nMake sure the automatic tests suite has passed successfully on the branch, check that everything is commited and pushed: $ git status Switch to the master branch and update it: $ git checkout master $ git pull Merge the task branch $ git merge my-task-description If there are conflicts between the task branch changes and master branch changes, git will show the notification and suggest to fix the conflicts. I recommend using kdiff3 as a merge tool. Install it and run git mergetool to review and resolve conflicts.\nPush changes to the server $ git push origin head How to Update staging / production Branches The staging branch is a code state which is a candidate for next production update. This branch is only updated from master branch:\n# checkout master and pull remote changes if any $ git checkout master $ git pull # checkout staging and pull remote changes if any $ git checkout staging $ git pull # merge master changes to staging $ git merge master $ git push origin HEAD Once testing on the staging branch is done, it can be merged to the production branch:\n# checkout staging and pull remote changes if any $ git checkout staging $ git pull # checkout production and pull remote changes if any $ git checkout production $ git pull # merge staging changes to production $ git merge staging $ git push origin HEAD Alternative git Scenarios Work on the Branch Created by Someone Else Note: while it is possible for several developers to work on the same branch at the same time, but it is better to avoid this and create separate branch for each developer.\nSo this is for the case when someone created the branch, but did not finish the work and passed the task to you. This also can be useful to review other developer\u0026rsquo;s work locally.\n# check current state, make sure everything is commited $ git status # get recent changes from the server $ git pull # check out and track remote branch created by someone else $ git checkout -t origin/the-branch-name # work on the branch as described in the main scenario Merge Changes from Another Branch Sometimes you may need to merge code from some other branch.\nFor example to stay in sync with master branch or to get changes from some other branch which is not finished yet.\nUsually it is better to avoid such merges because this makes your current branch history confusing. It is not clear from the first sight what was done on the current branch and what was merged from other branches, so it complicates the code review once the work is finished.\nHere we merge changes from master to our current task branch:\n# check current state, make sure everything is commited $ git status # check out the branch you want to merge from (for example, master) $ git checkout master # get recent changes from the server $ git pull # switch back to your branch $ git checkout my-branch-name # merge changes from master to your branch $ git merge master ","href":"/html/2016-07-03-simple-git-workflow.html","title":"Simple Git Workflow"},{"content":"","href":"/tags/docker/","title":"docker"},{"content":"","href":"/tags/scaleway/","title":"scaleway"},{"content":"The purpose of this setup is:\nSetup multiple web apps with different dependencies on the same server Link all apps to the same MySQL server Manage uploaded files for web apps in the single place (so it is easy to backup them) Automatically deploy and update apps on the remote server Run the same setup locally, so development environment is very close to production Setup backups for MySQL databases and for uploaded files In this case I deploy to Scaleway, but same approach can be used for almost any cloud service.\nCloud Server First, you need the cloud server with one of operation systems, supported by Docker Machine with SSH access. I used Scaleway\u0026rsquo;s VC1S server with Ubuntu 14.04. You also need to install Docker Engine and Docker Machine locally.\nDocker Machine - setup Docker remotely Next step is to setup the Docker on the server, this is done with docker-machine create command:\nSSH_HOST=111.222.333.44 SSH_USER=root SSH_KEY=~/.ssh/id_rsa_scaleway docker-machine create --driver generic \\ --generic-ip-address $SSH_HOST \\ --generic-ssh-user $SSH_USER \\ --generic-ssh-key $SSH_KEY \\ scaleway To make it work, it is necessary to open the 2375 port on the server. On Scaleway it is done via security group configuration. After changing the security group, it is necessary to reboot the server (stop / run via Archive option or Hard reboot).\nHere I have used the generic docker machine driver, there are also specific drivers for popular cloud providers - AWS, Digital Ocean, etc. Note: Scaleway has the Docker Machine plugin, using it you can do even more and automatically launch the new instance during the setup.\nCheck the full setup script here, on Scaleway I also had to create loopback devices because docker setup failed with [graphdriver] prior storage driver \\\u0026quot;devicemapper\\\u0026quot; failed: loopback attach failed.\nIf something goes wrong during the setup, run docker-machine rm scaleway, fix the problem and run the setup again.\nDeployment setup This project is responsible for deployment of the web applications to the remote host.\nProjects layout There are several web applications which I want do deploy, each packaged into the docker container.\nOn the filesystem they reside in the same ~/web folder along with the web-deploy project which acts as a glue and setups everything together along with MySQL container (used by all projects) and HAProxy (listens to the port 80 and forwards requests to individual containers):\nin ~/web $ projecta.com/ Dockerfile config/ db/ www/ projectb.com/ Dockerfile config/ db/ www/ projectc.com/ Dockerfile app/ config/ db/ ... web-deploy/ The typical Dockerfile for the php application looks like this:\nFROM php:5.6-apache RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y msmtp wget \u0026amp;\u0026amp; \\ apt-get clean RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libmcrypt-dev \\ libpng12-dev \\ \u0026amp;\u0026amp; docker-php-ext-install -j$(nproc) iconv mcrypt \\ \u0026amp;\u0026amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\ \u0026amp;\u0026amp; docker-php-ext-install -j$(nproc) gd pdo pdo_mysql mysql mysqli COPY config/msmtprc /etc/msmtprc RUN echo \u0026#39;sendmail_path = \u0026#34;/usr/bin/msmtp -t -i\u0026#34;\u0026#39; \u0026gt; /usr/local/etc/php/conf.d/mail.ini RUN a2enmod expires \u0026amp;\u0026amp; a2enmod rewrite COPY www/ /var/www/html/ VOLUME /var/www/html/files This container is based on the official php image. To make the php mail function work, I also setup msmtp and configure php to use it. The example of the msmtp configuration file is here.\nOn Scaleway by default SMTP ports are disabled. To make emails work, it is necessary to configure the security group (switch \u0026ldquo;Block SMTP\u0026rdquo; from On to Off). After changing the security group, server should be rebooted (stop / run via Archive option or Hard reboot).\nHere is also an example of the ruby-on-rails application (Redmine) Dockerfile:\n# re-use image which we already use FROM php:5.6-apache MAINTAINER serebrov@gmail.com RUN DEBIAN_FRONTEND=noninteractive apt-get update \\ \u0026amp;\u0026amp; apt-get install -y ruby libruby \\ libapache2-mod-passenger ruby-dev \\ libmysqlclient-dev libmagickcore-dev libmagickwand-dev \\ build-essential \\ apache2 ruby-passenger \\ \u0026amp;\u0026amp; gem install bundler WORKDIR /var/www/redmine/ COPY ./ /var/www/redmine/ COPY config_prod/*.yml /var/www/redmine/config/ RUN bundle install COPY config_prod/redmine.conf /etc/apache2/sites-available RUN chown -R www-data:www-data /var/www/redmine RUN a2enmod passenger RUN a2ensite redmine VOLUME /var/www/redmine/files This container is based on the same base official php container as php applications just to reuse the already downloaded layers. Ruby application is running under apache with mod passenger.\nThe web deploy project The web-deploy project is a glue to build and start the containers for all web projects, link them to mysql if necessary and setup the HAProxy to forward requests to each sub-project.\nin ~/web $ projecta.com/ projectb.com/ projectc.com/ ... web-deploy/ deploy.sh init-docker.sh init-db-files.sh mysql-cli.sh utils.sh /haproxy Dockerfile build.sh haproxy.cfg mycompany.pem /mysql dumps/ load-dumps.sh projecta.sql projectb.sql projectc.sql Dockerfile build.sh init-db.sh /build projecta.com.sh projectb.com.sh projectc.com.sh Top level scripts include:\nweb-deploy/deploy.sh - deploy all apps locally or to the remote instance web-deploy/init-docker.sh - use it for initial server setup (only needed once, for the new server) web-deploy/init-db-files.sh - uploads files and database dumps to remote server and then goes over dumps in mysql/dump and drops/creates databases and loads dumps, also users web-deploy/mysql/init-db.sh and web-deploy/mysql/dumps/load-dumps.sh, dump files should be under web-deploy/mysql/dumps web-deploy/mysql-cli.sh can be user to start the MySQL client for the MySQL container The deploy.sh script uses docker-machine to build and run the containers on the remote server. There are several modes it can be used it:\n./deploy.sh - deploy (or update) all projects locally ./deploy.sh local projecta.com - deploy only projecta.com locally ./deploy.sh production - deploy only projecta.com on the remote server ./deploy.sh production projecta.com - deploy only projecta.com on the remote server The script looks like this:\n#!/usr/bin/env bash set -e SCRIPT_PATH=`dirname $0` ENVIRONMENT=\u0026#34;local\u0026#34; if [[ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]]; then ENVIRONMENT=$1 else echo Environment name is not specified, using \u0026#39;local\u0026#39;. fi if [[ \u0026#34;$2\u0026#34; != \u0026#34;\u0026#34; ]]; then # deploy only the specified project # for example ./deploy.sh production projecta.com # will run ./build/projecta.com PROJECTS=$SCRIPT_PATH/build/$2.sh else # deploy all projects PROJECTS=$SCRIPT_PATH/build/*.sh fi echo \u0026#34;Deploy $PROJECTS\u0026#34; read -p \u0026#34;Do you want to continue? \u0026#34; -n 1 -r echo # (optional) move to a new line if [[ ! $REPLY =~ ^[Yy]$ ]] then exit 1 fi # add mysql and haproxy, always update them PROJECTS=\u0026#34;$SCRIPT_PATH/mysql/build.sh \u0026#34;$PROJECTS\u0026#34; $SCRIPT_PATH/haproxy/build.sh\u0026#34; if [[ \u0026#34;$ENVIRONMENT\u0026#34; == \u0026#34;production\u0026#34; ]]; then eval \u0026#34;$(docker-machine env scaleway)\u0026#34; docker-machine ip scaleway set +a export | grep DOCKER fi for project in $PROJECTS do echo \u0026#39;Project: \u0026#39; $project $project $ENVIRONMENT done if [[ \u0026#34;$ENVIRONMENT\u0026#34; == \u0026#34;production\u0026#34; ]]; then set +a export | grep DOCKER fi docker ps Internally, the deploy.sh script goes over the scripts under /build sub-folder with build scripts and executes them to create or update docker containers.\nThe build script looks like this:\n#!/usr/bin/env bash set -e SCRIPT_PATH=`dirname $0` APP_NAME=projecta CONTAINER_NAME=$APP_NAME-docker PORT=8091 source $SCRIPT_PATH/../utils.sh SRC=$SCRIPT_PATH/../../projecta.com # if we are running locally, the container will use sources from the host filesystem # (we create the volume pointing to /html) # for production, we will copy the sources into the container ENVIRONMENT=\u0026#34;local\u0026#34; if [[ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]]; then ENVIRONMENT=$1 fi ENV_DOCKER_OPTS=\u0026#34;\u0026#34; if [[ \u0026#34;$ENVIRONMENT\u0026#34; == \u0026#34;local\u0026#34; ]]; then ENV_DOCKER_OPTS=\u0026#34;-v $(realpath $SRC)/html:/var/www/html\u0026#34; fi echo \u0026#34;Environment: $ENVIRONMENT\u0026#34; # go to the source folder, where Dockerfile is cd $SRC # remove the image if it already exists docker_rm_app_image $APP_NAME # rebuild the image docker build -t $CONTAINER_NAME . # start the container docker run -d -p $PORT:80 -v /var/web/projecta.com/files:/var/www/html/data/upload --name $APP_NAME --link web-mysql:mysql --restart always $ENV_DOCKER_OPTS $CONTAINER_NAME # initially files belong to the root user of the host OS # make them available to containter\u0026#39;s www-data user docker exec $APP_NAME sh -c \u0026#39;chown -R www-data:www-data /var/www/html/files\u0026#39; Few interesting things happen here:\nThis project will use port 8091 on the host machine (can be accessed as localhost:8091), this port also will be mentioned in the HAProxy config to redirect requests to this app based on the requested domain name For local deployment the container will use source files from the host machine folder, so we can edit them directly without having to login to container, this is very convenient for local development The web application files will be stored in the volume, which is available on the host machine at /var/web/projecta.com/files The problem with permissions to the shared volume is solved by running chown from within the container (apache runs as www-data and after creation the files folder will belong to root user) This build script is used both for local and remote deployment, the remote part is handled by Docker Machine which allows to run docker commands against the remote host (this is handled in the deploy.sh script) Note: the docker_rm_app_image function from the build script is defined in the utils.sh script.\nThe web-deploy project also includes setup for HAProxy and MySQL containers.\nHAProxy setup This container (resides in the web-deploy/haproxy) runs HAProxy and it serves as main entry point on the server. Requests to port 80 are reverse-proxied to other containers. The Dockerfile is simple:\nFROM haproxy:1.5 COPY ./haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg # this is needed to setup ssl for one of projects COPY ./mycompany.pem /etc/ssl/certs/ The configuration file (web-deploy/haproxy/haproxy.cfg) looks like this:\nglobal log /dev/log local2 maxconn 2048 tune.ssl.default-dh-param 2048 #debug defaults log global option dontlognull mode http option forwardfor option http-server-close timeout connect 5000ms timeout client 50000ms timeout server 50000s stats enable stats auth haadimn:ZyXuuXYZ stats uri /haproxyStats frontend http-in bind *:80 # Define hosts based on domain names acl host_projecta hdr(host) -i projecta.com acl host_projecta hdr(host) -i www.projecta.com # this name is used for local testing acl host_projecta hdr(host) -i www.projecta.local acl host_projectb hdr(host) -i projectb.com acl host_projectb hdr(host) -i www.projectb.com # handle sub-domains (use hdr_end here, not hdr) acl host_projectc hdr_end(host) -i projectc.com # redirect projectd to https redirect scheme https if host_projectd !{ ssl_fc } ## figure out backend to use based on domainname use_backend projecta if host_projecta use_backend projectb if host_projectb use_backend projectc if host_projectc frontend https-in bind *:443 ssl crt /etc/ssl/certs/mycompany.pem # Define hosts based on domain names acl host_projectd hdr(host) -i projectd.com use_backend projectd if host_projectd backend projecta balance roundrobin server srv_pawnshop-soft-ru 127.0.0.1:8091 backend projectb balance roundrobin server srv_pawnshop-soft-kz 127.0.0.1:8092 backend projectc balance roundrobin server srv_lombard 127.0.0.1:8093 backend projectd balance roundrobin server srv_redmine 127.0.0.1:8100 And the build script (web-deploy/haproxy/build.sh) is this:\n#!/usr/bin/env bash export | grep DOCKER docker-machine ip scaleway SCRIPT_PATH=`dirname $0` APP_NAME=web-haproxy CONTAINER_NAME=web-haproxy-docker source $SCRIPT_PATH/../utils.sh pushd $SCRIPT_PATH docker_rm_app_image $APP_NAME docker build -t $CONTAINER_NAME . docker run -d --restart always --net=host -p 80:80 -p 443:443 -v /dev/log:/dev/log --name $APP_NAME $CONTAINER_NAME popd The HAProxy container is launched with --net=host option, so it can directly access all the ports exposed by other containers.\nThe HAProxy also handles SSL (HTTPS) connections. For one of projects it redirects http to https. See the config file and this post for more information.\nThe HAProxy stats are available via http://hostname.com/haproxyStats. Login and password are set in the config file (haproxy.cfg, see the stats auth line in defaults section).\nSome hints to to debug problems with HAProxy setup:\nUncomment \u0026lsquo;debug\u0026rsquo; in the config file Check \u0026lsquo;docker logs web-haproxy\u0026rsquo; Check system log (/var/log/syslog), with the configuration above the HAProxy container will log to host\u0026rsquo;s system log (see https://github.com/dockerfile/haproxy/issues/3) MySql setup The MySql container (it is under web-deploy/mysql folder) also has a simple Dockerfile:\nFROM mysql:5.6 ENV MYSQL_ROOT_PASSWORD=myrootpassword VOLUME /var/www/mysql The build script looks like this:\n#!/usr/bin/env bash SCRIPT_PATH=`dirname $0` APP_NAME=web-mysql CONTAINER_NAME=$APP_NAME-docker source $SCRIPT_PATH/../utils.sh pushd $SCRIPT_PATH docker_rm_app_image $APP_NAME docker build -t $CONTAINER_NAME . docker run -d -v /var/web/mysql-data:/var/lib/mysql --restart always --name $APP_NAME $CONTAINER_NAME popd The data folder is mapped to the host machine to /var/web/mysql-data. The app containers which need the database are linked to this container.\nUseful Docker Commands Some useful commands to view and manage Docker containers:\nInformation about docker docker info List running containers: docker ps Information about specific container docker inspect {appname} View application logs docker logs {appname} Restart application docker restart {appname} Run shell inside the container: docker exec -it {appname} bash # replace {appname} with name from docker ps Run the command inside the container (in this case - backup mysql databases): docker exec web-mysql sh -c 'exec mysqldump --all-databases -uroot -p\u0026quot;$MYSQL_ROOT_PASSWORD\u0026quot;' | gzip -9 \u0026gt; all-databases.sql.gz Cron in Docker and Backups on Scaleway There are few options to run cron with docker:\nRun cron on the host machine (cronjobs can interact with containers via docker exec ...) Run separate container with cron (cronjobs can interact with other containers via network or shared volumes) Run multiple processes inside application containers (application and cron) usind supervisor or runit, for example, see http://phusion.github.io/baseimage-docker/. Run multiple processes via CMD instruction in the Dockerfile (for example, see http://programster.blogspot.com/2014/01/docker-working-with-cronjobs.html) Here I have chosen the first option to use cron on the host machine. First, the host machine is Ubuntu 14.04, so it already has cron. Second, everything runs on the same machine and I have no plans to scale out this setup, so it was the easiest option.\nThe web-deploy/init-db-files.sh script contains code to setup cron, here is the related part:\ndocker-machine ssh scaleway apt-get install -y postfix mutt s3cmd docker-machine scp -r ./config/web-cron scaleway:/etc/cron.d/ docker-machine scp -r ./config/.s3cfg scaleway:/root The postfix and mutt are need for cron to send local emails about jobs. You can check these mails by ssh\u0026rsquo;ing to the server and running mutt. The s3cmd is used to backup databases and files from the shared storage to S3. The .s3cfg contains credentials for s3cmd, it can be generated by running s3cmd --configure.\nBackups are very important, because at the moment Scaleway does not provide highly reliable hardware (no RAID disks, see this thread). And to make a snapshot, you need to archive the instance (it takes few minutes), make the snapshot and launch the instance again, so there is a noticeable down-time period. So I used cron to backup files and databases to Amazon\u0026rsquo;s S3.\nNote: Scaleway has storage which is compatible to S3, called SIS, but at the moment it is not available (at least in my account). When I try to create bucket from the command line it returns S3 error: 400 (TooManyBuckets): You have attempted to create more buckets than allowed. And the note in the Scaleway UI states \u0026ldquo;We\u0026rsquo;re currently scaling up our SIS infrastructure to handle the load generated by all our new users. All new bucket creation are currently forbidden to preserve the service quality for current SIS users.\u0026rdquo;.\nThe cron file to perform backups looks this way:\n# In the case of problems, cron sends local email, can be checked with mutt # (it requires apt-get install postfix mutt) # Also check cron messages in /var/log/syslog # update geolite database # Note: docker exec should not have -t (tty) option, there is no tty, -i is also not needed 0 1 * * Sun root docker exec projecta /var/www/service/cron-update-geoip-db.sh # backup all databases to s3 0 5 * * * root docker exec web-mysql sh -c \u0026#39;exec mysqldump --all-databases -uroot -p\u0026#34;$MYSQL_ROOT_PASSWORD\u0026#34;\u0026#39; | gzip -9 \u0026gt; all-databases.sql.gz \u0026amp;\u0026amp; s3cmd put all-databases.sql.gz s3://myweb-backup/$(date +\\%Y-\\%m-\\%d-\\%H-\\%M-\\%S)-all-databases.sql.gz \u0026amp;\u0026amp; rm all-databases.sql.gz 0 6 * * * root s3cmd sync /var/web s3://myweb-backup/storage/ The command to backup mysql databases does few things:\nRun mysqldump inside the MySQL container via docker exec Pipe the result to gzip to archive it Put the archive to S3, add a timestamp to the file name And since all application containers map their files folders under host\u0026rsquo;s /var/web, I simply sync this folder to my S3 bucket.\nConclusion As we can see, Docker can be successfully used not only to isolate application dependencies, but, along with Docker Machine, also to deploy and update applications on the remote server.\nI don\u0026rsquo;t recommend using the approach above for serious production setups, but it can be useful for small / personal projects.\nUpdate: now it could be easier to use docker swarm mode to build the system like described above.\n","href":"/html/2016-06-10-scaleway-docker-deployment.html","title":"Setup Automatic Deployment, Updates and Backups of Multiple Web Applications with Docker on the Scaleway Server"},{"content":"","href":"/tags/oop/","title":"oop"},{"content":"According to the Wikipedia the Liskov Substitution Principle (LSP) is defined as:\nSubtype Requirement: Let f(x) be a property provable about objects x of type T. Then f(y) should be true for objects y of type S where S is a subtype of T. The basic idea - if you have an object of type T then you can also use objects of its subclasses instead of it.\nOr, in other words: the subclass should behave the same way as the base class. It can add some new features on top of the base class (that\u0026rsquo;s the purpose of inheritance, right?), but it can not break expectations about the base class behavior.\nThe expectations about the base class can include:\ninput parameters for class methods returned values of the class methods exceptions are thrown by the class methods how method calls change the object state other expectations about what the object does and how Some of these expectations can be enforced by the programming language, but some of them can only be expressed as the documentation.\nThis way to follow the LSP it is not only important to follow the coding rules, but also to use common sense and do not use the inheritance to turn the class into something completely different.\nLet\u0026rsquo;s see what rules do we need to follow in the code.\nMethods Signature Requirements Signature requirements are requirements for input argument types and return type of the class methods.\nLet\u0026rsquo;s imagine we have following class hierarchy:\n.------------. .------------. .-------------. | LiveBeing | | Animal |\u0026lt;|--------| Cat | |------------|\u0026lt;|--------|------------| |-------------| | + breeze() | | + eat() |\u0026lt;|---. | + mew() | \u0026#39;------------\u0026#39; \u0026#39;------------\u0026#39; | \u0026#39;-------------\u0026#39; | | .------------. | | Dog | \u0026#39;-----|------------| | + bark() | \u0026#39;------------\u0026#39; Here the LiveBeing is the base class which is inherited by Animal which in turn is inherited by Cat and Dog.\nI will use this hierarchy to explain the signature rules.\nCovariance (Parent -\u0026gt; Child -\u0026gt; \u0026hellip;) of return types in the subtype This rule means that the child class can override a method to return a more specific type (Cat instead of Animal).\nOf course, it can return the same type, but it can not return more generic type (like LiveBeing instead of Animal) and it can not return a completely different type (House instead of Animal).\nThis rule is easy to understand and it feels natural. Here is an example in pseudo-code:\nclass Owner Animal findPet() return new Animal() class CatOwner extends Owner Cat findPet() # Covariance - subclass returns more specific type return new Cat() class BadOwner extends Owner LiveBeing findPet() # Contravariance - breaks the rule and returns more generic type return new LiveBeing() function doAction(Owner owner) # OK for Owner, we can put an Animal object into the `animal` variable. # OK for CatOwner, we can put a Cat object into the `animal` variable. # Problem for a BadOwner object, a LiveBeing object can not use be used # the same way as Animal object. Animal animal = owner-\u0026gt;findPet(); amimal-\u0026gt;eat(); The doAction function demonstrates a possible use case. It is OK if owner is a CatOwner, because both Animal and Cat should behave the same.\nBut the BadOwner returns a LiveBeing and it is a problem. There is no guarantee that LiveBeing object behaves the same as Animal.\nFor example, if we call animal-\u0026gt;eat() this will not work for the LiveBeing (it doesn\u0026rsquo;t have such a method).\nContravariance (Child -\u0026gt; Parent -\u0026gt; \u0026hellip;) of method arguments in the subtype This means that a child class can override the method to accept a more generic argument type than the method in the base class (like accept the LiveBeing instead of Animal).\nclass Owner void feed(Animal animal) ... class GoodOwner extends Owner # Contravariance - subclass accepts more generic type void feed(LiveBeing being) ... class BadCatOwner extends Owner void feed(Cat cat) ... function doAction(Owner owner) owner-\u0026gt;feed(new Dog) # OK for Owner, he accepts any Animal, including the Dog # OK for GoodOwner, he accepts any LiveBeing, including the Dog # Problem for CatOwner, he doesn\u0026#39;t expect the Dog In practice, it may feel tempting to break this rule and define the class like BadCatOwner above.\nBut, as we can see, the BadCatOwner breaks LSP and we can not use it in the same case where we can use the Owner object.\nNote that although using the more generic type in the subclass is OK in terms of method signature, it may be problematic logically:\nclass Owner void feed(Animal animal) animal-\u0026gt;eat(this-\u0026gt;findFood()); class GoodOwner extends Owner void feed(LiveBeing being) # Problem: LiveBeing doesn\u0026#39;t have the `eat` method being-\u0026gt;eat(this-\u0026gt;findFood()); There is a problem here - the GoodOnwer::feed can not call the being-\u0026gt;eat() method because LiveBeing doesn\u0026rsquo;t have the eat method.\nAnd this way, GoodOwner also can not just forward the execution to the parent method with something like parent::feed(being).\nBy the way, if the method doesn\u0026rsquo;t use parent implementation, it may indicate the LSP violation - potentially we can have a different behavior for this subtype than in the parent class.\nExceptions should be same or subtypes of the base method exceptions No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exceptions thrown by the methods of the parent type.\nclass BadFoodException class BadCatFoodException extends BadFoodException class LowQualityFoodException class Owner void feed(Animal animal, Food food) if (not this-\u0026gt;isGoodFood(food)) throw BadFoodException() ... class BadOwner extends Owner void feed(Animal animal, Food food) if (not this-\u0026gt;isHighQualityFood(food)) throw LowQualityFoodException() ... function doAction(Owner owner) try owner-\u0026gt;feed(new Dog, new SomeFood) catch (BadFoodException error) # OK for Owner, it can raise BadFoodException # OK for CatOwner, it can raise BadCatFoodException (subclass of BadFoodException) # Problem for BadOwner, it can raise LowQualityFoodException and it will not be # caught here ... If we don\u0026rsquo;t follow the rule about exception types, the client code written for the base class Owner will fail for the subclass BadOwner and this way we violate the LSP.\nInheritance requirements These requirements describe additional rules for inherited methods related to the Design by contract concept. It defines the \u0026ldquo;contract\u0026rdquo; for each method which includes preconditions, postconditions and invariants:\nPrecondition is a condition or predicate that must always be true just prior to the execution of some section of code. Postcondition is a condition or predicate that must always be true just after the execution of some section of code Invariant is a condition that can be relied upon to be true during execution of a program, or during some portion of it. For example, a loop invariant is a condition that is true at the beginning and end of every execution of a loop. Preconditions cannot be strengthened in a subtype In most cases preconditions are expectations about method input arguments, also an object\u0026rsquo;s internal state can be a part of the precondition.\nThis is a more generic rule of the contravariance rule for method arguments. The contravariance rule says that subclass can accept more generic argument type (LiveBeing instead of Animal), this is a weaker precondition (subclass accepts a wider range of arguments).\nThe same logic applies not only to the types of arguments but to the other kind of expectations as well, such as a range of the integer argument:\nclass The24Hours void setHour(int hour) # hour should be between 0 and 23 assert (0 \u0026lt;= hour and hour \u0026lt;=23) ... class TheDay extends The24Hours void setHour (int hour) # breaks the rule and strengthens the precondition # day hour should be between 8 and 16 assert (8 \u0026lt;= hour and hour \u0026lt;= 16) ... function doAction(The24Hours hours) hours-\u0026gt;setHour(3); # OK for `The24Hours` object # Problem for `TheDay` - it will raise an error So the stronger precondition in the child class broke the client code which worked for the parent class.\nAt the same time, if we make the precondition weaker (or even remove it), the client code will work the same way as for parent class.\nPostconditions cannot be weakened in a subtype Postconditions are usually expectations related to the method return value.\nAgain, this the more generic rule similar to the covariance rule (method can return Cat instead of Animal), the postcondition is strengthened.\nAn example of postcondition rule violation:\nclass The24Hours number setHour(number hour) ... assert (this.hour is integer) return this.hour class TheTime extends The24Hours number setHour(number hour, number hourFraction) this.hour = hour + hourFraction / 100 # the postcondition is weaker (float is a wider area than integer) assert (this.hour is float) return this.hour function doAction(The24Hours hours) int result = hours-\u0026gt;setHour(3); # OK for The24Hours # Problem for TheTime, it returns float So again, due to LSP violation, we can not use the child class instead of the parent.\nInvariants of the parent type must be preserved in a subtype Invariant is something that is not changed during the method execution. It can be the whole or part of the object internal state:\nclass The24Hours # invariant: this.hour is not changed number getHour() return this.hour class TheCounter extends The24Hours # invariant violation: this.hour is changed number getHour() result = this.hour this.hour += 1 return result function doAction(The24Hours hours) if (hours-\u0026gt;getHour() \u0026lt;= 12) # OK for The24Hours # Problem for TheTime, now getHour() can return value \u0026gt; 12 print \u0026#39;First half of the day\u0026#39;, hours-\u0026gt;getHour() History constraint (the \u0026ldquo;history rule\u0026rdquo;) The subtypes should not introduce new methods that will allow modifying the object state in a way that is not possible for the parent class.\nThe internal object state should be modifiable only through their methods (encapsulation) and the client code can have some expectations as of the possible ways to modify the internal state.\nFor example:\nclass Time # it is immutable, once time is set, there is no way to change it constructor(int hour, int minute) getTime() class FlexibleTime extends Time # violates the \u0026#34;history\u0026#34; rule # it allows changing the object state # but the clients who use Time can be broken due to this setTime(int hour, int minute) doAction(Time time) print \u0026#39;Now it is: \u0026#39;, time-\u0026gt;getTime() doOtherAction(time) # OK for Time, it can not be changed, so value is the same # Problem for FlexibleTime, the `doOtherAction` could change it print \u0026#39;Now it is still: \u0026#39;, time-\u0026gt;getTime() Links Wikipedia:Liskov substitution principle\nAgile Design Principles: The Liskov Substitution Principle\nWikipedia: Covariance and contravariance\n","href":"/html/2016-02-18-oop-solid-l-liskov-substitution-principle.md","title":"OOP SOLID Principles \"L\" - Liskov Substitution Principle"},{"content":"Today I had a problem with PostgreSQL connection, both my application and psql tool returned an error:\nFATAL: remaining connection slots are reserved for non-replication superuser connections The PostgreSQL server was running on the db.t1.micro RDS instance and the \u0026lsquo;Current activity\u0026rsquo; column showed \u0026lsquo;22 connections\u0026rsquo; and a red line which should represent a connection limit was far away from the 22 value.\nHere is how it looked:\n.\nAnd this connection information is actually misleading - it shows 22 connections and it looks like around 30% consumed. While actually we already at 100% of connections.\nAfter some time, when the database load went down, I was able to login with psql to check max_connections parameter:\ntemplate1=\u0026gt; show max_connections; max_connections ----------------- 26 (1 row) So we have 26 max connections and, as stated in comments (thanks, Johannes Schickling), there are also 3 connections reserved to superuser.\nThat means we used 25 connections (22 + 3 reserved) out of 26.\nThe max_connection setting can also be found in the RDS UI, under \u0026ldquo;Parameter Groups\u0026rdquo;, but the value there is set as {DBInstanceClassMemory/31457280}, so it is complex to be sure about the actual value. The db.t1.micro instance has 1GB memory, and calculation like 1024*1024*1024/31457280 gives around 36, while actually it is 26.\nThe default value can be changed this way:\ncreate a new parameter group using the default group as parent and then edit the max_connection value once done - go to the db instance settings (Modify action) and set the new parameter group. But for me it seems to be dangerous solution to use in production, because more connections will consume more memory and instead of connection limit errors you can end up with out of memory errors.\nSo I decided to change the instance type to db.t2.small, it costs twice more, but it has 2GB RAM and default max_connections value is 60.\nIn my case the higher connection consumption was not something unexpected, the Elastic Beanstalk scaled the application to 6 EC2 instances under load. Each instance runs a flask application with several threads, so they can easily consume around 20 connections. To actually make sure that everything is OK with connections handling, run the following query:\ntemplate1=\u0026gt; select * from pg_stat_activity; datid | datname | ... | backend_start | ... | query_start | state_change | waiting | state | ... | query -------+----------+-----+---------------------+-----+---------------------+---------------------+---------+-------+-----+-------------------------- ... 16395 | myapp | ... | 2015-09-22 16:13:04 | ... | 2015-09-22 17:07:13 | 2015-09-22 17:07:13 | f | idle | ... | COMMIT 16395 | myapp | ... | 2015-09-22 16:14:13 | ... | 2015-09-22 17:07:21 | 2015-09-22 17:07:21 | f | idle | ... | ROLLBACK 16395 | myapp | ... | 2015-09-22 16:14:13 | ... | 2015-09-22 17:07:14 | 2015-09-22 17:07:14 | f | idle | ... | COMMIT 16395 | myapp | ... | 2015-09-22 16:14:13 | ... | 2015-09-22 17:07:18 | 2015-09-22 17:07:18 | f | idle | ... | ROLLBACK ... (16 rows) Here you can see all the connections, queries and last activity time for each connection, so it should be easy to see if there are any hanging connections.\nLinks Stackoverflow: Heroku “psql: FATAL: remaining connection slots are reserved for non-replication superuser connections”\nStackoverflow: Amazon RDS (postgres) connection limit?\nStackoverflow: Flask unittest and sqlalchemy using all connections\nStackoverflow: PostgreSQL ERROR: no more connections allowed\n","href":"/html/2015-09-22-aws-postgresql-max-connections.html","title":"AWS PostgreSQL RDS - remaining connection slots are reserved error"},{"content":"","href":"/tags/postgresql/","title":"postgresql"},{"content":"","href":"/tags/rds/","title":"rds"},{"content":"","href":"/tags/drone/","title":"drone"},{"content":"","href":"/tags/eb/","title":"eb"},{"content":"Drone CI is a Continuous Integration platform. It uses Docker containers to run tests for your application hosted on github.\nIt not complex to set up the automatic testing for your application and run Drone CI on EC2 instance using Elastic Beanstalk. It is even not necessary to have a dedicated EC2 instance for CI system, for example, I run it on the staging server.\nDrone CI setup First you\u0026rsquo;ll need to create a drone configuration file, .drone.yml, which looks like this:\nimage: serebrov/centos-python2.7-java env: - GOPATH=/var/cache/drone script: - pip install -r requirements.txt - ./scripts/ci_test.sh notify: email: recipients: - mymail@gmail.com As you can see the setup is really simple. I have a custom docker image which installs all the application requirements. Then drone runs pip install -r requirements.txt to python app dependencies and runs ci_test.sh shell script to launch testing. This script looks like this:\n#!/bin/bash # set -e SCRIPT_PATH=`dirname $0` PYTHON=python if which python27; then PYTHON=python27 fi cd $SCRIPT_PATH/.. $PYTHON -m unittest discover -s tests RESULT_MOCK=$? pushd $SCRIPT_PATH/../tests/dynamodb-local java -Djava.library.path=./DynamoDBLocal_lib -jar ./DynamoDBLocal.jar -inMemory -sharedDb \u0026amp; PID=$! popd echo \u0026#34;Started local dynamodb: $PID\u0026#34; USE_MOCK= $PYTHON -m unittest discover -s tests RESULT_LOCALDB=$? kill -9 $PID exit $(($RESULT_MOCK+$RESULT_LOCALDB)) My application uses Amazon DynamoDB, and I run tests twice - first using a simple in-memory DynamoDB mock and one more time using local DynamoDB.\nMock is used during development and tests run very fast (few seconds) and local DynamoDB is much slower and we need few minutes to run tests, but we can catch some specific errors related to the database usage.\nElastic Beanstalk setup Elastic Beanstalk setup allows to automatically install Drone CI when new EC2 instance is launched.\nHere it is good to have a single-instance Elastic Beanstalk environment and scripts below will install Drone on the EC instance.\nFirst, the EB config (.ebextensions/app.config):\ncontainer_commands: 004-start-container-commands: command: logger \u0026#34;Start deploy script\u0026#34; -t \u0026#34;DEPLOY\u0026#34; 005-command: command: chmod +x .ebextensions/deploy.sh 006-deploy: command: .ebextensions/deploy.sh 2\u0026gt;\u0026amp;1 | /usr/bin/logger -t \u0026#34;DEPLOY\u0026#34; ; test ${PIPESTATUS[0]} -eq 0 010-start-container-commands: command: logger \u0026#34;Start container commands\u0026#34; -t \u0026#34;DEPLOY\u0026#34; 190-clearcaches: command: (echo \u0026#39;flush_all\u0026#39; | nc localhost 11211) 2\u0026gt;\u0026amp;1 |/usr/bin/logger -t \u0026#34;DEPLOY\u0026#34; 200-end-container-commands: command: logger \u0026#34;End container commands\u0026#34; -t \u0026#34;DEPLOY\u0026#34; packages: yum: gcc: [] gcc-c++: [] docker: [] python-devel: [] atlas-sse3-devel: [] lapack-devel: [] services: sysvinit: docker: enabled: true ensureRunning: true It will install packages I need (including docker) and run the deploy.sh script which does the Drone installation:\n#!/usr/bin/env bash set -e SCRIPT_PATH=`dirname $0` if [[ $APP_ENV != \u0026#34;staging\u0026#34; ]]; then echo \u0026#39;Not a staging env, exit without installation\u0026#39; exit 0 fi copy_ext $SCRIPT_PATH/files/droned.conf /etc/init/droned.conf 0755 root root echo \u0026#34;Start drone check/install\u0026#34; if which /usr/local/bin/drone; then echo \u0026#34;Found drone, skip install\u0026#34; copy_ext $SCRIPT_PATH/files/drone.toml /etc/drone/drone.toml 0755 root root if (stop droned); then echo \u0026#39;Stopped drone\u0026#39; fi start droned else echo \u0026#34;Installing drone\u0026#34; wget downloads.drone.io/master/drone.rpm yum -y -q install drone.rpm # Create a user and group \u0026#39;droned\u0026#39;, -M == no home dir # Note: better to use separate user, but there was a permission error: # Post http:///var/run/docker.sock/v1.9/build?q=1\u0026amp;rm=1\u0026amp;t=drone-4dcf1ea3fb: dial unix /var/run/docker.sock: permission denied # so for now use sudo, would be good to use droned user (see upstart script) # probably this should help: https://docs.docker.com/installation/ubuntulinux/#create-a-docker-group adduser -M --user-group droned passwd -l droned chown -R droned:droned /var/lib/drone # # to prevent \u0026#39;sudo: sorry, you must have a tty to run sudo\u0026#39; sed -ie \u0026#39;s/Defaults.*requiretty.*/# Defaults requiretyy/gI\u0026#39; /etc/sudoers copy_ext $SCRIPT_PATH/files/drone.toml /etc/drone/drone.toml 0755 root root start droned fi This script uses utils.sh, you can find it here.\nThere are two more configuration files used in the setup process.\nFirst is droned.conf, an upstart script.\nYou may not need it if your system has systemd, but Amazon Linux I used on my instance didn\u0026rsquo;t have it and Drone raised this error:\nwhich: no systemctl in (/sbin:/bin:/usr/sbin:/usr/bin:/usr/X11R6/bin) Apr 14 11:55:44 ip-172-31-1-79 DEPLOY: Couldn\u0026#39;t find systemd to control Drone, cannot proceed. Apr 14 11:55:44 ip-172-31-1-79 DEPLOY: Open an issue and tell us about your system. added init script So I added a custom upstart script to fix this:\n#!upstart description \u0026#34;Droned upstart job\u0026#34; start on startup stop on shutdown script # custom http server settings # export DRONE_SERVER_PORT=\u0026#34;\u0026#34; # export DRONE_SERVER_SSL_KEY=\u0026#34;\u0026#34; # export DRONE_SERVER_SSL_CERT=\u0026#34;\u0026#34; # session settings # export DRONE_SESSION_SECRET=\u0026#34;\u0026#34; # export DRONE_SESSION_EXPIRES=\u0026#34;\u0026#34; # custom database settings # export DRONE_DATABASE_DRIVER=\u0026#34;\u0026#34; # export DRONE_DATABASE_DATASOURCE=\u0026#34;\u0026#34; # github configuration # export DRONE_GITHUB_CLIENT=\u0026#34;\u0026#34; # export DRONE_GITHUB_SECRET=\u0026#34;\u0026#34; # export DRONE_GITHUB_OPEN=false # email configuration # export DRONE_SMTP_HOST=\u0026#34;\u0026#34; # export DRONE_SMTP_PORT=\u0026#34;\u0026#34; # export DRONE_SMTP_FROM=\u0026#34;\u0026#34; # export DRONE_SMTP_USER=\u0026#34;\u0026#34; # export DRONE_SMTP_PASS=\u0026#34;\u0026#34; # worker nodes # these are optional. If not specified Drone will add # two worker nodes that connect to $DOCKER_HOST # export DRONE_WORKER_NODES=\u0026#34;tcp://0.0.0.0:2375,tcp://0.0.0.0:2375\u0026#34; echo $$ \u0026gt; /var/run/droned.pid # exec sudo -u droned \\ # DRONE_SERVER_PORT=$DRONE_SERVER_PORT \\ # DRONE_GIHUB_CLIENT=$DRONE_GIHUB_CLIENT \\ # /usr/local/bin/droned \u0026gt;\u0026gt; /var/log/droned.log 2\u0026gt;\u0026amp;1 # exec sudo -u droned \\ exec sudo \\ /usr/local/bin/droned --config=/etc/drone/drone.toml \u0026gt;\u0026gt; /var/log/droned.log 2\u0026gt;\u0026amp;1 end script pre-start script echo \u0026#34;[`date -u +%Y-%m-%dT%T.%3NZ`] (sys) Starting\u0026#34; \u0026gt;\u0026gt; /var/log/droned.log end script pre-stop script rm /var/run/droned.pid echo \u0026#34;[`date -u +%Y-%m-%dT%T.%3NZ`] (sys) Stopping\u0026#34; \u0026gt;\u0026gt; /var/log/droned.log end script Drone parameters can be changed via environment variables in the upstart script or you can attach these environment variables to the instance in the Elastic Beanstalk configuration (in the AWS UI). Or you can use a special drone.toml configuration file:\n# Drone configuration [server] port=\u0026#34;:8080\u0026#34; ##################################################################### # SSL configuration # # [server.ssl] # key=\u0026#34;\u0026#34; # cert=\u0026#34;\u0026#34; ##################################################################### # Assets configuration # # [server.assets] # folder=\u0026#34;\u0026#34; # [session] # secret=\u0026#34;\u0026#34; # expires=\u0026#34;\u0026#34; ##################################################################### # Database configuration, by default using SQLite3. # You can also use postgres and mysql. See the documentation # for more details. [database] driver=\u0026#34;sqlite3\u0026#34; datasource=\u0026#34;/var/lib/drone/drone.sqlite\u0026#34; [github] client=\u0026#34;xxxxxxxxxxxxxxxxxxxx\u0026#34; secret=\u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; # orgs=[] # open=true ##################################################################### # SMTP configuration for Drone. This is required if you plan # to send email notifications for build statuses. # [smtp] host=\u0026#34;email-smtp.us-east-1.amazonaws.com\u0026#34; port=\u0026#34;25\u0026#34; from=\u0026#34;myemail@gmail.com\u0026#34; user=\u0026#34;XXXXXXXXXXXXXXXXXXXX\u0026#34; pass=\u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; # [worker] # nodes=[ # \u0026#34;unix:///var/run/docker.sock\u0026#34;, # \u0026#34;unix:///var/run/docker.sock\u0026#34; # ] [worker] nodes=[ \u0026#34;unix:///var/run/docker.sock\u0026#34; ] To integrate Drone CI with github, you will need to create an application and generate client and secret tokens, see details here.\nThat\u0026rsquo;s all and now if you launch an environment with this configuration you will have Drone CI running. Access it via url like your_eb_environment_url.elasticbeanstalk.com:8080.\nSetup Drone users In the configuration above we didn\u0026rsquo;t set the option to open new user registration to drone, so random people will not see your repositories and tests. Instead you will need to explicitly invite all the people who need an access.\nFirst, login with github as the repository owner. This user will become Drone CI admin and will be able to invite other users. Now select Users in Drone menu and invite more users (using github accounts).\nIf you have problems with user setup, check this issue.\nPersist Drone CI users using custom API image We set up Drone CI to use local sqlite database, it is easy, but the problem is that when your instance is terminated and launched again you will lose all user accounts.\nSo when the setup is done the good idea is to make an image of the running instance and then set it as machine image for the Elastic Beanstalk environment:\nDo the following once your Drone is up and running and you added all the users Open EC2 instances page in AWS UI, select the instance with Drone and select an action Image -\u0026gt; Create image for it Open AMIs page and wait until image is created, get the AMI ID (like ami-d2faa5b0) Go to Elastic Beanstalk environment, Configuration, Instances and set Custom AMI ID Now if you instance is re-created all the settings will stay. The drawback is that you\u0026rsquo;ll need to repeat the process once you add more users. Alternative is to use external RDS instance with MySql as Drone database.\nHow to run build locally To run the build manually, on your local machine, use the following command (you also need to install docker and drone locally):\nsudo drone build You can also run build manually on the EC2 server if you ssh to it and run:\n# for some reason it doesn\u0026#39;t work without full path under sudo: $ sudo /usr/local/bin/drone build Custom Docker image For your drone environment, you can choose one of existing docker images, check the docker hub.\nBut custom image with additional setup steps can speed up the build process, because you don\u0026rsquo;t need to wait for dependencies install on each build.\nI use an automaited build feature of docker hub which means that I have a github repository with docker file and description.\nEvery time I push new Dockerfile version to this repository the image is re-build and becomes available on the docker hub.\nNote: if you modify a Dockerfile and have already running Drone then it will not see the change. By default image is fetched only once. To fix this - add :latest tag to the image name, trigger a build, drone will update the image. See this pull request for details.\nI didn\u0026rsquo;t try it, but probably this issue can also be fixed in a one of following ways:\nrebuild the elastic beanstalk environment, so everything will be re-built from scratch\nuse docker directly to pull the new version of the image\nI use t2.small instance to run drone in a single-instance Elastic Beanstalk environment. But take into account that a small EC2 instance can be not enough to build or re-build the image. If docker fails with can not allocate memory error (check in /var/log/docker) then it is possible to use a larger instance initially and then switch to a smaller one:\nassume that you have a running environment with t2.small instance change the instance type to t2.medium in the environment configuration save, wait until new instance is launched trigger a build by pushing some change, it will take longer than usual to initialize the new container image make sure build was successfully finished make an instance image (from the EC2 console) in the EB environment settings change type back to t2.small and point it to the new image save and wait until it launches, check if build passes on the small instance ","href":"/html/2015-07-05-elastic-beanstalk-drone-ci-setup.html","title":"How to set up Drone CI on EC2 instance via Elastic Beanstalk"},{"content":"After using CloudWatch Logs for some time I found that it is very inconvenient to have one stream per instance. The Logs UI is really complex to use - I need to remember instance names, open the log group I need and then go into each instance logs one-by-one to check them.\nA more convenient alternative is to use one stream like error_log for all instances.\nUpdate: logging to the same stream from multiple sources is not recommended and may cause duplicate records (although in my case this is fine). Check the comments for more information and thanks Sergei for pointing this out.\nTo use the single stream, we can just set a static string for stream name instead of {instance_id}, like this (see log_stream_name = error_log):\nAWSEBAutoScalingGroup: Metadata: \u0026#34;AWS::CloudFormation::Init\u0026#34;: CWLogsAgentConfigSetup: files: \u0026#34;/tmp/cwlogs/conf.d/apache-error.conf\u0026#34;: content : | [apache-error_log] file = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebErrorLogGroup\u0026#34;, \u0026#34;LogFile\u0026#34;]}` log_group_name = `{ \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebErrorLogGroup\u0026#34; }` log_stream_name = error_log datetime_format = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebErrorLogGroup\u0026#34;, \u0026#34;TimestampFormat\u0026#34;]}` mode : \u0026#34;000400\u0026#34; owner : root group : root Now we need to make sure our log records contain instance id or IP address, so we can understand which instance generated each log record.\nFortunately, python application logs in the apache\u0026rsquo;s error_log already have IP address, so nothing to do here. The log looks like this:\n[Wed Jun 17 16:02:46.944612 2015] [:error] [pid 20405] [remote 172.31.11.92:0] mod_wsgi (pid=20405): Exception occurred processing WSGI script \u0026#39;/opt/python/current/app/application.py\u0026#39;. [Wed Jun 17 16:02:46.944686 2015] [:error] [pid 20405] [remote 172.31.11.92:0] Traceback (most recent call last): [Wed Jun 17 16:02:46.944711 2015] [:error] [pid 20405] [remote 172.31.11.92:0] File \u0026#34;/opt/python/run/venv/lib/python2.7/site-packages/flask/app.py\u0026#34;, line 1701, in __call__ ... Here [remote 172.31.11.92:0] is a private instance IP.\nFor custom logs generated by application it is better to use python\u0026rsquo;s logging module where we can set a custom format and include instance id.\nHere is an example of how to get EC2 instance id or IP or any other metadata using boto:\n\u0026gt;\u0026gt;\u0026gt; from boto.utils import get_instance_metadata \u0026gt;\u0026gt;\u0026gt; print get_instance_metadata() { \u0026#39;ami-manifest-path\u0026#39;: \u0026#39;(unknown)\u0026#39;, \u0026#39;instance-type\u0026#39;: \u0026#39;t2.small\u0026#39;, \u0026#39;instance-id\u0026#39;: \u0026#39;i-977a265c\u0026#39;, \u0026#39;iam\u0026#39;: {...}, \u0026#39;local-hostname\u0026#39;: \u0026#39;ip-172-32-22-111.ap-southeast-1.compute.internal\u0026#39;, \u0026#39;network\u0026#39;: {... }, \u0026#39;hostname\u0026#39;: \u0026#39;ip-172-32-22-111.ap-southeast-1.compute.internal\u0026#39;, \u0026#39;ami-id\u0026#39;: \u0026#39;ami-44d4e414\u0026#39;, \u0026#39;instance-action\u0026#39;: \u0026#39;none\u0026#39;, \u0026#39;profile\u0026#39;: \u0026#39;default-hvm\u0026#39;, \u0026#39;reservation-id\u0026#39;: \u0026#39;r-fde77b77\u0026#39;, \u0026#39;security-groups\u0026#39;: [\u0026#39;ci-servers\u0026#39;, \u0026#39;awseb-e-3uijdzdiad-stack-AWSEBSecurityGroup-1R39VECLNK1YK\u0026#39;], \u0026#39;metrics\u0026#39;: {\u0026#39;vhostmd\u0026#39;: \u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\u0026#39;}, \u0026#39;mac\u0026#39;: \u0026#39;06:2d:22:2b:22:22\u0026#39;, \u0026#39;public-ipv4\u0026#39;: \u0026#39;52.77.99.111\u0026#39;, \u0026#39;services\u0026#39;: {\u0026#39;domain\u0026#39;: \u0026#39;amazonaws.com\u0026#39;}, \u0026#39;local-ipv4\u0026#39;: \u0026#39;172.32.22.111\u0026#39;, \u0026#39;placement\u0026#39;: {\u0026#39;availability-zone\u0026#39;: \u0026#39;ap-southeast-1b\u0026#39;}, \u0026#39;ami-launch-index\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;public-hostname\u0026#39;: \u0026#39;ec2-52-77-99-111.ap-southeast-1.compute.amazonaws.com\u0026#39;, \u0026#39;public-keys\u0026#39;: {...}, \u0026#39;block-device-mapping\u0026#39;: {\u0026#39;ami\u0026#39;: \u0026#39;/dev/xvda\u0026#39;, \u0026#39;root\u0026#39;: \u0026#39;/dev/xvda\u0026#39;} } And now here is how you can setup a logger to have instance id for log records:\nimport logging from boto.utils import get_instance_metadata # Logging configuration # Note: - `with_time` formatter contains non-standard [%(hostname)s] parameter LOGGING_CONFIG = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;formatters\u0026#39;: { \u0026#39;with_time\u0026#39;: { \u0026#39;format\u0026#39;: \u0026#39;[%(asctime)s] [%(levelname)s] [%(hostname)s] -- %(message)s\u0026#39;, \u0026#39;datefmt\u0026#39;: \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39; } }, \u0026#39;handlers\u0026#39;: { \u0026#39;sys_info\u0026#39;: { \u0026#39;class\u0026#39;: \u0026#39;logging.handlers.RotatingFileHandler\u0026#39;, \u0026#39;filename\u0026#39;: \u0026#39;log/sysinfo.log\u0026#39;, \u0026#39;formatter\u0026#39;: \u0026#39;with_time\u0026#39; }, \u0026#39;console\u0026#39;: { \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, \u0026#39;formatter\u0026#39;: \u0026#39;with_time\u0026#39; } }, \u0026#39;loggers\u0026#39;: { \u0026#39;sys_info\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;INFO\u0026#39;, \u0026#39;handlers\u0026#39;: [\u0026#39;sys_info\u0026#39;] } } } # Use `APP_ENV` environment variable or `development` by default # We assume that `development` is local and all other enviroments are # on Amazon EC2 instances APP_ENV = os.environ[\u0026#39;APP_ENV\u0026#39;] if \u0026#39;APP_ENV\u0026#39; in os.environ else \u0026#39;development\u0026#39; # A log filter class to add custom `hostname` property to log records class LogHostnameFilter(logging.Filter): def filter(self, record): if APP_ENV != \u0026#39;development\u0026#39;: meta = get_instance_metadata() record.hostname = meta[\u0026#39;instance-id\u0026#39;] else: record.hostname = \u0026#39;localhost\u0026#39; return True # Create and configure logger logging.config.dictConfig(LOGGING_CONFIG) sys_info_logger = logging.getLogger(\u0026#39;sys_info\u0026#39;) sys_info_logger.addFilter(LogHostnameFilter()) # Usage example: sys_info_logger.info(\u0026#39;Access token denied: %s\u0026#39; % access_token) Links Elastic Beanstalk - how to setup CloudWatch Logs\n","href":"/html/2015-06-17-cloudwatch-logs-single-stream.html","title":"CloudWatch Logs - how to log data from multiple instances to the single stream"},{"content":"","href":"/tags/cw-logs/","title":"cw-logs"},{"content":"CloudWatch Logs is an AWS service to collect and monitor system and application logs. On the top level setup is this:\ninstall CloudWatch agent to collect logs data and send to CloudWatch Logs service define log metric filters to extract useful data, like number of all errors or information about some specific events create alarms for metrics to get notifications about logs make sure that the instance role has permissions to push logs to CloudWatch (see comments for details about this issue) All the configuration can be done using the Elastic Beanstalk config. In this case when the new environment is launched or existing environment is updated the CloudWatch Logs setup is done automatically.\nThere is an example of the configuration in the Elastic Beanstalk docs - Using Elastic Beanstalk with Amazon CloudWatch Logs. The Setting Up CloudWatch Logs Integration with Configuration Files section briefly describes how to use config examples for different environments, but there is no detailed information about the config files. And config files are complex. It is easy to use examples as is, but it is not that easy to do the setup for own logs. I will start with the review of the Apache access log example and will show how to change it to collect data from the error log as well.\nAn example for php and python is an archive containing:\ncloudwatchlogs-apache/ cwl-setup.config eb-logs.config cwl-webrequest-metrics.config Files need to be placed under .ebextensions folder. I recommend extracting an archive into .ebextensions/cloudwatchlogs-apache. You can also put an archive file itself, but it will be not convenient to view/edit configs then.\nFirst two files (cwl-setup.config, click to preview and eb-logs.config, click to preview) are generic and can be used as is. These files will setup CloudWatch Logs agent on the instance and configure Elastic Beanstalk logs publication to S3.\nNote: you also need to enable the access for Elastic Beanstalk instances to Cloudwatch Logs: add the appropriate permission for the aws-elasticbeanstalk-ec2-role in IAM settings.\nThe last one (cwl-webrequest-metrics.config, click to preview) is an example of CloudWatch Logs setup for Apache\u0026rsquo;s access log.\nConfig file consists of several sections - \u0026lsquo;Mappings\u0026rsquo;, \u0026lsquo;Outputs\u0026rsquo;, \u0026lsquo;Resources\u0026rsquo;. There are cross-references between these sections, like filters defined in \u0026lsquo;Mappings\u0026rsquo; section are used later in \u0026lsquo;Resources\u0026rsquo; section for metric filters configuration.\nUnfortunately, there is no complete description of this config format in the Elastic Beanstalk documentation. As I understand this is actually a CloudFormation template but written in yaml (Elastic Beanstalk config format) instead of json (regular CloudFormation template format).\nGeneral template structure and information about its sections can be found here: CloudFormation - Template Anatomy.\nLet\u0026rsquo;s examine a cwl-webrequest-metrics.config file. The first section is \u0026lsquo;Mappings\u0026rsquo;, here it is:\n# Apache access log pattern: ## \u0026#34;%h %l %u %t \\\u0026#34;%r\\\u0026#34; %\u0026gt;s %b \\\u0026#34;%{Referer}i\\\u0026#34; \\\u0026#34;%{User-Agent}i\\\u0026#34;\u0026#34; ## remote_host remote_logname remote_user time_received \u0026#34;request\u0026#34; status response_size \u0026#34;referrer\u0026#34; \u0026#34;user-agent\u0026#34; Mappings: CWLogs: WebRequestLogGroup: LogFile: \u0026#34;/var/log/httpd/access_log\u0026#34; TimestampFormat: \u0026#34;%d/%b/%Y:%H:%M:%S %z\u0026#34; FilterPatterns: Http4xxMetricFilter: \u0026#34;[..., status=4*, size, referer, agent]\u0026#34; HttpNon4xxMetricFilter: \u0026#34;[..., status!=4*, size, referer, agent]\u0026#34; Http5xxMetricFilter: \u0026#34;[..., status=5*, size, referer, agent]\u0026#34; HttpNon5xxMetricFilter: \u0026#34;[..., status!=5*, size, referer, agent]\u0026#34; Here we see some definitions like log file path (LogFile), log timestamp format (TimestampFormat) and filter patterns (Http4xxMetricfilter, HttpNon4xxMetricFilter, \u0026hellip;). These definitions work as constants defined at the top of the template and are referred from other sections of the file.\nFilter patterns will be used to setup metric filters for the access log. Here we have patterns which will find all requests with 4XX response code, all requests with non 4XX code, all 5XX responses and all non 5XX responses.\nThe TimestampFormat setting is used by CloudWatch Logs agent to get timestamps for log records, so it is important to verify that format is set correctly. The timestamp format is the same as used by python\u0026rsquo;s strptime function. Some more information about the format can be found in the CloudWatch Logs agent setup file, check around the middle of the file, the description of the datetime_format parameter - there is a table with placeholders and examples.\nSince timestamp format is the same as used by python, it is easy to test it using python interpreter. Start python in command line and use code like this:\n\u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; time.strptime(\u0026#39;30/03/09 16:31:32.123\u0026#39;, \u0026#39;%d/%m/%y %H:%M:%S.%f\u0026#39;) In some cases, it can be complex to set the timestamp format because there can be just no placeholders to express the real format in the log. In this case, try to match at least part of the timestamp.\nFor example, Apache error log has a timestamp like Sun May 17 21:59:15.837463 2015. The problem is that there is a fractional part of the second in the middle, before the year and official python docs doesn\u0026rsquo;t have a placeholder for this case (edit: there is an %f for microseconds, but let\u0026rsquo;s pretend we don\u0026rsquo;t know this).\nThe pattern I used for Apache error log is %a %b %d %H:%M:%S (short weekday, short month name, day, hour:minute:second) and it matches only the start of the timestamp and does not include year. But it works good and I guess that CloudWatch agent takes the current year as default.\nNext section in the config file is Outputs:\nOutputs: WebRequestCWLogGroup: Description: \u0026#34;The name of the Cloudwatch Logs Log Group created for this environments web server access logs. You can specify this by setting the value for the environment variable: WebRequestCWLogGroup. Please note: if you update this value, then you will need to go and clear out the old cloudwatch logs group and delete it through Cloudwatch Logs.\u0026#34; Value: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34;} Here we describe CloudWatch Logs group. Log group is a top level entity, like \u0026ldquo;production-apache-errors\u0026rdquo;, \u0026ldquo;staging-syslog\u0026rdquo;, etc. Inside the group we have streams, each stream contains log records from some EC2 instance.\nNext section is Resources where we define AWS resources used in our setup:\nResources : AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup: ## Must have prefix: AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0 Type: \u0026#34;AWS::Logs::LogGroup\u0026#34; DependsOn: AWSEBBeanstalkMetadata DeletionPolicy: Retain ## this is required Properties: LogGroupName: \u0026#34;Fn::GetOptionSetting\u0026#34;: Namespace: \u0026#34;aws:elasticbeanstalk:application:environment\u0026#34; OptionName: WebRequestCWLogGroup DefaultValue: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;-\u0026#34;, [{ \u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34; }, \u0026#34;webrequests\u0026#34;]]} RetentionInDays: 14 Above is the definition of the log group resource. Notice the usage of \u0026ldquo;Fn::FunctionName\u0026rdquo; constructs, the config file also has a mini-language to refer other sections of the config or to join strings. For example, {\u0026quot;Fn::Join\u0026quot;:[\u0026quot;-\u0026quot;, [{ \u0026quot;Ref\u0026quot;:\u0026quot;AWSEBEnvironmentName\u0026quot; }, \u0026quot;webrequests\u0026quot;]]} will take Elastic Beanstalk environment name and add \u0026lsquo;-webrequests\u0026rsquo; after it, so the log group name will be \u0026ldquo;environment-webrequests\u0026rdquo;.\nOr function call like this (it is used below) {\u0026quot;Fn::FindInMap\u0026quot;:[\u0026quot;CWLogs\u0026quot;, \u0026quot;WebRequestLogGroup\u0026quot;, \u0026quot;TimestampFormat\u0026quot;]} will look up a TimestampFormat value in the Mappings config section.\nYou can find more information on functions here:\nElastic Beanstalk - Intrinsic Function Reference CloudFormation - Intrinsic Function Reference Next resource in the Resources section is an autoscaling group:\n## Register the files/log groups for monitoring AWSEBAutoScalingGroup: Metadata: \u0026#34;AWS::CloudFormation::Init\u0026#34;: CWLogsAgentConfigSetup: files: ## any .conf file put into /tmp/cwlogs/conf.d will be added to the cwlogs config (see cwl-agent.config) \u0026#34;/tmp/cwlogs/conf.d/apache-access.conf\u0026#34;: content : | [apache-access_log] file = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebRequestLogGroup\u0026#34;, \u0026#34;LogFile\u0026#34;]}` log_group_name = `{ \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; }` log_stream_name = {instance_id} datetime_format = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebRequestLogGroup\u0026#34;, \u0026#34;TimestampFormat\u0026#34;]}` mode : \u0026#34;000400\u0026#34; owner : root group : root Here we describe autoscaling group and say that during the new EC2 instance initialization (AWS::CloudFormation::Init) the /tmp/cwlogs/conf.d/apache-access.conf file should be created. This is a CloudWatch agent configuration which instructs an agent to collect data from the apache access log. We also describe a content for this file:\ncontent : | [apache-access_log] ## We take file name from the Mappings - CWLogs - WebRequestLogGroup - LogFile file = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebRequestLogGroup\u0026#34;, \u0026#34;LogFile\u0026#34;]}` ## Log group is a reference to the log group resource we defined above log_group_name = `{ \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; }` ## Log stream name is an instance id log_stream_name = {instance_id} ## date_format for cloudwatch agent is also defined in Mappings section above datetime_format = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebRequestLogGroup\u0026#34;, \u0026#34;TimestampFormat\u0026#34;]}` You can find more information about the CloudWatch agent configuration here.\nNote: it is not always convenient to have one stream per instance, see the follow up note on how to setup on stream for all instances.\nNext four resources are metric filters. These filters will extract and count messages with specific status codes from the Apache access log.\n####################################### ## Cloudwatch Logs Metric Filters AWSEBCWLHttp4xxMetricFilter : Type : \u0026#34;AWS::Logs::MetricFilter\u0026#34; Properties : LogGroupName: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; } FilterPattern : {\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;FilterPatterns\u0026#34;, \u0026#34;Http4xxMetricFilter\u0026#34;]} MetricTransformations : - MetricValue : 1 MetricNamespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} MetricName : CWLHttp4xx AWSEBCWLHttpNon4xxMetricFilter : Type : \u0026#34;AWS::Logs::MetricFilter\u0026#34; DependsOn : AWSEBCWLHttp4xxMetricFilter Properties : LogGroupName: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; } FilterPattern : {\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;FilterPatterns\u0026#34;, \u0026#34;HttpNon4xxMetricFilter\u0026#34;]} MetricTransformations : - MetricValue : 0 MetricNamespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} MetricName : CWLHttp4xx AWSEBCWLHttp5xxMetricFilter : Type : \u0026#34;AWS::Logs::MetricFilter\u0026#34; Properties : LogGroupName: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; } FilterPattern : {\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;FilterPatterns\u0026#34;, \u0026#34;Http5xxMetricFilter\u0026#34;]} MetricTransformations : - MetricValue : 1 MetricNamespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} MetricName : CWLHttp5xx AWSEBCWLHttpNon5xxMetricFilter : Type : \u0026#34;AWS::Logs::MetricFilter\u0026#34; DependsOn : AWSEBCWLHttp5xxMetricFilter Properties : LogGroupName: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebRequestLogGroup\u0026#34; } FilterPattern : {\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;FilterPatterns\u0026#34;, \u0026#34;HttpNon5xxMetricFilter\u0026#34;]} MetricTransformations : - MetricValue : 0 MetricNamespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} MetricName : CWLHttp5xx Metric filters use filter patterns defined in the Mappings section. Pattern syntax is described here and it is convenient to test patterns in the AWS console:\nOpen CloudWatch service in AWS console Click Logs on the left, select log group on the right Click Create Metric Filter button Now you can select existing log data if you already have it or just paste some text from the local log file, enter filter pattern and test it on the log data.\nFor example, entering the filter like [timestamp, type = *, ...] you can see what is taken as a timestamp. If we have log data like this:\n[Thu May 14 16:26:54.396347 2015] [suexec:notice] [pid 2631] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec) [Thu May 14 16:26:54.405887 2015] [auth_digest:notice] [pid 2631] AH01757: generating secret for digest authentication ... [Thu May 14 16:26:54.406397 2015] [lbmethod_heartbeat:notice] [pid 2631] AH02282: No slotmem from mod_heartmonitor [Thu May 14 16:26:54.407930 2015] [mpm_prefork:notice] [pid 2631] AH00163: Apache/2.4.10 (Amazon) mod_wsgi/3.5 Python/2.7.5 configured -- resuming normal operations Then the test result for a pattern [timestamp, type = *, ...] will be this:\nLine Number $timestamp $type $3 ... 1 Thu May 14 16:26:54.396347 2015 suexec:notice pid 2631 2 Thu May 14 16:26:54.405887 2015 auth_digest:notice pid 2631 3 Thu May 14 16:26:54.406397 2015 ... Here the data is considered as a space-delimited, so each word becomes a data column. Spaces can be \u0026ldquo;escaped\u0026rdquo; with square brackets, like [Thu May 07 07:03:48.655204 2015] will be considered one field.\nThis way for custom application logs it is better to use square brackets to specify log record fields. For example, it is better to use [2015-05-14 19:00:03] [WARNING] -- the warning message instead of 2015-05-14 19:00:03 WARNING -- the warning message.\nNote: in the Mappings section we also specify TimestampFormat, but it has no effect for filters and only used by CloudWatch agent.\nAnd finally we define alarms which will watch for metrics above and generate SNS notification if we have too many 5XX responses (here we count responses) or if percent of 4XX responses becomes high (here we calculate percent value of 4XX responses).\n###################################################### ## Alarms AWSEBCWLHttp5xxCountAlarm : Type : \u0026#34;AWS::CloudWatch::Alarm\u0026#34; DependsOn : AWSEBCWLHttpNon5xxMetricFilter Properties : AlarmDescription: \u0026#34;Application is returning too many 5xx responses (count too high).\u0026#34; MetricName: CWLHttp5xx Namespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} Statistic: Sum Period: 60 EvaluationPeriods: 1 Threshold: 10 ComparisonOperator: GreaterThanThreshold AlarmActions: - \u0026#34;Fn::If\u0026#34;: - SNSTopicExists - \u0026#34;Fn::FindInMap\u0026#34;: - AWSEBOptions - options - EBSNSTopicArn - { \u0026#34;Ref\u0026#34; : \u0026#34;AWS::NoValue\u0026#34; } AWSEBCWLHttp4xxPercentAlarm : Type : \u0026#34;AWS::CloudWatch::Alarm\u0026#34; DependsOn : AWSEBCWLHttpNon4xxMetricFilter Properties : AlarmDescription: \u0026#34;Application is returning too many 4xx responses (percentage too high).\u0026#34; MetricName: CWLHttp4xx Namespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} Statistic: Average Period: 60 EvaluationPeriods: 1 Threshold: 0.10 ComparisonOperator: GreaterThanThreshold AlarmActions: - \u0026#34;Fn::If\u0026#34;: - SNSTopicExists - \u0026#34;Fn::FindInMap\u0026#34;: - AWSEBOptions - options - EBSNSTopicArn - { \u0026#34;Ref\u0026#34; : \u0026#34;AWS::NoValue\u0026#34; } More information about Resources section:\nElastic Beanstalk - Customizing Environment Resources Elastic Beanstalk - Customizing AWS Resources Apache error log setup To create a CloudWatch Log configuration for another log file do the following:\nCopy cwl-webrequest-metrics.config and save under new name into the same folder In the Mappings section - change the log file path, timestamp format and filter patterns In the AWSEBAutoScalingGroup resource - change the config file name: apache-access.conf to my-log-name.conf and change [apache-access_log] section name in the content Search for webrequest and replace all occurences with appropriate name, do the case-insensetive search to change webrequest, WebRequest, etc Review filter patterns, metrics and alarms - these will be different for each log file For example, a config for apache error log can look like this (file cwl-weberror-metrics.config):\nMappings: CWLogs: WebErrorLogGroup: LogFile: \u0026#34;/var/log/httpd/error_log\u0026#34; TimestampFormat: \u0026#34;%a %b %d %H:%M:%S\u0026#34; FilterPatterns: AllErrorsFilter: \u0026#34;[timestamp, type = *error*, ...]\u0026#34; Outputs: WebErrorCWLogGroup: Description: \u0026#34;Apache error log - WebErrorCWLogGroup\u0026#34; Value: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebErrorLogGroup\u0026#34;} Resources : AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebErrorLogGroup: Type: \u0026#34;AWS::Logs::LogGroup\u0026#34; DependsOn: AWSEBBeanstalkMetadata DeletionPolicy: Retain ## this is required Properties: LogGroupName: \u0026#34;Fn::GetOptionSetting\u0026#34;: Namespace: \u0026#34;aws:elasticbeanstalk:application:environment\u0026#34; OptionName: WebErrorCWLogGroup DefaultValue: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;-\u0026#34;, [{ \u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34; }, \u0026#34;weberrors\u0026#34;]]} RetentionInDays: 14 AWSEBAutoScalingGroup: Metadata: \u0026#34;AWS::CloudFormation::Init\u0026#34;: CWLogsAgentConfigSetup: files: \u0026#34;/tmp/cwlogs/conf.d/apache-error.conf\u0026#34;: content : | [apache-error_log] file = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebErrorLogGroup\u0026#34;, \u0026#34;LogFile\u0026#34;]}` log_group_name = `{ \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebErrorLogGroup\u0026#34; }` log_stream_name = {instance_id} datetime_format = `{\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;WebErrorLogGroup\u0026#34;, \u0026#34;TimestampFormat\u0026#34;]}` mode : \u0026#34;000400\u0026#34; owner : root group : root AWSEBCWLAllErrorsFilter : Type : \u0026#34;AWS::Logs::MetricFilter\u0026#34; Properties : LogGroupName: { \u0026#34;Ref\u0026#34; : \u0026#34;AWSEBCloudWatchLogs8832c8d3f1a54c238a40e36f31ef55a0WebErrorLogGroup\u0026#34; } FilterPattern : {\u0026#34;Fn::FindInMap\u0026#34;:[\u0026#34;CWLogs\u0026#34;, \u0026#34;FilterPatterns\u0026#34;, \u0026#34;AllErrorsFilter\u0026#34;]} MetricTransformations : - MetricValue : 1 MetricNamespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} MetricName : CWLAllErrorRecords AWSEBCWLAllErrorsCountAlarm : Type : \u0026#34;AWS::CloudWatch::Alarm\u0026#34; DependsOn : AWSEBCWLAllErrorsFilter Properties : AlarmDescription: \u0026#34;Application generated an error in error_log\u0026#34; MetricName: CWLAllErrorRecords Namespace: {\u0026#34;Fn::Join\u0026#34;:[\u0026#34;/\u0026#34;, [\u0026#34;ElasticBeanstalk\u0026#34;, {\u0026#34;Ref\u0026#34;:\u0026#34;AWSEBEnvironmentName\u0026#34;}]]} Statistic: Sum Period: 60 EvaluationPeriods: 1 Threshold: 1 ComparisonOperator: GreaterThanThreshold AlarmActions: - \u0026#34;Fn::If\u0026#34;: - SNSTopicExists - \u0026#34;Fn::FindInMap\u0026#34;: - AWSEBOptions - options - EBSNSTopicArn - { \u0026#34;Ref\u0026#34; : \u0026#34;AWS::NoValue\u0026#34; } Note that metrics and alarms are optional and log data will be sent to CloudWatch Logs even if there are no metrics/alarms. But in this case, it will be necessary to review logs manually and setup metrics and alarms later if needed.\nLinks Using Elastic Beanstalk with Amazon CloudWatch Logs (+ config examples)\nAmazon CloudWatch Monitoring Scripts for Linux (sample Perl scripts that demonstrate how to produce and consume Amazon CloudWatch custom metrics)\nAWS Blog: Store and Monitor OS \u0026amp; Application Log Files with Amazon CloudWatch (manual setup, note about elastic beanstalk, information about cloud formation and ops works)\nLogs Monitoring Using AWS CloudWatch (awslogs agent setup via script + metrics / alarms from the AWS console)\nStackoverflow: What\u0026rsquo;s a good way to collect logs from Amazon EC2 instances?\nHow to use CloudWatch to generate alerts from logs?\n","href":"/html/2015-05-20-cloudwatch-setup.html","title":"Elastic Beanstalk - how to setup CloudWatch Logs"},{"content":"","href":"/tags/celery/","title":"celery"},{"content":"Elastic beanstalk python application is deployed under /opt/python/. The application is running under Apache web server.\nSource folder structure is this:\nbin httpdlaunch - a tool script to set environment variables and launch httpd bundle - dir with app source code, used during updates current - symlink to the recent source code version under bundle app - application sources env - shell script with environment variables (passed from EB environment settings) etc - supervisord config log - supervisord logs run - virtual environments Apache logs, deployment logs and system messages log are under /var/log.\nAnother important directory is /opt/elasticbeanstalk - here there are EB tool scripts and app server restart hooks.\nApache is managed by supervisord, to check the status run this command:\nsudo /usr/local/bin/supervisorctl -c /opt/python/etc/supervisord.conf status And you can restart apache like this:\nsudo /usr/local/bin/supervisorctl -c /opt/python/etc/supervisord.conf restart httpd How to launch python application manually I have a flask application and usually it is started as python application.py. To run it on the server instance you need to init virtual environment and set environment variables first:\nsource /opt/python/run/venv/bin/activate \u0026amp;\u0026amp; source /opt/python/current/env Now you can start the application manually:\ncd /opt/python/current/app python application.py Or start flask shell with flask shell.\nHow to install celery Celery is a distributed task queue.\nOur requirements are following:\nthe celery application should be launched automatically when new version is deployed to Elastic Beanstalk the celery application should be watched by supervisord and restarted in the case of failure The final project structure will be this:\n.ebextensions/ - elastic beanstal configs myapp.config - main config deploy.sh - deployment script, launched from main config utils.sh - utility functions for deployment script files/ celeryd.conf - celery app config for supervisord 99_restart_services.sh - app server restart / reload hook scripts/ celeryd - script to run celery application celery.py - celery application application.py - main application requirements.txt - project requirements for pip First, add the following line to the requirements.txt in the root folder of your project (these requirements will be installed with pip automatically):\n... Celery==3.1.17 Note that the version number can be different, 3.1.17 is just an actual version at the moment of writing.\nThen create a main celery application file. In my case, this is celery.py file in the root folder. Here I don\u0026rsquo;t describe the celery application file content - do whatever you need from celery here.\nNow modify the elasticbeanstalk config (.elasticbeanstak/myapp.config) to include the deployment script:\ncontainer_commands: 004-start-container-commands: command: logger \u0026#34;Start deploy script\u0026#34; -t \u0026#34;DEPLOY\u0026#34; 005-command: command: chmod +x .ebextensions/deploy.sh 006-deploy: command: .ebextensions/deploy.sh 2\u0026gt;\u0026amp;1 | /usr/bin/logger -t \u0026#34;DEPLOY\u0026#34; ; test ${PIPESTATUS[0]} -eq 0 200-end-container-commands: command: logger \u0026#34;End container commands\u0026#34; -t \u0026#34;DEPLOY\u0026#34; Here we configure ElasticBeanstalk to launch the custom deployment script.\nIn general I find the approach with shell script for beanstalk configuration to be much more convenient than the standard way to put shell script code to the text config file. More details about this method can be found in the great Innocuous looking Evil Devil article. Check also related posts.\nThis deployment script .ebextensions/deploy.sh:\n#!/usr/bin/env bash set -e SCRIPT_PATH=`dirname $0` source $SCRIPT_PATH/utils.sh # Check for leader: see utils.sh if is_leader; then echo \u0026#34;Start leader deploy\u0026#34; else echo \u0026#34;Start non-leader deploy\u0026#34; fi # copy celery app config copy_ext $SCRIPT_PATH/files/celeryd.conf /opt/python/etc/celeryd.conf 0755 root root # copy restart hook to different hooks folders copy_ext $SCRIPT_PATH/files/99_restart_services.sh /opt/elasticbeanstalk/hooks/appdeploy/enact/99_restart_services.sh 0755 root root copy_ext $SCRIPT_PATH/files/99_restart_services.sh /opt/elasticbeanstalk/hooks/configdeploy/enact/99_restart_services.sh 0755 root root copy_ext $SCRIPT_PATH/files/99_restart_services.sh /opt/elasticbeanstalk/hooks/restartappserver/enact/99_restart_services.sh 0755 root root # include celeryd.conf into the supervisord.conf script_add_line /opt/python/etc/supervisord.conf \u0026#34;include\u0026#34; \u0026#34;[include]\u0026#34; script_add_line /opt/python/etc/supervisord.conf \u0026#34;celeryd.conf\u0026#34; \u0026#34;files=celeryd.conf \u0026#34; # Reread the supervisord config supervisorctl -c /opt/python/etc/supervisord.conf reread # Update supervisord in cache without restarting all services supervisorctl -c /opt/python/etc/supervisord.conf update # Start/Restart celeryd through supervisord supervisorctl -c /opt/python/etc/supervisord.conf restart celeryd A small inconvenience with supervisord is that configuration for all apps managed by supervisord should be inside the main supervisord.conf file or additional configs should be included into it. So in any case we need to modify the main supervisord config. The script above does this by adding a following lines to it:\n[include] ; it is possible to include multiple files, names should be separated by space files=celeryd.conf The celeryd.conf is copied into the same folder where supervisord.conf resides.\nThe .ebextensions/utils.sh script contains additional functions used by deployment script:\n#!/usr/bin/env bash set -e SCRIPT_PATH=`dirname $0` # An error exit function error_exit() { echo \u0026#34;$1\u0026#34; 1\u0026gt;\u0026amp;2 exit 1 } # Copy + chmod + chown # copy_ext source target 0755 user:group copy_ext() { #cp + chmod + chown local source=$1 local target=$2 local permission=$3 local user=$4 local group=$5 if ! cp $source $target; then error_exit \u0026#34;Can not copy ${source} to ${target}\u0026#34; fi if ! chmod -R $permission $target; then error_exit \u0026#34;Can not do chmod ${permission} for ${target}\u0026#34; fi if ! chown $user:$group $target; then error_exit \u0026#34;Can not do chown ${user}:${group} for ${target}\u0026#34; fi echo \u0026#34;cp_ext: ${source} -\u0026gt; ${target} chmod ${permission} \u0026amp; chown ${user}:${group}\u0026#34; } is_leader() { # Check for leader: /opt/elasticbeanstalk/bin/leader-test.sh: # use as # if is_leader; then # dosmth # else # doelse # fi if [[ \u0026#34;$EB_IS_COMMAND_LEADER\u0026#34; == \u0026#34;true\u0026#34; ]]; then # to be used in if\u0026#39;s, so \u0026#39;0\u0026#39; means true (like for script exit code - 0 is success) #return 0 #more clear (true returns 0) true else # to be used in if\u0026#39;s, so \u0026#39;1\u0026#39; means false #return 1 #more clear (false returns non zero) false fi } script_add_line() { local target_file=$1 local check_text=$2 local add_text=$3 if grep -q \u0026#34;$check_text\u0026#34; \u0026#34;$target_file\u0026#34; then echo \u0026#34;Modification ${check_text} found in ${target_file}\u0026#34; else echo ${add_text} \u0026gt;\u0026gt; ${target_file} echo \u0026#34;Modification ${add_text} added to ${target_file}\u0026#34; fi } The celeryd.conf in .ebextensions/files/celeryd.conf is a supervisord config:\n[program:celeryd] command=/opt/python/current/app/scripts/celeryd directory=/opt/python/current/app user=wsgi numprocs=1 stdout_logfile=/opt/python/log/celery-worker.log stderr_logfile=/opt/python/log/celery-worker.log autostart=true autorestart=true startsecs=10 ; Need to wait for currently executing tasks to finish at shutdown. ; Increase this if you have very long running tasks. stopwaitsecs = 60 ; When resorting to send SIGKILL to the program to terminate it ; send SIGKILL to its whole process group instead, ; taking care of its children as well. killasgroup=true ; if rabbitmq is supervised, set its priority higher ; so it starts first ; priority=998 Restart hook .ebextensions/files/99_restart_services.sh restarts celery application when application server is reloaded or restarted:\n#!/bin/bash set -xe # check if we already have the celeryd service /usr/bin/supervisorctl -c /opt/python/etc/supervisord.conf status | grep celeryd if [[ $? ]]; then /usr/bin/supervisorctl -c /opt/python/etc/supervisord.conf restart celeryd fi eventHelper.py --msg \u0026#34;Application server successfully restarted.\u0026#34; --severity INFO Finally the script under scripts/celeryd is used to set environment variables, activate virtual environment and run celery application:\n#!/bin/bash source /opt/python/current/env source /opt/python/run/venv/bin/activate cd /opt/python/current/app # Note: exec is important here - this way supervisord will control # the python script and not the bash script # See also: http://sortedaffairs.tumblr.com/post/49113594655/managing-virtualenv-apps-with-supervisor # exec /opt/python/run/venv/bin/celery worker -A tasks --loglevel=INFO Actually we configured Elastic Beanstalk to run and manage an additional application. The same approach can be used for any kind of application, not necessary celery - it can be any other application even written in other than python language.\nBonus: how to install ZeroMQ libaray To install ZeroMQ python library add it to requirements.txt:\nFlask==0.9 boto==2.34.0 ... pyzmq==13.0.2 Installation process also includes a compilation phase and you can get an error like this:\ngcc: error trying to exec \u0026#39;cc1plus\u0026#39;: execvp: No such file or directory To solve this problem, add following packages into the elasticbeanstalk config:\npackages: yum: gcc: [] gcc-c++: [] ","href":"/html/2015-04-02-elastic-beanstalk-python.html","title":"Elastic Beanstalk - python application server structure and celery installation"},{"content":"","href":"/tags/zeromq/","title":"zeromq"},{"content":"Note: this post is outdated, because it is already possible to add a secondary index to the existing table (it was not possible in earlier DynamoDB versions).\nAt the moment it is not possible to add a secondary index into the existing table. This feature is announced but not yet available.\nSo the only way is to create a new table and migrate the existing data to it. This can be done using Amazon EMR.\nCreate the new table The new table can be created from the DynamoDB console or with code like this (in python / boto):\nTable.create( \u0026#39;my_table_v2\u0026#39;, schema = [HashKey(\u0026#39;my_hash\u0026#39;), RangeKey(\u0026#39;a_range\u0026#39;, data_type=NUMBER)], global_indexes = [ GlobalAllIndex(\u0026#39;SecondaryIndexName\u0026#39;, parts=[ HashKey(\u0026#39;my_another_hash\u0026#39;), RangeKey(\u0026#39;another_range\u0026#39;, data_type=NUMBER) ], throughput={\u0026#39;read\u0026#39;: 1, \u0026#39;write\u0026#39;: 3} ) ], throughput={\u0026#39;read\u0026#39;: 1, \u0026#39;write\u0026#39;: 3}) Move data from the old table to new table This is not complex to implement such data transfer also in python / boto, but if there is a lot of data the process can take long time to complete.\nIn this case it is much more convenient to use EMR for this - process is launched and monitored and all logs are recorded.\nAnother option is to use AWS Data Pipeline service, see the example of cross-region data copy. Table to table data copy can be done in a similar way.\nBut in my case the AWS Data Pipeline service was not available for the region where DynamoDB was launched, so I used EMR (and actually data pipeline also uses EMR behind the scenes).\nAmazon EMR allows to create a step of type \u0026lsquo;Hive program\u0026rsquo; where we can only specify the S3 path to the hive script to run (all other parameters are optional).\nSo we create and upload to S3 the hive script like shown below and then launch or use existing EMR cluster to transfer the data:\n-- This only removes Hive view for dynamo table, not the table itself DROP TABLE IF EXISTS my_table; CREATE EXTERNAL TABLE my_table (my_hash string, a_range bigint, my_another_hash string, another_range bigint) STORED BY \u0026#39;org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler\u0026#39; TBLPROPERTIES (\u0026#34;dynamodb.table.name\u0026#34; = \u0026#34;my_table\u0026#34;, \u0026#34;dynamodb.column.mapping\u0026#34; = \u0026#34;my_hash:my_hash,a_range:a_range,my_another_hash:my_another_hash,another_range:another_range\u0026#34;); -- The my_table_v2 should alreay exists in dynamo and have the secondary index DROP TABLE IF EXISTS my_table_v2; CREATE EXTERNAL TABLE my_table_v2 (my_hash string, a_range bigint, my_another_hash string, another_range bigint) STORED BY \u0026#39;org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler\u0026#39; TBLPROPERTIES (\u0026#34;dynamodb.table.name\u0026#34; = \u0026#34;my_table_v2\u0026#34;, \u0026#34;dynamodb.column.mapping\u0026#34; = \u0026#34;my_hash:my_hash,a_range:a_range,my_another_hash:my_another_hash,another_range:another_range\u0026#34;); -- Use 90% of read/write throughput SET dynamodb.throughput.read.percent=0.9; SET dynamodb.throughput.write.percent=0.9; -- Copy data from table to table INSERT INTO TABLE my_table_v2 SELECT * FROM my_table; View results Once the process is finished EMR will show process logs (controller, syslog, stderr, stdout). Most of the information is in the stderr log. Also note that logs are not available just when the process is finished. Just wait few minutes and logs will appear.\nLinks Amazon DynamoDB, EMR and Hive notes\nOptimizing Performance for Amazon EMR Operations in DynamoDB\nHive \u0026amp; DynamoDB Pitfalls\nStackoverflow: Amazon Elastic MapReduce - mass insert from S3 to DynamoDB is incredibly slow\n","href":"/html/2015-01-25-aws-add-secondary-index.html","title":"Amazon DynamoDB - how to add global secondary index"},{"content":"","href":"/tags/dynamodb/","title":"dynamodb"},{"content":"","href":"/tags/emr/","title":"emr"},{"content":"Setup Download and extract dynamodb local to some folder.\nLaunch it (-sharedDb allows us to connect to the same database with other tools):\n$ java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb By default it will be running on the port 8000 and will create the db file in the same directory where it was launched.\nWithout the -sharedDB parameter the DB file name depends on connection parameters, the name is {aws_access_key_id}_{region_name}.db. So different clients can use different databases.\nThere is also an -inMemory parameter to keep data in memory:\n$ java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar - inMemory -sharedDb How to load a test data and dump/restore the data The easiest way is to create some script to generate the test data. This way it is possible to run DynamoDB local in memory and populate it with test data after launch. Another way is to dump real db data and restore it into local DB. Here are some related tools to help with this and similar tasks:\ndynamodump - Simple backup and restore for Amazon DynamoDB using boto dynamodb_utils - A toolchain for Amazon\u0026rsquo;s DynamoDB to make common operations (backup, restore backups) easier DynamoDBtoCSV - Dump DynamoDB data into a CSV file and related blog post dynamo-archive - Archive and Restore DynamoDB Tables AWS DynamoDB to MongoDB Conversion DynamoDb local web shell DynamoDB local also has a web shell available via http://localhost:8000/shell. Make sure you launched the database with a -sharedDb flag.\nWithout the -sharedDb it will use {access_key}_us-west-2.db file. Access key can be set in shell settings, but the region name is hard-coded as us-west-2 (see source in chrome dev tools, /shell/jsrepl/sandbox.js file).\nAWS Explorer AWS Explorer is a toolkit for eclipse which contains the DynamoDb GUI.\nInstall it as described here.\nTo browse an application database start the DynamoDb local with the -sharedDb flag and select \u0026rsquo;local\u0026rsquo; region in the AWS regions dropdown.\nWithout the -sharedDb Eclipse will use a file like AKXAX4X6XAFXIXNIXEXA_local.db.\nTable prefixes DynamoDB has a single namespace for all tables, so it is better to use table prefixes for different environments and/or different applications.\nDynamoDB mocks and testing While tools mentioned above are good to examine the data generated by your application the simplest way to verify your code is to build a set of unit tests.\nDynamoDB Local is too slow for unit tests and it is better to use some other dynamo mock. In my current project I use a custom mock which is a simplified boto API implementation. Try to search for some native implementation of the dynamo API in your project language which will hold all the data in memory.\nAlso, a good idea is to make DB access configurable for tests so it will be easy to switch tests between mock / local db / real db.\nDynamoDB Mocks:\nAmazon DynamoDB Libraries, Mappers, and Mock Implementations Galore! ddbmock - Amazon DynamoDB mock implementation (python) and a bitbucket repository dynalite - A mock implementation of Amazon\u0026rsquo;s DynamoDB built on LevelDB ","href":"/html/2015-02-01-dynamodb-local.html","title":"Local Amazon DynamoDB - tools, dump/restore and testing"},{"content":"First you need the EMR cluster running and you should have ssh connection to the master instance like described in the getting started tutorial.\nNow it is possible to run Hive commands in few following ways:\nConnect via ssh, launch hive and run commands interactively Create a script file with commands, upload it to S3 and launch as a ERM \u0026lsquo;Hive program\u0026rsquo; step Run it from Hue web-interface (see below) Connect to Hue Hue is a Hadoop web interface. It is automatically installed when EMR cluster is launched. The recommended way to connect to Hue is via ssh tunnel, see also.\nBut there is a simpler way (but less secure) - open the 8888 port on the master EMR instance:\nopen security groups in EC2 console, find ElasticMapReduce-master and add Custom TCP Rule, Port 8888, Anywhere Check EMR master public DNS Open Hue: http://ec2-XX-XXX-XX-XXX.region-name.compute.amazonaws.com:8888/ Analyze DynamoDB data with Hive To analyze the DynamoDB data there are following options:\nCreate the external Hive table pointing to DynamoDB table and make queries against it (slow and consumes DynamoDB resources) Export data from dynamo to the native Hive table then query this data off-line Export data from dynamo to S3 and then query it, in this case queries are a bit slower than for a native Hive table, but data persists on S3, so it is possible to terminate the cluster and then launch it again when necessary Example of the script to move data from dynamo to hive native table:\n-- Here you can drop/create an external table at any time - this will not affect real data CREATE EXTERNAL TABLE dynamo_table (hash string, range bigint, data string) STORED BY \u0026#39;org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler\u0026#39; TBLPROPERTIES (\u0026#34;dynamodb.table.name\u0026#34; = \u0026#34;mytable\u0026#34;, \u0026#34;dynamodb.column.mapping\u0026#34; = \u0026#34;hash:hash,range:range,data:data\u0026#34;); CREATE TABLE hive_table (hash string, range bigint, data string); SET dynamodb.throughput.read.percent=0.9; INSERT OVERWRITE TABLE hive_table SELECT * FROM dynamo_table; Example of the script to move data from dynamo to S3:\nCREATE EXTERNAL TABLE dynamo_table (hash string, range bigint, data string) STORED BY \u0026#39;org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler\u0026#39; TBLPROPERTIES (\u0026#34;dynamodb.table.name\u0026#34; = \u0026#34;mytable\u0026#34;, \u0026#34;dynamodb.column.mapping\u0026#34; = \u0026#34;hash:hash,range:range,data:data\u0026#34;); CREATE EXTERNAL TABLE s3_table(hash string, range bigint, data string) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; LOCATION \u0026#39;s3://my-bucket-name/2015-01-24-dynamo-table-data/\u0026#39;; INSERT OVERWRITE TABLE s3_table SELECT * FROM dynamo_table; Hive - timestamp to date conversions Examples of some queries to convert the timestamp to date string:\n-- Convert timestamp to date in UTC+2 timezone select hash, stamp, from_unixtime(stamp, \u0026#39;y-M-d Hⓜ️sZ+0200\u0026#39;) from hive_table limit 10; -- Select data by date string select hash, stamp from hive_table where stamp \u0026gt; unix_timestamp(\u0026#39;2014-12-25 10:18:41+0200\u0026#39;) limit 10; Execute python script from Hive script It is possible to transform data in Hive using external python script (see here and here). And using a fake table it is possible to run the python script from the hive script:\n-- -- Run python script to create test tables on dynamo. -- Script should be uploaded to tapway-scripts/ on s3 -- CREATE TABLE IF NOT EXISTS empty_src (id string); add file s3://tapway-scripts/hive.copy.test.py; select transform (id) using \u0026#39;hive.copy.test.py\u0026#39; from empty_src; Links Hive Language Manual\nHive Operators and User-Defined Functions\nDynamoDB Guide: Hive Command Examples for Exporting, Importing, and Querying Data in DynamoDB\nEMR Guide: Export, Import, Query, and Join Tables in DynamoDB Using Amazon EMR\nOptimizing Performance for Amazon EMR Operations in DynamoDB\nUsing DynamoDB with Amazon Elastic MapReduce\nHive \u0026amp; DynamoDB Pitfalls\nStackoverflow: Amazon Elastic MapReduce - mass insert from S3 to DynamoDB is incredibly slow\nAmazon DynamoDB, Apache Hive and Leaky Abstractions\nAmazon AWS: Hive, EMR and DynamoDb\nExploring Dynamo DB\n","href":"/html/2015-01-24-aws-dynamodb-emr-hive.html","title":"Amazon DynamoDB, EMR and Hive notes"},{"content":"","href":"/tags/hive/","title":"hive"},{"content":"Below is a simple python script which performs application deployment using OpsWorks API library (boto). Script performs following steps\nExecute \u0026lsquo;update_custom_cookbooks\u0026rsquo; deployment command and wait for successful completion (or stop with an error) Execute \u0026lsquo;deploy\u0026rsquo; command and wait for completion At the top there are aws configuration parameters (aws_access_key, aws_secret_key) - these can be left empty if the script is launched on the AWS instance which has IAM role assigned.\n#!/usr/bin/python import boto.opsworks from datetime import datetime import time import sys import logging stack_id=\u0026#34;mystackid\u0026#34; # see this in the opsworks stack properties app_id=\u0026#34;myappid\u0026#34; # see this in the opsworks app properties aws_access_key = \u0026#34;my_access_key\u0026#34; aws_secret_key = \u0026#34;my_secret_key\u0026#34; ec2_region_name = \u0026#39;us-east-1\u0026#39; # your region name # see region endpoints - http://docs.aws.amazon.com/general/latest/gr/rande.html ec2_region_endpoint = \u0026#39;region_endpoint\u0026#39; def wait_for_deployment(deployment_id): print(\u0026#39;Waiting for deployment %s to complete\u0026#39; % deployment_id) while True: result = opsworks.describe_deployments(deployment_ids=[deployment_id]) data = result[\u0026#39;Deployments\u0026#39;][0] if data[\u0026#39;Status\u0026#39;] == \u0026#39;running\u0026#39;: sys.stdout.write(\u0026#39;.\u0026#39;) elif data[\u0026#39;Status\u0026#39;] == \u0026#39;successful\u0026#39;: print(\u0026#39;Done %s\u0026#39; % deployment_id) return elif data[\u0026#39;Status\u0026#39;] == \u0026#39;failed\u0026#39;: print(\u0026#39;Failed %s\u0026#39; % deployment_id) raise Exception(\u0026#39;Deployment failed\u0026#39;) else: raise Exception(\u0026#39;Unknown deployment stauts: %s\u0026#39; % data[\u0026#39;Status\u0026#39;]) sys.stdout.flush() time.sleep(3) if aws_access_key: opsworks = boto.opsworks.connect_to_region(ec2_region_name, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key) else: opsworks = boto.opsworks.connect_to_region(ec2_region_name) result = opsworks.create_deployment(stack_id, {\u0026#34;Name\u0026#34;:\u0026#34;update_custom_cookbooks\u0026#34;}, app_id) print(\u0026#39;Update custom cookbooks %s\u0026#39; % result) wait_for_deployment(result[\u0026#39;DeploymentId\u0026#39;]) result = opsworks.create_deployment(stack_id, {\u0026#34;Name\u0026#34;:\u0026#34;deploy\u0026#34;}, app_id) print(\u0026#39;Deploy %s\u0026#39; % result) wait_for_deployment(result[\u0026#39;DeploymentId\u0026#39;]) The script launches deployment command and then polls its status every three seconds until it completes or fails. Results look like this:\n$ ./deploy.py Update custom cookbooks {u\u0026#39;DeploymentId\u0026#39;: u\u0026#39;e83a0100-52b2-2fe1-ad26-1db85b62dbfb\u0026#39;} Waiting for deployment e83a0100-52b2-2fe1-ad26-1db85b62dbfb to complete ........................Done e83a0100-52b2-2fe1-ad26-1db85b62dbfb Deploy {u\u0026#39;DeploymentId\u0026#39;: u\u0026#39;85fda652-f215-25fd-b13d-e061adccf535\u0026#39;} Waiting for deployment 85fda652-f215-25fd-b13d-e061adccf535 to complete .................................Done 85fda652-f215-25fd-b13d-e061adccf535 Here is also a similar shell script. It is simpler than the python code and just launches deployments without waiting for results:\n#!/usr/bin/env bash STACK_ID=0a000a00-00a0-00a0-0000-00a00000000a APP_ID=00aa000a-aaaa-000a-0a00-a000000a0000 export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY=xXXxxxxxXXxXXxxXXxXXxxxxXxxXxxxxXXXxXxXX aws opsworks --region us-east-1 create-deployment --stack-id $STACK_ID --app-id $APP_ID --command \u0026#34;{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;update_custom_cookbooks\\\u0026#34;}\u0026#34; aws opsworks --region us-east-1 create-deployment --stack-id $STACK_ID --app-id $APP_ID --command \u0026#34;{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;deploy\\\u0026#34;}\u0026#34; Links OpsWorks cli - create-deployment command\nOpsWorks docs:Deploy an App (create-deployment)\n","href":"/html/2014-12-30-aws-opsworks-cmd-deployment.html","title":"AWS - Deployment via OpsWorks from the command line"},{"content":"I described how to setup mongodb on EC2 using OpsWorks and here is how to setup mongo data backups.\nIn my case all mongo data is stored on the same EBS volume so I just need to make a volume snapshot.\nThe relevant part from the mongodb docs:\nBackup with --journal The journal file allows for roll forward recovery. The journal files are located in the dbpath directory so will be snapshotted at the same time as the database files. If the dbpath is mapped to a single EBS volume then proceed to Backup the Database Files. If the dbpath is mapped to multiple EBS volumes, then in order to guarantee the stability of the file-system you will need to Flush and Lock the Database. NOTE Snapshotting with the journal is only possible if the journal resides on the same volume as the data files, so that one snapshot operation captures the journal state and data file state atomically. This is my case - I have all data on the same EBS volume and journal option is enabled by default for MongoDB 2.0 and higher on 64-bit systems. In other cases it is necessary to flush and lock the database. This method is supported by the ec2-consistent-snapshot tool.\nMy solution is based on the modified version of aws-snapshot-tool. I wanted a completely automated setup where I don\u0026rsquo;t need to take manual steps like assigning tags to the volumes I need to backup.\nThe process I have now does this:\nThere is a special \u0026rsquo;ec2-backup\u0026rsquo; chef recipe which I assign to the instance (or instances) which volumes I need to backup This recipe is added to the mongodb layer in OpsWorks, so every mongo instance will have it Recipes assigns \u0026lsquo;MakeSnapshot\u0026rsquo;=True tag to the instance Recipe also sets up cron jobs to perform daily, weekly and monthly backups Snapshots are created by the aws snapshot tool launched by cron Aws snapshot tool also sends results via SNS and I get backup notifications by email The recipe (ec2-backup/recipes/default.rb) looks like this:\n# see https://github.com/stuart-warren/chef-aws-tag # Assign tag to the instance include_recipe \u0026#34;aws\u0026#34; tags = { \u0026#34;MakeSnapshot\u0026#34; =\u0026gt; \u0026#34;True\u0026#34; } aws_resource_tag node[\u0026#39;ec2\u0026#39;][\u0026#39;instance_id\u0026#39;] do tags(tags) action :update end # Create directory where snapshot tool will be stored directory \u0026#34;/srv/backup\u0026#34; do owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0644\u0026#39; action :create end # Copy the snapshot tool to /srv/backup cookbook_file \u0026#34;makesnapshots.py\u0026#34; do path \u0026#34;/srv/backup/makesnapshots.py\u0026#34; action :create end # Copy the snapshot tool config to /srv/backup cookbook_file \u0026#34;config.py\u0026#34; do path \u0026#34;/srv/backup/config.py\u0026#34; action :create end # Setup a cron job for daily backups cron \u0026#34;backup-daily\u0026#34; do path \u0026#34;/usr/local/bin:$PATH\u0026#34; hour \u0026#34;1\u0026#34; minute \u0026#34;30\u0026#34; weekday \u0026#39;1-6\u0026#39; command \u0026#34;cd /srv/backup \u0026amp;\u0026amp; /usr/bin/python makesnapshots.py day 2\u0026gt;\u0026amp;1 |/usr/bin/logger -t \\\u0026#34;CRON: makenapshot\\\u0026#34;\u0026#34; end # Setup a cron job for weekly backups cron \u0026#34;backup-weekly\u0026#34; do path \u0026#34;/usr/local/bin:$PATH\u0026#34; hour \u0026#34;2\u0026#34; minute \u0026#34;30\u0026#34; weekday \u0026#39;7\u0026#39; command \u0026#34;cd /srv/backup \u0026amp;\u0026amp; /usr/bin/python makesnapshots.py week 2\u0026gt;\u0026amp;1 |/usr/bin/logger -t \\\u0026#34;CRON: makenapshot\\\u0026#34;\u0026#34; end # Setup a cron job for monthly backups cron \u0026#34;backup-monthly\u0026#34; do path \u0026#34;/usr/local/bin:$PATH\u0026#34; hour \u0026#34;3\u0026#34; minute \u0026#34;30\u0026#34; day \u0026#39;1\u0026#39; command \u0026#34;cd /srv/backup \u0026amp;\u0026amp; /usr/bin/python makesnapshots.py month 2\u0026gt;\u0026amp;1 |/usr/bin/logger -t \\\u0026#34;CRON: makenapshot\\\u0026#34;\u0026#34; end Under the recipe files folder (ec2-backup/files/default/) I have a modified copy of the aws-snapshot-tool. The recipe folder layout is this:\n├── Berksfile ├── ec2-backup │ ├── files │ │ └── default │ │ ├── config.py │ │ ├── makesnapshots.py │ ├── metadata.rb │ └── recipes │ └── default.rb The Berksfile contains dependency info (only aws is relevant to ec2-backup recipe):\nsource \u0026#39;https://supermarket.getchef.com\u0026#39; cookbook \u0026#39;mongodb\u0026#39; cookbook \u0026#39;aws\u0026#39;, \u0026#39;\u0026gt;= 0.2.4\u0026#39; My version of the makesnapshots.py is here. My modification was to allow tagging instances instead of volumes. There is already a similar pull request in the original repository so I didn\u0026rsquo;t submit my change back. There is also one new option in the config.py (see the full config example here):\n... # Set to True to use intance tags, False - volume tags \u0026#39;use_instance_tag\u0026#39;: True, ... That\u0026rsquo;s all, now I have mongo data backups with email notifications.\nLinks MongoDB documentation - backup and restore on Amazon EC2\nec2-consistent-snapshot tool\naws-snapshot-tool and related post Automated Amazon EBS volume snapshots with boto\nBash script for Automatic EBS Snapshots and Cleanup on Amazon Web Services and related post\nec2-automate-backup tool and related post and another one\nautomated-ebs-snapshots tool\nDevOps Backup in Amazon EC2 article\nStackoverflow: MongoDb EC2 EBS backups\nAutomated amazon ebs snapshot backup script with 7 day retention\nMongodb to Amazon s3 Backup Script\nOpsWorks docs - Running Cron Jobs\nChef - cron resource\nChef - file resource and cookbook_file\nChef - directory resource\n","href":"/html/2014-12-30-aws-ebs-mongo-backups.html","title":"AWS OpsWorks - setup mongodb ebs volume backups"},{"content":"","href":"/tags/mongodb/","title":"mongodb"},{"content":"Amazon OpsWorks provides a way to manage AWS resources using Chef recipes.\nHere I describe a simple setup of the single-instance node.js app with single-node MongoDB server. It is similar to the php application + mysql setup described in the OpsWorks Getting Started guide.\nThe OpsWorks setup includes:\nStack - a container for the deployment process we will setup Two Layers - node.js app and MongoDB Two Instances - one EC2 instance for node app and another for mongo One Application - this is the code we will deploy MongoDb setup is based on this blog post. We will use MongoDB Chef cookbook which allows us to deploy different MongoDB setups - single instance, replica set, sharding, replica sets with sharding. It is also possible to manage mongo users and setup MongoDB Monitoring System (MMS) agent.\nSee the detailed step-by-step guide on working with OpsWorks UI is here. I will describe only important steps and problems I had. Also the difference from the standard setup is that I actually launched two node apps on the node.js server instance - the main application and the test application (both are from the same source code repository).\nCreate a new stack, set a name and check other default parameters It is good to create and set the \u0026lsquo;Default SSH key\u0026rsquo;, so you will be able to ssh to instances later to debug problems with the setup Add Node.js App Server layer - select \u0026lsquo;Node.js App Server\u0026rsquo; as a type Add MongoDB layer - select \u0026lsquo;Custom\u0026rsquo; as a type Node.js layer setup. As mentioned above, I have two node apps which I wanted to launch on the same server - main app and test app. The main app works on the port 80 and the test app on the 3300. Repository structure is this:\n├── app ├── public ├── package.json ├── server.js - this is the main app entry point ├── ... └── test-app ├── bin/www - this is the test app entry point ├── app.js ├── ... The standard node.js layer uses chef recipes from the opsworks cookbooks repository and has some limitations:\nThe main file must be named server.js and reside in the deployed application\u0026rsquo;s root directory. Express apps must include a package.json file in the application\u0026rsquo;s root directory. The application must listen on port 80 (for HTTP requests) or port 443 (for HTTPS requests). In my case the main app already had the required layout (it is based on mean.js). I only had to setup port 80 for the production environment.\nThe test app requires a simple custom chef recipe and some additional layer settings. To use custom recipes it is necessary to create a separate cookbook repository. It can be, for example, public or private git repository on github or bitbucket. In my case I used private bitbucket repository with following structure:\n├── Berksfile ├── mongodb-singlenode │ ├── metadata.rb │ └── recipes │ └── default.rb ├── nodeapp │ ├── metadata.rb │ ├── recipes │ │ └── default.rb │ └── templates │ └── default │ └── app_test.monitrc.erb └── README.md Here I have custom recipe for mongodb setup (it is described below) and a custom recipe with template for the node app. The nodeapp/metadata.rb just contains a cookbook metadata, see example. The recipes/default.rb is this:\ndeploy = node[:deploy][\u0026#39;my_app\u0026#39;] script \u0026#34;setup_test_app\u0026#34; do interpreter \u0026#34;bash\u0026#34; user \u0026#34;root\u0026#34; cwd \u0026#34;#{deploy[:deploy_to]}/current/app-test\u0026#34; code \u0026lt;\u0026lt;-EOH npm install -d EOH end template \u0026#34;#{node.default[:monit][:conf_dir]}/node_web_app-app-test.monitrc\u0026#34; do source \u0026#39;app_test.monitrc.erb\u0026#39; owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0644\u0026#39; variables( :deploy =\u0026gt; deploy, :deploy_to =\u0026gt; \u0026#34;#{deploy[:deploy_to]}/current/app-test\u0026#34;, :test_port =\u0026gt; 3300, :test_env =\u0026gt; \u0026#39;production\u0026#39;, :application_name =\u0026gt; \u0026#39;app-test\u0026#39;, :monitored_script =\u0026gt; \u0026#34;#{deploy[:deploy_to]}/current/app-test/bin/www\u0026#34; ) notifies :restart, \u0026#34;service[monit]\u0026#34;, :immediately end This recipe invokes npm install -d inside the test app folder and then creates a monit service config. So our test app will be watched by monit and restarted in the case of failure. This is the same setup the standard OpsWorks node.js layer uses for the main application.\nTo see the status of the application run the following command:\nsudo monit status This will display status of all services watched by monit. And monit logs it\u0026rsquo;s events to syslog (/var/log/syslog).\nThe templates/default/app_test.monitrc.erb contains a config file template:\ncheck host node_web_app_\u0026lt;%= @application_name %\u0026gt; with address 127.0.0.1 start program = \u0026#34;/bin/sh -c \u0026#39;cd \u0026lt;%= @deploy_to %\u0026gt; ; source \u0026lt;%= @deploy_to %\u0026gt;/../shared/app.env ; /usr/bin/env PORT=\u0026lt;%= @test_port %\u0026gt; NODE_ENV=\u0026lt;%= @test_env %\u0026gt; NODE_PATH=\u0026lt;%= @deploy_to %\u0026gt;/node_modules:\u0026lt;%= @deploy_to %\u0026gt; /usr/local/bin/node \u0026lt;%= @monitored_script %\u0026gt;\u0026#39;\u0026#34; stop program = \u0026#34;/usr/bin/pkill -f \u0026#39;node \u0026lt;%= @monitored_script %\u0026gt;\u0026#39;\u0026#34; \u0026lt;% if @deploy[:ssl_support] -%\u0026gt; if failed port \u0026lt;%= @test_port %\u0026gt; type TCPSSL protocol HTTP \u0026lt;% else -%\u0026gt; if failed port \u0026lt;%= @test_port %\u0026gt; protocol HTTP \u0026lt;% end -%\u0026gt; request / with timeout 10 seconds then restart This way we setup monit to run and monitor the test app. App is started with command like PORT=3300 NODE_ENV=production path/to/deployment/current/app-test/bin/www.\nNow change the layer settings:\nCustom recipe For the Node.js App Server select \u0026lsquo;Recipes\u0026rsquo;, click \u0026lsquo;Edit\u0026rsquo; Set the custom recipes repository URL and add \u0026rsquo;nodeapp::default\u0026rsquo; recipe to the \u0026lsquo;Deploy\u0026rsquo; step. This way we will launch our custom recipe each time the app is deployed Network - optional step, here you may want to enable Elastic IP, so the app server will always have the same IP Security - here you need to add a custom security group to the layer to open port 3300 for our test app Open the EC2 service, Security Groups, create new security group and allow port 3300, see also docs Go back to the node app layer settings, Security tab and add a custom security group you created MongoDB layer setup The setup is based on this blog post. It uses the custom chef recipe from our cookbook repository:\n├── Berksfile ├── mongodb-singlenode │ ├── metadata.rb │ └── recipes │ └── default.rb ├── nodeapp │ ├── ... Here we have Berksfile which describes dependencies:\nsource \u0026#39;https://supermarket.getchef.com\u0026#39; cookbook \u0026#39;mongodb\u0026#39; The mongodb-singlenode/metadata.rb with recipe dependency info:\nname \u0026#34;mongodb-singlenode\u0026#34; description \u0026#39;MongoDB single node Berkshelf based install\u0026#39; maintainer \u0026#34;Company\u0026#34; license \u0026#34;Apache 2.0\u0026#34; version \u0026#34;1.0.0\u0026#34; depends \u0026#39;mongodb\u0026#39; And a custom recipe mongodb-singlenode/recipes/default.rb:\nnode.normal[:mongodb][:config][:bind_ip] = \u0026#34;127.0.0.1,#{node[:opsworks][:instance][:private_ip]}\u0026#34; include_recipe \u0026#34;mongodb::default\u0026#34; The important part here is the first string where we add instance\u0026rsquo;s private ip to the \u0026lsquo;bind_ip\u0026rsquo; parameter. This way the MongoDB instance will be available to our node.js app server.\nThe way I set the bind_ip parameter is different from the recommended because I had an issue with recommended setup. Also it is necessary to use Amazon Linux instance for mongo because there is also a problem (solvable, but I didn\u0026rsquo;t tested the solution) with ubuntu setup.\nNow open the mongo layer in the OpsWorks UI, \u0026lsquo;Recipes\u0026rsquo; settings and add our custom mongodb-singlenode::default recipe to the \u0026lsquo;Setup\u0026rsquo; step.\nFinalize the setup Now add one instance to each layer. You can do this from both Layers and Instances pages in the OpsWorks UI. I use Ubuntu 14.04 for node.js app layer and Amazon Linux 2014.09 for mongo layer.\nGive some meaningful name for the mongo instance (the Hostname parameter). This data goes to /etc/hosts on each instance.\nFor example, if the mongo host is \u0026lsquo;my-app-db\u0026rsquo; then the node.js app can connect to the database using mongodb://my-app-db/dbname connection string.\nAdd an application (from Apps page). Essential parameters are:\nType - Node.js Data source type - None (we use custom mongo setup) Set repository parameters Add an environment variable: NODE_ENV - production Now go to \u0026lsquo;Deployments\u0026rsquo; and deploy an app. If everything is done right it will setup mongodb and node.js and deploy application code. In the case of failure the OpsWorks will display a link to the log file.\nAdditional information and related links MongoDB documentation has a section about setup on Amazon EC2. Already mentioned blog post about mongo setup with OpsWorks and another blog post about replicaset setup. OpsWorks cookbooks repository Stackoverflow: setting up mongodb via AWS opsworks opsworks-mongodb-example repository MongoDB on AWS (RDS-Style) Fault Tolerant MongoDB on EC2 NoSQL Database in the Cloud: MongoDB on AWS High Performance MongoDB Clusters with Amazon EBS Provisioned IOPS Building a Mongo Replica Set with Chef and Vagrant article Linode: Creating a MongoDB Replication Set on Ubuntu 12.04 (Precise) Hosting meteor with MongoDb on Webfaction OpsWorks, Chef and Ruby:\nOpsWorks attribute reference OpsWorks resource reference Cookbook repository structure How to use shell scripts Stackoverflow: How I can change a file with chef? Just Enough Ruby for Chef About the Recipe DSL Stackoverflow: what ruby features are used in chef recipes? Stackoverflow: Ruby Code Blocks and Chef Hosted MongoDB: compose.io (as I understand former MongoHQ), MongoDirector, mongolab, ObjectRocket and dotCloud.\n","href":"/html/2014-12-19-aws-opsworks-mongo-and-nodejs.html","title":"Amazon OpsWorks - node.js app with MongoDB setup"},{"content":"Let\u0026rsquo;s assume we have a history like this:\nG1 - G2 - G3 - B1 - B2 - B3 Where G1-G3 are \u0026lsquo;good\u0026rsquo; commits and B1-B3 are \u0026lsquo;bad\u0026rsquo; commits and we want to revert them.\nHere is the shell script to create the revision history like above, you can use it to try and see the effect of different commands.\ngit reset The first method is a simple way to throw away few recent commits. It re-writes the commit history, so only use it when your changes are not public yet (you can do this locally or on your private branch).\nThe git reset command can be used to throw away recent commits (the --hard flag will also remove any local changes that are not commited yet):\nCareful: `git reset` will rewirte history. Careful: `--hard` will remove not-commited local changes. $ git reset --hard HEAD~3 Here we can refer to B3 as HEAD, B2 is HEAD~1, B1 is HEAD~2. This way the last good commit G3 is HEAD~3:\nG1 - G2 - G3 - B1 - B2 - B3 \\ \\ \\ \\-- HEAD \\ \\ \\------ HEAD~1 \\ \\---------- HEAD~2 \\-------------- HEAD~3 git revert If your changes are public already (for example, merged to master or other public branch), then it is better to avoid history rewrites and use git revert to generate anti-commit for your changes.\nThe revert command takes SHA1 of one or several commits and generates the new change to reverse the effect of these commits.\nNote for Mercurial users: in Mercurial, the revert command works differently - it takes the revision identifier and reverts the state of the repository to that revision. So we can ask Mercurial to revert current state to the state of the revision G3. In git, the revert takes one or multiple revision identifiers and reverts the effect of the specified revisions. When we are talking to git, we ask it to revert the effect of revisions B3, B2, B1.\nHere is how we can use git revert:\n$ git revert --no-commit HEAD~2^..HEAD Or:\n$ git revert --no-commit HEAD~3..HEAD We need to revert a range of revisions from B1 to B3. The range specified with two dots like \u0026lt;rev1\u0026gt;..\u0026lt;rev2\u0026gt; includes only commits reachable from \u0026lt;rev2\u0026gt;, but not reachable from \u0026lt;rev1\u0026gt; (see man -7 gitrevisions).\nSince we need to include B1 (represented by HEAD~2) we use HEAD~2^ (its parent) or HEAD~3 (also parent of HEAD~2). The HEAD~2^ syntax is more convenient if commit SHAs are used to name commits.\nThe --no-commit option tells git to do the revert, but do not commit it automatically.\nSo now we can review the repository state and commit it. After that we will get the history like this:\nG1 - G2 - G3 - B1 - B2 - B3 - R` Where R' is a revert commit which will return repository state to the commit G3. Run git diff to check this (output should be empty):\n$ git diff HEAD~4 HEAD Another way to run revert is to specify commits one by one from newest to oldest:\n$ git revert --no-commit HEAD HEAD~1 HEAD~2 In this case there is no need to specify HEAD~3 since it is a good commit we do not want to revert. This is very useful if we want to revert some specific commits, for example, revert B3 and B1, but keep B2:\n$ git revert --no-commit HEAD HEAD~2 Revert the commit in the middle of the history For example, we have history like this:\nG1 - G2 - G3 - B1 - G4 - G5 And want to revert the B1 commit.\nThe safe way is to use the git revert B1-sha1 command, as described above - it will generate an anti-commit for B1.\nThe other method is to use interactive rebase to move the bad commit to the top of the history and then remove it with git reset:\ngit rebase -i HEAD~3 pick B1-sha commit message pick G4-sha commit message pick G5-sha commit message Here (in the file, opened by git rebase) we can edit the order of the commits and move the bad commit to the end of the commit history:\npick G4-sha commit message pick G5-sha commit message pick B1-sha commit message Save the file and exit the editor. If everything is alright (commit has no dependencies), the command will be completed successfully. Note: otherwise (you\u0026rsquo;ve got conflicts error) it is probably better to do git rebase --abort to get back to the previous state and re-consider the update strategy.\nNow the history should look like this:\nG1 - G2 - G3 - G4 - G5 - B1 And we can remove the last commit with git reset --hard HEAD~1 (again, careful, it will remove any local uncommited changes and will rewrite history, do not use on public branches).\nRollback to the specific revision The git revert command reverts a range of specified revisions, but sometimes we just want to restore the state of some specific revision instead of reverting commits one-by-one.\nThis is also how hg revert works in Mercurial.\nThis is especially useful if we want to revert past the merge point, where it can be quite difficult to use git revert because we will also need to specify which parent to follow at merge point (you\u0026rsquo;ll see a Commit XXX is a merge but no -m option was given. message).\nRevert to the specific revision using git checkout Having a history like this:\n/-- master / G1 - G2 - G3 - B1 - B2 - B3 \\--- branch We can rollback the branch state to G3 commit this way:\ngit checkout G3-sha git branch -D branch # Delete existing branch git checkout -b branch # Checkout branch with the same name again git push -f origin HEAD # Force-push the new branch So, basically, we are removing the existing branch and creating another branch with the same name from G3.\nRevert to the specific revision using git reset The solution comes from the Revert to a commit by a SHA hash in Git? question.\nHere we first hard reset the state of the repository to some previous revision and then soft reset back to current state. The soft reset will keep file modifications, so it will bring old state back on top of the current state:\nCareful, reset --hard will remove non-commited changes $ git reset --hard 0682c06 # Use the SHA1 of the revision you want to revert to HEAD is now at 0682c06 G3 $ git reset --soft HEAD@{1} $ git commit -m \u0026#34;Reverting to the state of the project at 0682c06\u0026#34; Rollback to the specific revision using git read-tree This solution comes from How to do hg revert \u0026ndash;all with git? question:\ngit read-tree -um @ 0682c06 # Use the SHA1 of the revision you want to revert to The -m option instructs read-tree to merge the specified state and -u will update work tree with the results of the merge.\nLinks Stackoverflow: Revert multiple git commits\nStackoverflow: Revert a range of commits in git\nStackoverflow: Git diff .. ? What\u0026rsquo;s the difference between having .. and no dots\nWhat\u0026rsquo;s the difference between HEAD^ and HEAD~ in Git?\nStackoverflow: Revert to a commit by a SHA hash in Git?\nHow to do hg revert \u0026ndash;all with git?\n","href":"/html/2014-01-04-git-revert-multiple-recent-comments.html","title":"git - how to revert multiple recent commits"},{"content":"When git uses less as pager the output of commands like git log disappears from the console screen when you exit from less.\nThis is not convenient in many cases so here is how to fix this.\nJust for git commands:\ngit config --global --replace-all core.pager \u0026#34;less -iXFR\u0026#34; For less command, globally (including git) - add to your shell profile (.bashrc, .zshrc, etc):\nexport LESS=-iXFR Options we set for less:\n-i - ignore case when searching (but respect case if search term contains uppercase letters) -X - do not clear screen on exit -F - exit if text is less then one screen long -R - was on by default on my system, something related to colors Links Linux Questions: more or less - but less does not keep its output on the screen\n","href":"/html/2014-01-04-git-log-and-less-keep-output.html","title":"How to keep `git log` and `git diff` output on the screen after exit"},{"content":"To debug mocha test with node inspector use the delay before test:\nbeforeEach(function(done) { //start mocha as //mocha -t 10000 --debug setTimeout(function() { done(); }, 5000); }); This way there are 5 seconds to start the node inspector and set a breakpoint. Mocha should be lauched as this:\n$ mocha -t 10000 --debug Same approach can be used not only for tests but for any short-living node app - just wrap the startup code into the setTimeout() call.\n","href":"/html/2013-12-02-node-debug-mocha.html","title":"Node.js - how to debug mocha test with node inspector"},{"content":"It is possible to ask node to show its core module source.\nFor example, we want to check the source of the readFileSunc() method:\n$ node \u0026gt; fs = require('fs'); \u0026gt; fs.writeFileSync('fs.js', fs.toString()) \u0026gt; fs.writeFileSync('fs.readFileSync.js', fs.readFileSync.toString()) [Ctrl-C][Ctrl-C] Now check the fs.readFileSync.js file in the current folder.\nAlso on some systems source code of core node modules is in the /usr/lib/nodejs/.\nAnd another (less interesting) way to get core module source - is to look for it in the node github repository:\ncheck installed node.js version (node \u0026ndash;version) find appropriate release on github (https://github.com/joyent/node/releases), for example 0.8.17 click commit id on the left c50c33e click \u0026ldquo;Browse code\u0026rdquo; and see the source ","href":"/html/2013-12-02-node-core-module-source.html","title":"Node.js - how to get core module source"},{"content":"Problem I have a console command in php which needs an access to DB. The command need to be launched via cron.\nThe DB connection string looks like like this\n'connectionString' =\u0026gt; 'mysql:host='.$_SERVER['RDS_HOSTNAME'].';port='.$_SERVER['RDS_PORT'].';dbname='.$_SERVER['RDS_DB_NAME'], where RDS_xxx parameters come from environment variables.\nThe problem is that cron launches the command with a clean environment (there are no RDS_xx variables). So the command fails to access the database.\nSolution Solution is to set the required environment variables before launching the command and this can be done with \u0026lsquo;/opt/elasticbeanstal/support/envvars\u0026rsquo; script:\n0 3 * * * . /opt/elasticbeanstalk/support/envvars; /var/www/html/console/yiic mycommand ","href":"/html/2013-10-22-elastic-beanstalk-cron-and-db.html","title":"Elastic Beanstalk - cron command and RDS DB access"},{"content":"By default Elastic Beanstalk console tool (eb) adds config files to .gitignore. If there are manual changes to EB configs it can be complex to manually sync these changes between different machines / different users. Of cause it is possible to add config files to git repository but there are also several parameters in the main config which are absolute paths to local files. This way it makes configs not useful for other users (except for the case when different users have exactly the same files layout). Here is how this problem can be fixed:\nAdd eb tools under git control Add eb configs under git control Patch eb tools to accept relative file paths Change paths in configs to relative Below are details for each step.\nAdd eb tools under git control This is necessary because we want all users to have the same tools version. Also we are going to make some changes to eb sources to make it understand relative paths.\nJust download eb tools and extract somewhere inside your project. Or copy existing version into the project.\nAdd a wrapper script to run the original eb tool:\n#!/bin/sh # Run the eb tool from the root project directory as # console/eb {parameters} # Use linux python2.7 version SCRIPT_PATH=`dirname $0` export PATH=$PATH:$SCRIPT_PATH/AWS-ElasticBeanstalk-CLI-2.5.1/eb/linux/python2.7 eb \u0026#34;$@\u0026#34; Script should be on the same lever as the extracted directory. For example I have:\nproject_root/ | .ebextensions/ | .elasticbeanstalk/ | .git/ | application/ | console/ | | AWS-ElasticBeanstalk-CLI-2.5.1/ | | eb* | | ... | ... | .gitignore So I launch the \u0026rsquo;eb\u0026rsquo; as \u0026lsquo;console/eb parameters\u0026rsquo; from the project root directory.\nAdd eb configs under git control Add the EB configs under git control:\n$ git add -f .elasticbeanstalk Patch eb tools to accept relative file paths For the 2.5.1 version I had to make two changes in the config file parser class.\nThe file path is \u0026hellip;/AWS-ElasticBeanstalk-CLI-2.5.1/eb/linux/python2.7/lib/utility/configfile_parser.py.\nAnd the changes are:\n[Lines 41 - 47] def read(self, pathfilename): #seb: expand path to allow using homedir and relative paths pathfilename = os.path.realpath(os.path.expanduser(pathfilename)) print \u0026#39;Load sectioned config file: \u0026#39; + pathfilename with codecs.open(pathfilename, \u0026#39;r\u0026#39;, encoding=ServiceDefault.CHAR_CODEC) as input_file: _RawConfigParser.readfp(self, input_file) [Lines 74 - 80] def read(self, pathfilename): #seb: expand path to allow using homedir and relative paths pathfilename = os.path.realpath(os.path.expanduser(pathfilename)) print \u0026#39;Load config file: \u0026#39; + pathfilename with codecs.open(pathfilename, \u0026#39;r\u0026#39;, encoding=ServiceDefault.CHAR_CODEC) as input_file: config_pairs = input_file.read() Now you can use relative to project root paths in your configs.\nChange paths in configs to relative I had three absolute paths in my ./elasticbeanstalk/config:\n[global] .. AwsCredentialFile=/home/username/.elasticbeanstalk/aws_credential_file .. [branches] staging=staging production=production [branch:staging] OptionSettingFile=/path/to/project/.elasticbeanstalk/optionsettings.staging ... [branch:production] OptionSettingFile=/path/to/project/.elasticbeanstalk/optionsettings.production .. Path to aws credential file can be easily fixed by just removing it - eb tools will use config from your home dir by default (~/.elacticbeanstalk/aws_credential_file).\nOther two paths (for branch configs) can now be changed to relative:\n[global] # Default credential file path (recognized by both eb tool and git aws.push tool) # AwsCredentialFile=~/.elasticbeanstalk/aws_credential_file ... [branches] staging=staging production=production [branch:staging] OptionSettingFile=./.elasticbeanstalk/optionsettings.staging ... [branch:production] OptionSettingFile=./.elasticbeanstalk/optionsettings.production ... Use it Make sure you are using the \u0026rsquo;eb\u0026rsquo; wrapper script from the root project directory. For example, for \u0026rsquo;eb status\u0026rsquo;:\n$ cd project/root/dir $ ./path/to/eb status Load sectioned config file: project/root/dir/.elasticbeanstalk/config Load config file: /home/user/.elasticbeanstalk/aws_credential_file URL\t: staging-jp48gay9nx.elasticbeanstalk.com Status\t: Ready Health\t: Green url: \u0026ldquo;/html/\n","href":"/html/2013-09-11-elastic-beanstalk-configs.html","title":"Elastic Beanstalk - deploy from different machines / by different users (or how to get rid of absolute paths in configs)"},{"content":"Git Server Prerequisites are git and ssh-server (apt-get install openssh-server).\nThe installation process is described in the Pro Git book. Below is the setup process with some comments and updates.\nAdd git user, set some password (you will be asked for it):\n$ sudo adduser git Log in as git user and setup authorized ssh keys:\n$ su git git@localname$ cd ~ git@localname$ mkdir .ssh For each user who need an access to the server add user\u0026rsquo;s public key into ~/.ssh/authorized_keys to generate new key-pair for the user use ssh-keygen see github manual for details.\ngit@localname$ cat /home/usera/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys git@localname$ cat /home/userb/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys Change permissions for .ssh folder and authorized_keys file:\ngit@localname$ chmod 600 ~/.ssh/authorized_keys git@localname$ chmod 755 ~/.ssh Make a dir for repositories and init bare repositories:\ngit@localname$ mkdir ~/server git@localname$ cd server git@localname$ mkdir project.git git@localname$ cd project.git git@localname$ git --bare init Sequrity - set git-shell for the git user:\n# check where git-shell is $ which git-shell /usr/bin/git-shell # edit the etc/passwd $ sudo vim /etc/passwd # find the string for git user: git❌1000:1000::/home/git:/bin/sh # change shell for git user: git❌1000:1000::/home/git:/usr/bin/git-shell On the user side - clone the repository or add a remote to existing repository:\n# clone $ git clone ssh://git@server.host.name/home/git/server/project.git # or add remote $ cd project $ git remote add origin ssh://git@server.host.name/home/git/server/project.git For \u0026lsquo;server.host.name\u0026rsquo; there are several options:\nif the server has a domain name then just use it use sever IP address instead of host name use any host name you like and add it to local hosts file to map to the IP address Possible problems and solutions git-shell shows the \u0026ldquo;Interactive git shell is not enabled.\u0026rdquo; This is OK and it will work, additional setup can be done to allow the git user to log-in via ssh and execute special commands like \u0026ldquo;list\u0026rdquo; to get repository listing. See:\nman git-shell cat /usr/share/doc/git/contrib/git-shell-commands/README http://serverfault.com/questions/285324/git-shell-not-enabled When the user tries to execute a server operation git asks for git user password instead of ssh key passphrase. Check auth log (/var/log/auth.log) for errors. In my case there were errors related to git user\u0026rsquo;s .ssh and authorized_keys permissions:\nAug 15 15:25:24 seb-ubu sshd[4561]: Authentication refused: bad ownership or modes for file /home/git/.ssh/authorized_keys ... Aug 15 15:47:48 seb-ubu sshd[7145]: Authentication refused: bad ownership or modes for directory /home/git/.ssh Changing access permissions as described above fixed the issue.\nHow to make existing git repository bare If you already have a repository and want to put it on the server then it seems to be safe to just copy the .git folder from the existing repository:\n# source: project/.git $ cp -r project/.git project.git $ cd project.git $ git config --bool core.bare true Git Server - email notifications on push Git already has a script to handle email notifications after push. Check the /usr/share/doc/git/contrib/hooks/post-receive-email for instructions:\n$ sudo chmod a+x /usr/share/git-core/contrib/hooks/post-receive-email $ cd /path/to/your/repository.git $ ln -sf /usr/share/git-core/contrib/hooks/post-receive-email hooks/post-receive Configure notifications:\n$ cd /path/to/your/repository.git # who should receive notifications $ git config hooks.mailinglist \u0026#34;user1@example.com user2@example.com\u0026#34; # send emails from $ git config hooks.envelopesender git@myserver.com # email subject prefix $ git config hooks.emailprefix \u0026#34;[Git]\u0026#34; # project name - edit \u0026#39;description\u0026#39; file in the git repository folder $ vim description The post-receive-email script requires sendmail to work. Below is a description of the sendmail setup process.\nSetup sendmail on Ubuntu Install it:\n$ sudo apt-get install sendmail Check your hosts file - in my case sendmail was incredibly slow and this was fixed by following line in /etc/hosts:\n127.0.0.1 localhost.localdomain localhost myhostname \u0026lt;--- order matters!!! Note that you need to use the same order as above - localhost.localdomain, localhost and then myhostname (replace myhostname with your real host name, check the output of \u0026lsquo;hostname\u0026rsquo; command).\nSend a test email:\n$ echo \u0026#34;My test email being sent from sendmail\u0026#34; | /usr/sbin/sendmail myemail@domain.com If you have problems with emails then check the log: /var/log/mail.log and error log: /var/log/mail.err.\nSetup SMTP for sendmail If you want to setup SMTP server for you emails do the following:\n$ cd /etc/mail $ sudo mkdir auth $ sudo chmod 700 auth $ sudo vim client-info Enter following line into the client-info file:\nAuthInfo:smtp.server.com \u0026#34;U:mymail@server.com\u0026#34; \u0026#34;I:mymail@server.com\u0026#34; \u0026#34;P:mypassword\u0026#34; Here you put you smtp server name (instead of \u0026lsquo;smtp.server.com\u0026rsquo;) and your credentials. \u0026lsquo;U\u0026rsquo; is an smtp user (usually your email), \u0026lsquo;I\u0026rsquo; is an account (usually also your email) and \u0026lsquo;P\u0026rsquo; is a password. See details for parameters here.\nContinue setup:\n$ sudo bash -c \u0026#34;makemap hash client-info \u0026lt; client-info\u0026#34; Edit the /etc/mail/sendmail.mc and add following lines before the \u0026ldquo;MAILER_DEFINITIONS\u0026rdquo; line:\ndefine(\u0026#39;SMART_HOST\u0026#39;,\u0026#39;smtp.server.com\u0026#39;)dnl define(\u0026#39;confAUTH_MECHANISMS\u0026#39;, \u0026#39;EXTERNAL GSSAPI DIGEST-MD5 CRAM-MD5 LOGIN PLAIN\u0026#39;)dnl FEATURE(\u0026#39;authinfo\u0026#39;,\u0026#39;hash /etc/mail/auth/client-info\u0026#39;)dnl Process the sendmail.mc with m4:\n$ sudo bash -c \u0026#34;m4 sendmail.mc \u0026gt; sendmail.cf\u0026#34; Restart sendmail:\n$ sudo service sendmail restart Resources:\nsendmail: how to configure sendmail on ubuntu? Configuring Sendmail with smarthost Ubuntu Gutsy Sendmail startup slow, \u0026ldquo;unqualified hostname unknown; sleeping for retry\u0026rdquo; Links gitosis vs gitolite? (and vs simple git server) Pro Git: Git on the Server - Setting Up the Server git push email notification Setting Up Git Commit Email Notifications ","href":"/html/2013-06-14-git-server-setup.html","title":"How to setup git server on ubuntu with push email notifications"},{"content":" D3.js D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.\nBrowser support: IE9+, For IE8 compatibility it is recommended to use Aight. See also tutorials and re-usable charts for d3.js.\nFlot Attractive JavaScript plotting for jQuery Flot is a pure JavaScript plotting library for jQuery, with a focus on simple usage, attractive looks and interactive features.\nBrowser support: Works with Internet Explorer 6+, Chrome, Firefox 2+, Safari 3+ and Opera 9.5+. For Internet Explorer \u0026lt; 9 you can use Excanvas, a canvas emulator (included into Flot download archive).\nChart.js Easy, object oriented client side graphs for designers and developers.\nBrowser support: All modern \u0026amp; major mobile browsers with canvas element support. For IE8 \u0026amp; below use the polyfill ExplorerCanvas.\nGoogle Charts Google Charts provides a perfect way to visualize data on your website. From simple line charts to complex hierarchical tree maps, the chart gallery provides a large number of ready-to-use chart types.\nCan not be downloaded, should be used directly from Google servers.\nBrowser support: Cross-browser compatibility (adopting VML for older IE versions) and cross-platform portability to iOS and new Android releases. No plugins are needed.\nJavaScript InfoVis Toolkit The JavaScript InfoVis Toolkit provides tools for creating Interactive Data Visualizations for the Web.\nBrowser Support: IE6+, Firefox2+, Safari3+, Opera9.5+ (see http://flowingdata.com/2009/06/05/javascript-infovis-toolkit-new-version-released/).\njqPlot jqPlot is a plotting and charting plugin for the jQuery Javascript framework. jqPlot produces beautiful line, bar and pie charts with many features.\nBrowser support: jqPlot has been tested on IE 7, IE 8, Firefox, Safari, and Opera.\nOther resources The 20 best tools for data visualization\nStackoverflow: JavaScript Chart Library\n50 JavaScript Libraries for Charts and Graphs\n","href":"/html/2013-06-14-js-libraries-for-charts.html","title":"JS libraties for charts (links)"},{"content":"Amazon provides following NoSQL storage options:\n[SimpleDB] (http://aws.amazon.com/simpledb/) - Amazon SimpleDB is a highly available and flexible non-relational data store that offloads the work of database administration. Developers simply store and query data items via web services requests and Amazon SimpleDB does the rest. [DynamoDB] (http://aws.amazon.com/dynamodb/) - Amazon DynamoDB is a fully-managed, high performance, NoSQL database service that is easy to set up, operate, and scale. Amazon\u0026rsquo;s review of big data solutions Amazon\u0026rsquo;s review Big Data on AWS mentions only DynamoDB and Elastic Map Reduce (based on Hadoop) as tools for big data management. As one of major DynamoDB benefits it is highlighted that it uses solid state drives, but this option is also available for other technologies:\nSolid state, at your service: NoSQL data stores benefit greatly from the speed of solid state drives. DynamoDB uses them by default, but if you are using alternatives from the AWS Marketplace, such as Cassandra or MongoDB, accelerate your access with on-demand access to terabytes of solid state storage, with the High I/O instance class. Learn more about the options with EC2 instance types (http://aws.amazon.com/ec2/instance-types). [Elastic Map Reduce] (http://aws.amazon.com/elasticmapreduce/) is a computing service which can be used in conjunction with storage services to perform operations on large datasets (indexing, data mining, log file analysis, etc). Amazon Elastic MapReduce is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. Amazon Elastic Compute Cloud (EC2) - Amazon Elastic Compute Cloud delivers scalable, pay-as-you-go compute capacity in the cloud.\n[Hadoop] (http://hadoop.apache.org/) The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\nQ: When would I use Amazon RDS vs. Amazon EC2 Relational Database AMIs vs. Amazon SimpleDB vs. Amazon DynamoDB? (http://aws.amazon.com/rds/faqs/#4)\nAmazon Web Services provides a number of database alternatives for developers. Amazon RDS enables you to run a fully featured relational database while offloading database administration; Amazon SimpleDB provides simple index and query capabilities with seamless scalability; Amazon DynamoDB is a fully managed NoSQL database service that offers fast and predictable performance with seamless scalability; and using one of our many relational database AMIs on Amazon EC2 and Amazon EBS allows you to operate your own relational database in the cloud. There are important differences between these alternatives that may make one more appropriate for your use case. See Running Databases on AWS for guidance on which solution is best for you. Amazon DynamoDB General Info Get Started with Amazon DynamoDB:\nAmazon DynamoDB automatically spreads the data and traffic for the table over a sufficient number of servers to handle the request capacity specified by the customer and the amount of data stored, while maintaining consistent and fast performance. All data items are stored on Solid State Disks (SSDs) and are automatically replicated across multiple Availability Zones in a Region to provide built-in high availability and data durability. Read / write throughput limits:\nProvisioned Throughput - When you create or update a table, you specify how much provisioned throughput capacity you want to reserve for reads and writes. Amazon DynamoDB will reserve the necessary machine resources to meet your throughput needs while ensuring consistent, low-latency performance. If your application requirements change, simply update your table throughput capacity using the AWS Management Console or the Amazon DynamoDB APIs. You are still able to achieve your prior throughput levels while scaling is underway. When you create a table, you must provide a table name, its primary key and your required read and write throughput values. Except for the required primary key, an Amazon DynamoDB table is schema-less. Individual items in an Amazon DynamoDB table can have any number of attributes, although there is a limit of 64 KB on the item size. A unit of read capacity represents one strongly consistent read per second (or two eventually consistent reads per second) for items as large as 4 KB. A unit of write capacity represents one write per second for items as large as 1 KB. Reads = Number of item reads per second × 4 KB item size (If you use eventually consistent reads, you'll get twice as many reads per second.) Writes = Number of item writes per second × 1 KB item size. Amazon DynamoDB supports the following two types of primary keys:\nHash Primary Key – The primary key is made of one attribute, a hash attribute. For example, a ProductCatalog table can have ProductID as its primary key. Amazon DynamoDB builds an unordered hash index on this primary key attribute. Hash and Range Primary Key – The primary key is made of two attributes. The first attribute is the hash attribute and the second attribute is the range attribute. For example, the forum Thread table can have ForumName and Subject as its primary key, where ForumName is the hash attribute and Subject is the range attribute. Amazon DynamoDB builds an unordered hash index on the hash attribute and a sorted range index on the range attribute. Local Secondary Indexes:\nWhen you create a table with a hash-and-range key, you can optionally define one or more local secondary indexes on that table. A local secondary index lets you query the data in the table using an alternate range key, in addition to queries against the primary key. Amazon DynamoDB Data Types Amazon DynamoDB supports the following data types:\nScalar data types - Number, String, and Binary. Multi-valued types - String Set, Number Set, and Binary Set. API / SDK:\nAmazon DynamoDB is a web service that uses HTTP and HTTPS as a transport and JavaScript Object Notation (JSON) as a message serialization format. Your application code can make requests directly to the Amazon DynamoDB web service API Reference (http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/Welcome.html). There is also PHP library as a part of Amazon PHP SDK (https://github.com/aws/aws-sdk-php).\nDynamoDB as events storage and limitations For example, we have a table to store events data with following fields:\nid, primary key, integer user_id, integer event_type_id, integer data, text create_time, timestamp And we have large amount of events for which we need to perform following operations:\nfilter by date filter by user filter by event type filter by data sorting pagination export DynamoDB limitations:\nFor \u0026lsquo;data\u0026rsquo; filed we can store up to 64KB (DynamoDB limitation). If we need to store more data then we need to use compression. Filters require additional indexes (not recommended to add many indexes, also indexes consume write limits and add a data size limitation). Filter by \u0026lsquo;data\u0026rsquo; filed - can be not possible if we will compress data (or we may need to maintain own index) Sorting - no, query results are always sorted by the range key Pagination - yes (using Limit, LastEvaluatedKey, and ExclusiveStartKey), but single request can not return more then 1MB, so if page contains more data then less items will be returned. Export - no (we can not get more then 1MB via single request) Additional indexes to perform data filtering (see http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForLSI.html ): having many indexes is not recommended because they consume storage and provisioned throughput and make table operations slower.\nFor tables with local secondary indexes there is size limit of 10GB for data with the same hash key (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html).\nThe maximum size of any item collection is 10 GB. This limit does not apply to tables without local secondary indexes; only tables that have one or more local secondary indexes are affected. We recommend as a best practice that you instrument your application to monitor the sizes of your item collections. One way to do so is to set the ReturnItemCollectionMetrics parameter to SIZE whenever you use BatchWriteItem, DeleteItem, PutItem or UpdateItem. Your application should examine the ReturnItemCollectionMetrics object in the output and log an error message whenever an item collection exceeds a user-defined limit (8 GB, for example). Setting a limit that is less than 10 GB would provide an early warning system so you know that an item collection is approaching the limit in time to do something about it. Item collection - is any group of items that have the same hash key, across a table and all of its local secondary indexes. For instance, consider an e-commerce application that stores customer order data in a DynamoDB table with hash-range schema of customer id-order timestamp. Without LSI, to find an answer to the question “Display all orders made by Customer X with shipping date in the past 30 days, sorted by shipping date”, you had to use the Query API to retrieve all the objects under the hash key “X”, sort the results by shipment date and then filter out older records. It is not possible to add secondary indexes into existing table.\nExisting indexes also can not be changed or deleted.\nQuery results sorting:\nQuery results are always sorted by the range key. If the data type of the range key is Number, the results are returned in numeric order; otherwise, the results are returned in order of ASCII character code values. By default, the sort order is ascending. To reverse the order use the ScanIndexForward parameter set to false. Get records count:\nin a request, set the Count parameter to true if you want Amazon DynamoDB to provide the total number of items that match the scan filter or query condition, instead of a list of the matching items. In a response, Amazon DynamoDB returns a Count value for the number of matching items in a request. If the matching items for a scan filter or query condition is over 1 MB, Count contains a partial count of the total number of items that match the request. To get the full count of items that match a request, use the LastEvaluatedKey in a subsequent request. Repeat the request until Amazon DynamoDB no longer returns a LastEvaluatedKey. Single operation size limit:\nA single operation can retrieve up to 1 MB of data, which can comprise as many as 100 items. BatchGetItem will return a partial result if the response size limit is exceeded, the table's provisioned throughput is exceeded, or an internal processing failure occurs. If a partial result is returned, the operation returns a value for UnprocessedKeys. You can use this value to retry the operation starting with the next item to get. For example, if you ask to retrieve 100 items, but each individual item is 50 KB in size, the system returns 20 items (1 MB) and an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. Export - data export can be implemented as off-line operation using Elastic Map Reduce. See Elastic Map Reduce use cases:\nExporting data stored in Amazon DynamoDB to Amazon S3. Importing data in Amazon S3 to Amazon DynamoDB. Querying live Amazon DynamoDB data using SQL-like statements (HiveQL). Joining data stored in Amazon DynamoDB and exporting it or querying against the joined data. Loading Amazon DynamoDB data into the Hadoop Distributed File System (HDFS) and using it as input into an Amazon EMR job flow. To perform advanced queries on data it is also possible to copy data to Amazon Redshift:\nAmazon Redshift complements Amazon DynamoDB with advanced business intelligence capabilities and a powerful SQL-based interface. When you copy data from an Amazon DynamoDB table into Amazon Redshift, you can perform complex data analysis queries on that data, including joins with other tables in your Amazon Redshift cluster. In terms of provisioned throughput, a copy operation from an Amazon DynamoDB table counts against that table's read capacity. After the data is copied, your SQL queries in Amazon Redshift do not affect Amazon DynamoDB in any way. This is because your queries act upon a copy of the data from DynamoDB, rather than upon DynamoDB itself. Cost Dynamo db pricing page and price calculator.\nAssumptions (having events table described above):\nfor each user (user_id in events table) we add 1 million records per day (24 hours) each logged event data size is about 256 Bytes DynamoDB table does not have local secondary indexes Parameters for cost calculation:\nNumber of events logged per day (24 hours): 1 million records Number of write requests (events logged per second): 12427 (for simplicity assume app launches are uniformly distributed across 24 hours) Number or read requests: for simplicity assume we have no read requests (read requests are much cheaper then write requests) Data size added per month: 30 million record * 0.25KB ~= 7GB Cost (according to the price calculator) is about $6500 per month (for a single user\u0026rsquo;s data).\nNotes:\nReads are much cheaper then writes, so if we add the same number or reads into calculation it will not grow much. We pay not for actual load, but for reserved load. So if we set writes per second to 12000 then we will pay for this even if actual load will be low. Limits can be changed to higher value at any time, but can be lowered only 4 times per day. Other DynamoDB resources Amazon Dynamo: The Next Generation Of Virtual Distributed Storage\nAmazon DynamoDB – a Fast and Scalable NoSQL Database Service Designed for Internet Scale Applications\n[Why My Team Went with DynamoDB Over MongoDB] (http://slashdot.org/topic/bi/why-my-team-went-with-dynamodb-over-mongodb/) - limitations and possible solutions; custom index (maybe at that time secondary indexes where not present); data as json + compression (other way is to store data as file to S3).\nDynamoDB shortcomings (and our work arounds) they decided not to use DynamoDB for events (SQL was a better tool in this case, so we decided not to use DynamoDB at all for storing events) and build cache between DynamoDB and their app.\nDYNAMODB IS AWESOME, BUT… - about limitations.\nAmazon DynamoDB - about provisioned throughput, Query, Scan and indexing.\nExpanding the Cloud: Faster, More Flexible Queries with DynamoDB - about secondary indexes\n[My Disappointments with Amazon DynamoDB] (http://whynosql.com/my-disappointments-with-amazon-dynamodb/)\n[Amazon DynamoDB Part III: MapReducin’ Logs] (http://www.newvem.com/amazon-dynamodb-part-iii-mapreducin-logs/)\nAmazon forum threads:\ndata import/export DynamoDB Use case DynamoDB Table Design Can i build a leaderboard with local secondary index? DynamoDB Hash key partition strategy Partition size / key range info DynamoDB mocks:\nForum thread: DynamoDB Mock clientside_aws FakeDynamo Alternator - A DynamoDB Mock Server Amazon SimpleDB General Info Simple DB description page.\nDeveloper Guide.\nPHP SDK and HTTP API.\nFeatures Complex Queries\nOne of the main uses for Amazon SimpleDB involves making complex queries against your data set, so you can get exactly the data you need. For more information, refer to the Select section of the Amazon SimpleDB Developer Guide. Select operator supports:\nwhere (comparison operators, including like), sort and limit. count() (but query should be no longer then 5 seconds otherwise you will get partial result and need to repeat count() operation) max / min values can be selected using ordering by value and limit 1. Data Storage and Performance\nFor information on how quickly stored data is recorded to Amazon SimpleDB, refer to the Consistency section of the Amazon SimpleDB Developer Guide. Limits and Restrictions\nDuring development, it is important to understand Amazon SimpleDB's limits when storing data, the amount of data Amazon SimpleDB can return from a query, and what to do if the limits are exceeded. For more information, refer to the Limits section of the Amazon SimpleDB Developer Guide. Limits:\nCurrently, you can store up to 10 GB per domain and you can create up to 250 domains.\nMaximum 1 billion attributes per domain (probably this is summary number of attributes for all items).\nAttribute value length - 1024 bytes.\nMaximum items in select response - 2500.\nMaximum query execution time - 5 seconds.\nMaximum response size for Select - 1MB.\nRequests per second (did not found in SimpleDB docs, but it is in the DynamoDB FAQ) - 25 writes per second (per domain?)\nDesigned for use with other Amazon Web Services—Amazon SimpleDB is designed to integrate easily with other web-scale services such as Amazon EC2 and Amazon S3. For example, developers can run their applications in Amazon EC2 and store their data objects in Amazon S3. Amazon SimpleDB can then be used to query the object metadata from within the application in Amazon EC2 and return pointers to the objects stored in Amazon S3.\nPartition data to domains strategy.\nSimpleDB as events storage and limitations A table to store events data has following fields:\nid, primary key, integer user_id, integer event_type_id, integer data, text create_time, timestamp There is large amount of data and we want to perform following operations:\nfilter by date filter by user filter by event type filter by data sorting pagination export Limitations:\nNow data size is not limited (text field) and it will be limited (total item size has a limit of 1KB). Filters - select operation allows us to implement all filters Sorting - select operation allows us to sort data Pagination - yes, but single request can not return more then 1MB, so if page contains more data then less items will be returned. Export - no (we can not get more then 1MB via single request) Cost Cost calculator for SimpleDB (http://calculator.s3.amazonaws.com/calc5.html#s=SIMPLEDB).\nIf we have the same assumtions as for DynamoDB (see above) then we have:\nNumber of Items - 30000000 (per month) Average Number of Attributes Per Item - 5 Total Size of Attribute Values - 7GB Number of BatchPuts - 30000000 Number of Gets - 30000000 Number of Simple Selects - 30000000 Data Transfer Out: - 7GB Data Transfer In: - 7GB Calculated cost: $173.\nAmazon SimpleDB vs DynamoDB Q: How does Amazon DynamoDB differ from Amazon SimpleDB? Which should I use? (http://aws.amazon.com/dynamodb/faqs/#How_does_Amazon_DynamoDB_differ_from_Amazon_SimpleDB_Which_should_I_use)\nBoth services are non-relational databases that remove the work of database administration. Amazon DynamoDB focuses on providing seamless scalability and fast, predictable performance. It runs on solid state disks (SSDs) for low-latency response times, and there are no limits on the request capacity or storage size for a given table. This is because Amazon DynamoDB automatically partitions your data and workload over a sufficient number of servers to meet the scale requirements you provide. In contrast, a table in Amazon SimpleDB has a strict storage limitation of 10 GB and is limited in the request capacity it can achieve (typically under 25 writes/second); it is up to you to manage the partitioning and re-partitioning of your data over additional SimpleDB tables if you need additional scale. While SimpleDB has scaling limitations, it may be a good fit for smaller workloads that require query flexibility. Amazon SimpleDB automatically indexes all item attributes and thus supports query flexibility at the cost of performance and scale. Note: in the DynamoDB there is also 10GB limitation for item collections (items with the same hash key) if we use secondary indexes.\nFor events storage we have:\nquery /filter data with different criteria: can be done with SimpleDB, much more complex with DynamoDB sort data: yes for SimpleDB, no for DynamoDB (fixed sorting) export data: complex for both, need to implement offline operation response time / requests performane: much better for DynamoDB table size limit and scaling: better for DynamoDB cost: much higher for DynamoDB Additional Resources Quora: What is the difference between SimpleDB and DynamoDB?\nStackoverflow: Amazon SimpleDB vs Amazon DynamoDB\nStackoverflow: Amazon SimpleDB or DynamoDB\nGeneral Resourses Overview of Big Data and NoSQL Technologies as of January 2013\nWhat The Heck Are You Actually Using NoSQL For?\nScaling Twitter: Making Twitter 10000 Percent Faster\nStackoverflow: How to store 7.3 billion rows of market data (optimized to be read)?\nRunning Databases on AWS\nRunning NoSQL Databases on AWS\nAnti-RDBMS: A list of distributed key-value stores\nCouchDB: Why NoSQL?\nThoughts on SimpleDB, DynamoDB and Cassandra\nSome Other NoSQL solutions MongoDB and MongoDB use cases: Storing log data.\nMongoDB NoSQL Database on AWS.\nCassandra and Cassandra use cases.\nHypertable.\nHBase.\nCouchDB.\nVoltDB.\nIs VoltDB really as scalable as they claim?.\nVoltDB Decapitates Six SQL Urban Myths And Delivers Internet Scale OLTP In The Process.\n","href":"/html/2013-06-11-amazon-nosql-review.html","title":"Amazon NoSQL Solutions"},{"content":"With angular.js you have an HTML which looks like this:\n\u0026lt;span\u0026gt;{{variableValue}}\u0026lt;/span\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li ng-repeat=\u0026#34;item in items\u0026#34; ng-bind=\u0026#34;item.name\u0026#34;\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; The simple way to make this content SEO-friendly is to pre-render data on the server and then allow angular to do it\u0026rsquo;s job on the client.\nFor simple variables there is ng-bind. And for lists there is ng-include. Here is the example from above with pre-rendered content:\n\u0026lt;span ng-bind=\u0026#34;variableValue\u0026#34;\u0026gt;Static indexed value\u0026lt;/span\u0026gt; \u0026lt;ul ng-include=\u0026#34;\u0026#39;your/dynamic/list\u0026#39;\u0026#34;\u0026gt; \u0026lt;li\u0026gt;seo-friendly item1\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;seo-friendly item2\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;script type=\u0026#34;text/ng-template\u0026#34; id=\u0026#34;your/dynamic/list\u0026#34;\u0026gt; \u0026lt;li ng-repeat=\u0026#34;item in items\u0026#34; ng-bind=\u0026#34;item.name\u0026#34;\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/script\u0026gt; This way we have initial content for search bots and instructions for angular at the same time.\nNote that the solution above works for simple cases but it can be not good for complex pages or if there are many pages. Actually what we do is duplicate the content - we provide static content on the server site and then angular requests content dynamically an overrides static data.\nA better (and more complex) solution is to crawl the website and give static page snapshots to search engines. This can be done with a headless browser like PhantomJS which you can run periodically to generate static page snapshots.\nThere is also an easy way to do this - use a service like BromBone. It will do complex part of the job for you and you only need to redirect search bots to static versions of your pages generated by BromBone.\nLinks Pre-rendering AngularJS on the server with Node.js and jsdom and discussion on reddit\n[angular and SEO discussion on google groups] (https://groups.google.com/forum/m/?fromgroups#!topic/angular/2uk2GYff28E)\n[Ajax crawling - google developers] (https://developers.google.com/webmasters/ajax-crawling/docs/getting-started)\n[AngularJS and SEO - approach based on PantomJS] (http://www.yearofmoo.com/2012/11/angularjs-and-seo.html) and github repository\n[BromBone - make your javascript powered website crawlable by Google (and other search engines)] (http://www.brombone.com)\n","href":"/html/2013-05-24-angular-seo.html","title":"Angular.js and SEO - pre-render content on the server"},{"content":"","href":"/tags/angularjs/","title":"angularjs"},{"content":"","href":"/tags/php/","title":"php"},{"content":"Enable mbstring function overloading mode and set default encoding for string functions to utf-8 in php.ini:\nmbstring.internal_encoding = UTF-8 mbstring.func_overload = 7 These settings allow us to use \u0026ldquo;usual\u0026rdquo; php string functions like substr() for utf-8 strings. It is not recommended to set function overloading in per-directory context (via Apache config or in the .htaccess).\nDefault encoding can also be set using mb_internal_encoding function:\nmb_internal_encoding('UTF-8'); Or encoding can be set explicitly as argument in mbstring function:\n$sub = mb_substr($mbstr, 0, 1, 'utf-8'); Links PHP Docs - Multibyte String\nYii Wiki - How to set up Unicode\n","href":"/html/2013-03-23-php-utf-8-strings.html","title":"PHP - utf-8 strings handling"},{"content":"The Scalable JavaScript Application Architecture is a presentation by Nicholas Zakas where he suggests a flexible and scalable architecture for JavaScript applications. Here are other related resources:\nPresentation summary Presentation slides Patterns For Large-Scale JavaScript Application Architecture by Addy Osmani The presentation is interesting but it also leaves many open questions. In short, the architecture contains following application layers:\nbase library (jquery, etc) application core: manages modules (register modules, tell when to start and when to stop) handle errors (like wrap all modules\u0026rsquo; methods into try/catch and log errors) enable inter-module communication should be extensible (error handling, ajax wrapper, general utilites, anything!) can use base library sandbox: facade for modules above the core interaction between modules via messages (events) modules: do not know about each other, only about sandbox call only own methods or sandbox methods DOM access only inside own box (but do not use base library) no access to non-native global objects, don\u0026rsquo;t create global objects ask sandbox for anything you need, don\u0026rsquo;t reference other modules preferably no access to base library, use pure JS Here are some questions raised by the presentation:\nIt is mentioned that the architecture covers \u0026lsquo;controller\u0026rsquo; part of MVC, but it is not clear how to plug M and V here. Should the module act as controller and handle model / view interaction (this can be complex without access to the base library)? For me the \u0026lsquo;module\u0026rsquo; here looks more like a widget - some self-contained element with own box in the HTML and related js code. Modules do not interact directly and only send messages to each other. This way we can change them independently. But messages are sent with some data and the receiver should expect some data structure. So in the case when this data changes it is necessary to review and fix all possible receivers. App core responsibilities are too wide (rememver \u0026ldquo;anything!\u0026rdquo;) and the same is related to sandbox (because it acts as a facade on top of the core). Of course good answers can be found for all these points and more questions can be raised. It is clear that the suggested architecture is an idea and the way to go but not a complete design. It was interesting to see if there are frameworks build upon this idea. And here is the list (descriptions are taken from their sites):\nAura is a decoupled, event-driven architecture for developing widget-based applications. Hydra.js is the library that will help you to scale your app. Kernel.js is an ultra lightweight (~4k) architecture for building scalable javascript applications. TerrificJS provides you a Scalable Javascript Architecture, that helps you to modularize your jQuery/Zepto Code in a very intuitive and natural way. scaleApp is a tiny JavaScript framework for scalable One-Page-Applications / Single-Page-Applications. framework by Legalbox - the Scalable JavaScript Application framework. AngularJS - Superheroic JavaScript MVW Framework. Stackoverflow question on the topic Presence of the AngualrJS here can be unexpected and reasons why I included it are below. Others are directly built on the idea from Nicholas\u0026rsquo; presentation and most popular of them (looking at the number of stars on the github) is Aura. Here are todo application examples built on Aura.\nSo why AngularJS is here? Actually I do not have much experience with it, but from what I know it looks very close to the Nicholas\u0026rsquo; Scalable Architecture:\nbase library - jQuery or angular\u0026rsquo;s own jqLite implementation app core - angular itself sandbox - scope passed to the controller module - angular\u0026rsquo;s controller Besides the main parts of the scalable architecture AngularJS has more. For example, along with the sandbox (scope) we can pass additional services to the controller (like $http for ajax requests). So the sandbox does not turn into a God-object with too much responsibilities.\nAnd angular controllers are very similar to scalable architecture modules:\nself-contained and sandboxed can be built in pure JS without access to the base library interaction between modules is done via events or via special service but not directly This way the idea of the scalable architecture is present in the AngularJS. And angular also gives other essential components (views, models, services, etc) and provides complete and ready to use architecture.\nUpdate: Toptal has reached out to me asking to link their article and I think it is quite useful not only when you need to hire someone, but also when learning or refreshing angular knowledge, so here it is: How to Hire a Great AngularJS Developer.\n","href":"/html/2013-03-18-js-scalable-architecture.html","title":"Want Scalable Application Architecture? Check AngularJS."},{"content":"","href":"/tags/chrome/","title":"chrome"},{"content":"A list of extensions to manage position of chrome tabs / windows and to create splits. Descriptions are original texts from extension pages.\nTabs Outliner Next Generation Session/Windows/Tabs Manager and Too_Many_Open_Tabs Solution That Really Works.\nUltimate overview for all of your open, and not so open (read further), browser windows and tabs, in a resizable vertical side view, with a Tree Style Tab feature. A KILLER FEATURE – Has the unique ability to close and preserve “in place” any tab or window, without removing them from the original context in the tree. Then, if needed, reopen them in the same way you select already open windows or tabs. This is a real innovation in Web browser usability. It diminishes the cognitive distinction between closed-preserved and open tabs. Therefore, there will be no substantial difference between closed or open tabs, and because of this, there will be no desire to keep them all open anymore!!! Outliner Features – You will be able annotate open or closed windows and tabs, add comments to them, notes, a summary of main ideas, to-do items. Organize all of this in logical hierarchies and delimited groups; freely reorder to specify priority or importance. Crash Resistance and Restore Feature Done Right. Dualless Dualless - For those who don\u0026rsquo;t have dual monitor. Dualless splits your browser windows into two by just 2 clicks. The ratio can be adjusted according to your needs.You may have a browser window showing on the right that occupy 30% of your screen. Then the rest of space leave to Google+.\nThe extension simulates the environment of dual monitor. Utilize the space for 16:9 monitor.\nTab Resize - split screen layouts Split Screen Layouts made easy. Automates resizing tab windows to predefined and user defined layouts. A simple extension designed to provide ease in resizing your tabs. A set of default layouts are provided but you can add and remove from the list of layouts to fit your needs.\nDEMO video\nHow it works The selected/highlighted tab along with all tabs to the right of it will be considered. Whether you have more or less tabs than are needed the extension will resize only the available tabs. Undo button will undo the previous layout resize. You can only perform undo once at any time. In \u0026lsquo;single tab\u0026rsquo; mode, only the selected/highlighted tab will considered. Only the current window/tab will change in size, all other tabs to the right will be ignored. You can create your own custom layouts within reason and reset to default configurations if desired. Layouts are sorted most recent used on right. Tab Scissors This simple tab organization tool divides one window into two. If you have at least two tabs in the selected Chrome window, it… This simple tab organization tool divides one window into two. If you have at least two tabs in the selected Chrome window, it will split that window into two smaller side-by-side windows. All the tabs on the left side of the selected tab will stay in the left window, and the rest will move to the window on the right. It is especially helpful for those who have lots of tabs opened at a time. This extension works great with Tab Glue, which will glue all those pesky Chrome windows back together.\nFrame two pages Merge the active tab and the previous one into a frameset.\nOnce you click at the extension\u0026rsquo;s icon, the current tab will be merged with the previous one (on the left) into a frameset with two columns (frames) or rows (options offer 3 buttons: rows/columns/ad_hoc). They\u0026rsquo;ll be ordered in the same way as the tabs (left, right).\nIn other words, your physical screen will be split into two areas.\nIf the current window only has one tab, you\u0026rsquo;ll be asked about the URL of the left frame. Recursive frames work - e.g. if you activate the extension on a new tab - but each smaller frame\u0026rsquo;s URL has to be typed manually (\u0026ldquo;same origin policy\u0026rdquo;).\nThis gadget may be helpful if you are e.g. translating one page to another page or otherwise editing a text, using two windows.\nSplit Screen Prompts the user for two URL addresses and then displays both in one window!\nOpens a new tab and prompts the user for two URL\u0026rsquo;s, then displays both sites on one page. Great for cross-reference studying and surfing the web in general! Watch a video, (or wait for it to buffer) while you surf another webpage.\nDual View Split Screen Splits the screen into two vertical tiles. Dual View Split Screen lets you split your screen into two vertical tiles. You are able to view two different pages side by side which comes in handy when you want to compare content, translate content and so on.\nClick on the screen icon in your browser bar. You will be asked to provide the URL of the left window. Then you will be asked for the right window accordingly.\nNothing else to take care of. It is as easy as that. Enjoy!\nAlum: Window and tab manager This extension is for power users who constantly find themselves moving around windows and tabs to maximize usable screen area. The goal is efficiency and reducing RSI. Please see the video at right for a full demo of Alum.\nFeatures:\nOpen non-overlapping windows to take up full screen area Rotate tabs between windows under Alum\u0026rsquo;s control Focus stays on primary content window\u0026ndash; never touch your mouse! Move tabs between Alum windows for quick webpage classificiation Hot-keys for all actions Known issues:\nNo support for multiple monitors A few screen pixels unaccounted for on Mac OS X Chromie: Window and Tab Tools Manipulates (splits, merges, scrolls, indexes) Chrome\u0026rsquo;s windows with simple clicks.\nSplit/Merge Chrome Windows to occupy the whole screen with different workspaces. Double-click Chromie icon to create 2 workspaces. Triple-click Chromie icon to create 3 workspaces. Quadruple-click to create 4 workspaces. and so on to unlimited workspaces theoretically (but it seems that 4 windows or less are the most efficient). Yeah and of course, single-click will merge all windows into 1 big Chromie. Another single-click will toggle between the big Chromie and the original Chromie where we started off with. Synchronize Windows scrolling actions (syncScroll) among different windows by holding Alt + mouse wheel(or Shift + mouse wheel will be implemented soon enough). There are options of layouts for you to pick and you decide how fast you can click by choosing click timeout parameter (where 200 millisecs is recommended). Split Meister V Makes a vertical split by productively moving the tab to the side, instead of using frames. This extension makes a vertical split for vieing two tabs at once. It\u0026rsquo;s accomplished by resizing your current window, and putting the current tab to the right of it.\nOther Browser instead of window manager\nProject Tab Manager\n","href":"/html/2012-12-09-google-chrome-window-manager.html","title":"Window managers for Google Chrome"},{"content":"Branches that were not merged with master:\ngit branch -a --no-merged (or git branch -a --no-merged master) Branches that were merged with feature:\ngit branch -a --merged feature Branches that were merged not with feature:\ngit branch -a --no-merged feature Reference The git-branch(1) command:\nwith --contains flag shows only the branches that contain the named commit (in other words, the branches whose tip commits are descendants of the named commit). with --merged shows only branches merged into the named commit (i.e. the branches whose tip commits are reachable from the named commit) will be listed. with --no-merged shows only branches not merged into the named commit will be listed if the argument is missing, it defaults to HEAD (i.e. the tip of the current branch). ","href":"/html/2012-10-01-git-find-not-merged-branches.html","title":"git - find not merged branches"},{"content":"Diff and log between two branches (all changes on both branches) To see all changes between two branches, use diff with two or three dots and log with three dots between branch names.\nNote: the confusing part is that log with two dots only shows changes on one branch, while diff with two dots includes changes on both branches (showing changes from one branch as removed and from another branch - as added). See the section below on how to see only new changes on the branch.\nFor example, we have a history like this:\n... A ---- B ---- C ---- D master \\ E ---- F branch List of new commits on both branches (log with three dots): git log master...branch commit F commit D commit E commit C # With --left-right it shows additional markers for commits git log --left-right master...branch commit \u0026gt; F commit \u0026lt; D commit \u0026gt; E commit \u0026lt; C Diff with two dots, master commits shown as removed changes, branch commits - as added: git diff master..branch -changes from C,D +changes from E,F # Same is without dots git diff master branch -changes from C,D +changes from E,F Diff with two dots, branch commits are shown as removed changes, master commits as added: git diff branch..master +changes from C,D -changes from E,F # Same is without dots git diff branch master +changes from C,D -changes from E,F Diff and log between two branches (only branch changes) Often, we want to only see changes on the branch, without seeing what has changed on master (or on another base branch).\nIn this case, we can use git log with two dots to get the list of changes and git log -p to see the diff (while git diff with two dots will still show changes from the base branch, so is not suitable for this task).\nFor example, we have a history like this:\n... A ---- B ---- C ---- D master \\ E ---- F branch List of new commits on branch (log with two dots): git log master..branch F E List of new commits on branch (cherry): git cherry master + F + E git cherry -v master + F commit message + E commit message The git cherry command provides a more compact output with the list of new commits on the branch comparing to master.\nList of new commits on master (log with two dots): git log branch..master D C Diff, only new commits on branch (log with two dots and -p or diff with three dots): git log -p master..branch +changes from F +changes from E git diff master...branch +changes from E +changes from F Diff, only new commits on master (log with two dots and -p or diff with tree dots): git log -p branch..master +changes from D +changes from C git diff branch...master +changes from C +changes from D Git diff and log inconsistency In the sections above we can see that git diff and git log with three and two dots seem to behave inconsistently:\ngit log master..branch - only new commits on branch git diff master..branch - changes on both master and branch git log master...branch - new commits on both master and branch git diff master...branch - only changes on branch The definition of two and three dots:\nDouble Dot - range of commits that are reachable from one commit but aren’t reachable from another (in other words, new commits on branch that are not present on master) Triple-dot - all the commits that are reachable by either of two references but not by both of them (changes on both branches, but not common changes before that). And git log does exactly that - new commits on the branch with two dots and all new commits on both branches with tree dots.\nWhile git diff works differently, here is a summary from the man page:\nComparing branches $ git diff topic master (1) $ git diff topic..master (2) $ git diff topic...master (3) 1. Changes between the tips of the topic and the master branches. 2. Same as above. 3. Changes that occurred on the master branch since when the topic branch was started off it. This is why I like git log -p to show diffs instead of git diff. It is more consistent and easier to remember - less dots (two) - less changes (only new changes on the branch), more dots (three) - more changes (changes on both branches):\ngit log master..branch - only new commits on branch git log -p master..branch - only changes on branch git log master...branch - new commits on both master and branch git log -p master...branch - changes on both master and branch Diff staged changes Show staged changes - added to index, but not commited yet:\ngit diff --cached # or git diff --staged Diff pulled changes Show diff of changes pulled from upstream:\ngit pull origin git diff @{1}.. Here @ means HEAD and @{1} means \u0026ldquo;prior value of the HEAD\u0026rdquo;, the list of prior values can be seen with git reflog.\nLog without merge commits To show the log without merge commits use --no-merges flag:\ngit log --no-merges Log without another branch commits To exclude commits on master (or another) branch, use --not flag:\ngit log contrib --not master # or git log contrib ^master To exclude both commits on master commits and merge commits:\ngit log contrib ^master --no-merges What was added in remote branch, but not in local:\ngit log origin/featureA ^featureA Note: these three commands are equivalent:\ngit log refA..refB git log ^refA refB git log refB --not refA Show all commits that are reachable from refA or refB but not from the refC:\ngit log refA refB ^refC git log refA refB --not refC Git log formatting Log with diff (p), only last two entries:\ngit log -p -2 One line log:\ngit log --pretty=oneline Log with graph:\ngit log --pretty=format:\u0026#34;%h %s\u0026#34; --graph Format log:\ngit log --pretty=format:\u0026#34;%h - %an, %ar : %s\u0026#34; Log formatting options:\n%H Commit hash %h Abbreviated commit hash %T Tree hash %t Abbreviated tree hash %P Parent hashes %p Abbreviated parent hashes %an Author name %ae Author e-mail %ad Author date (format respects the –date= option) %ar Author date, relative %cn Committer name %ce Committer email %cd Committer date %cr Committer date, relative %s Subject Links Stackoverflow: How can I generate a git diff of what\u0026rsquo;s changed since the last time I pulled?\nStackoverflow: What are the differences between double-dot \u0026ldquo;..\u0026rdquo; and triple-dot \u0026ldquo;\u0026hellip;\u0026rdquo; in Git diff commit ranges?\nProGit: Git Tools - Revision Selectio\nProGit: Viewing Your Staged and Unstaged Changes\n","href":"/html/2012-10-01-git-diff-and-log.html","title":"git - viewing changes - diff and log"},{"content":"Solution from stackoverflow.\nFind all branches which contain a change to FILENAME (even if before the (non-recorded) branch point):\ngit log --all --format=%H FILENAME | while read f; do git branch --contains $f; done | sort -u Manually inspect:\ngitk --all --date-order -- FILENAME Find all changes to FILENAME not merged to master:\ngit for-each-ref --format=\u0026#34;%(refname:short)\u0026#34; refs/heads | grep -v master | while read br; do git cherry master $br | while read x h; do if [ \u0026#34;`git log -n 1 --format=%H $h -- FILENAME`\u0026#34; = \u0026#34;$h\u0026#34; ]; then echo $br; fi; done; done | sort -u ","href":"/html/2012-09-30-git-all-branches-with-file.html","title":"git - find all branches where file was changed"},{"content":"Your branch is ahead of \u0026lsquo;origin/master\u0026rsquo; by 1 commit (or X commits) after git pull origin master.\nThe sequence:\nHave up-to-date repository There is a change in the origin/master Do git pull origin master Change is received and merged git status shows “Your branch is ahead of \u0026lsquo;origin/master\u0026rsquo; by 1 commit.” The reason is because during pull origin master reference to the remote origin/master is not changed (still points to older version).\nSolution: you, probably, do not want to use the git pull origin master command. Use git pull instead (will fetch remote changes, update remote references and merge remote changes to your local branch) or git fetch (will fetch remote references, but without merge to your local branch, you can merge manually via git merge origin/master).\nTo check this:\ngit diff origin/master It will show the difference - it should be the last master change.\nTo fix this execute:\ngit fetch origin This will update git links to remotes.\nIf you have this situation then you probably wanted to do just git pull instead of git pull origin master.\nLinks Stackoverflow: \u0026lsquo;git pull origin mybranch\u0026rsquo; leaves local mybranch N commits ahead of origin. Why?\nGit repo on a machine with zero commits is ahead of remote by 103 commits.. !\nQuora: Git (revision control): What are the differences between \u0026ldquo;git pull\u0026rdquo;, \u0026ldquo;git pull origin master\u0026rdquo;, and \u0026ldquo;git pull origin/master\u0026rdquo;?\nStackoverflow: Differences between git pull origin master \u0026amp; git pull origin/master\nStackoverflow: Why do I have to push the changes I just pulled from origin in Git?\n","href":"/html/2012-09-30-git-branch-ahead-after-pull.html","title":"git - Your branch is ahead of 'origin/master' by 1 commit after pull"},{"content":"","href":"/tags/jquery/","title":"jquery"},{"content":"With this method we work on the server with UTC timezone dates and convert them to a user local timezone on client.\nUse ‘TIMESTAMP’ type for date/datetime DB fields Setup MySQL and PHP timezone Set UTC timezone for both MySQL and PHP.\nYii db config (MySQL):\n‘db’ =\u0026gt; array( \u0026#39;connectionString\u0026#39; =\u0026gt; \u0026#39;...’, \u0026#39;initSQLs\u0026#39;=\u0026gt;\u0026#34;set time_zone=\u0026#39;+00:00’;\u0026#34;, ); And PHP timezone:\ndate_default_timezone_set(‘UTC’); Generate HTML with UTC dates in ISO 8601 See date formats here Helper functions to convert dates:\nMySQL date to UTC\n$utcString = gmdate(\u0026lsquo;Y-m-d\\TH:i:s\\Z\u0026rsquo;, strtotime($dateTimeString));\nUTC date to HTML\necho CHtml::tag(\u0026lsquo;span\u0026rsquo;, array(\u0026lsquo;class\u0026rsquo;=\u0026gt;\u0026lsquo;localtime\u0026rsquo;), $utcString);\nUTC date to timestamp\n$ts = strtotime($utcString);\nAdd localtime plugin and localtimex extension Include scripts into the page, see jquery.localtime.js plugin here and jquery.localtimex.js code at the end. To disable jquery.localtime.js default initialization pass empty format to setFormat() method (or remove jQuery.ready block at the end of jquery.localtime.js).\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;/js/jquery.localtime-0.5.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;/js/jquery.localtimex.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt;$.localtime.setFormat({}); \u0026lt;/script\u0026gt; Use localtimex:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; $(\u0026#39;.localtime\u0026#39;).localtimex(\u0026#39;dd MMM yyyy HH:mm:ss\u0026#39;); \u0026lt;/script\u0026gt; By default plugin will localize specified elements and re-localize them in the case of ajax updates. Date format is ISO 8601, see description.\nDate inputs jquery.localtimex.js plugin also supports date inputs localization (tested with jQuery UI datepicker and depends on it).\nIt will convert UTC value to local time, so user can work with it. Before form submit plugin will convert it back to UTC.\nExample of usage:\nUse initial UTC value for date input or leave it empty:\n$myDate = gmdate(\u0026lsquo;Y-m-d\\TH:i:s\\Z\u0026rsquo;, time()); //today, use it as initial date input value\nWhen configuring the jQuery datepicker plugin - set date format to \u0026lsquo;mm/dd/yy\u0026rsquo; and add css class \u0026rsquo;localdatepicker\u0026rsquo;.\nAttach jquery.localtimex plugin to the same input, set date format to \u0026lsquo;MM/dd/yyyy\u0026rsquo; and use css class \u0026rsquo;localdatepicker\u0026rsquo;:\n$(\u0026rsquo;.localdatepicker\u0026rsquo;).localtimex(\u0026lsquo;MM/dd/yyyy\u0026rsquo;);\nOn the client side localtimex plugin will convert initial UTC value to the local client time and then will convert it back when form is submitted\nOn the server side - accept UTC value, convert it to the MySQL format / timestamp:\n$myDateMySql = Yii::app()-\u0026gt;dateFormatter-\u0026gt;format(\u0026lsquo;yyyy-MM-dd\u0026rsquo;, strtotime($UTCValue)); $myDateTimestamp = strtotime($UTCValue);\njquery.localtimex.js ------------------------------------------- /** * @name jquery.localtimex.js * @author Boris Serebrov * * Depends on: jQuery, jQuery UI datepicker (local date parsing), * jquery.localtime-0.5.js (http://code.google.com/p/jquery-localtime/) * * Usage: * - do not configure jquery.localtime plugin, instead use this plugin * on elements you need localize: * $(\u0026#39;.localtime\u0026#39;).localtimex(\u0026#39;dd MMM yyyy HH:mm:ss\u0026#39;); * $(\u0026#39;.localdate\u0026#39;).localtimex(\u0026#39;dd MMM yyyy\u0026#39;); * * Ajax response handling: * - by default plugin will localize elements after ajax requests * - to disable this behavior pass \u0026#39;ajaxLocalize\u0026#39;:false to options * $(\u0026#39;.localdate\u0026#39;).localtimex(\u0026#39;dd MMM yyyy\u0026#39;, {\u0026#39;ajaxLocalize\u0026#39;:false}); * * Date inputs handling: * - server put UTC time into input (or leave it empty) * - UTC value converted to local time, user can modify it in local format * - on form submit value converted back to UTC and posted to server * * jQuery UI datepicker configuration: * - add \u0026#39;localtimex\u0026#39; plugin to date input * - set dateFormat of a date picker accordingly to localtime plugin output format * - they use different standards for date presentation, * so we have to describe the same presentation twice * - for example: localtime plugin - \u0026#39;MM/dd/yyyy\u0026#39;, datepicker - \u0026#39;mm/dd/yy\u0026#39; * - initial input value should be UTC in ISO format or empty */ (function($) { $.fn.localtimex = function(format, opt) { format = format || \u0026#39;yyyy-MM-dd HH:mm:ss\u0026#39;; opt = $.extend({ \u0026#39;ajaxLocalize\u0026#39;: true }, opt); if (opt.ajaxLocalize) { var self = this; $(\u0026#39;body\u0026#39;).ajaxComplete(function() { $(self.selector).each(function() { $.localtimex.localize(this, format); }); }); } return this.each(function() { $.localtimex.localize(this, format); if ($(this).is(\u0026#34;:input\u0026#34;)) { var self = this; $(this.form).on(\u0026#39;submit form-pre-serialize\u0026#39;, function() { var dt = $.datepicker.parseDate( $(self).datepicker(\u0026#39;option\u0026#39;, \u0026#39;dateFormat\u0026#39;), $(self).val() ); if (dt) { $(self).val(dt.toISOString()); } }); } }); }; $.localtimex = { localize: function(element, format) { if (!$(element).attr(\u0026#39;data-localized\u0026#39;)) { if ($(element).is(\u0026#34;:input\u0026#34;)) { var utcVal = $(element).val(); //input can be empty initially if (utcVal.length) { $(element).attr(\u0026#39;value\u0026#39;, $.localtime.toLocalTime($(element).val(), format)); } } else { $(element).text($.localtime.toLocalTime($(element).text(), format)); } } $(element).attr(\u0026#39;data-localized\u0026#39;, true); } }; }) (jQuery); Links Yii wiki: Local time zones and locales\nYii wiki: Using International Dates\nYii extension: i18n-datetime-behavior\nYii forum: DATETIME and internationalization\nYii forum: Dealing with i18N date formats\nYii playground: User input advanced example\nYii playground: LocaleManager application component\n","href":"/html/2012-09-29-yii-jquery-localtime.html","title":"Yii and jquery.localtime.js - display dates in user local timezone"},{"content":"The jquery.localtime plugin allows to convert date/time strings to a local user time on a client site. By default it works when the page is loaded initially, but if some elements are updated via ajax then they do not converted and left in an UTC format.\nPossible solution is to add some special handling to $.ajax \u0026lsquo;success\u0026rsquo; handlers, but it can require a lot of modifications. Better way is to set some global handler for all ajax requests and apply conversion to local time there. I evaluated several approaches before I could find a working solution.\nFirst try - run local time conversion in $.ajaxComplete (doesn\u0026rsquo;t work) $('body').ajaxComplete(function() { //code from jquery.localtime.js, jQuery.ready handler var format; var localise = function () { if (jQuery(this).is(\u0026quot;:input\u0026quot;)) { jQuery(this).val(jQuery.localtime.toLocalTime(jQuery(this).val(), format)); } else { jQuery(this).text(jQuery.localtime.toLocalTime(jQuery(this).text(), format)); } }; var formats = jQuery.localtime.getFormat(); var cssClass; for (cssClass in formats) { if (formats.hasOwnProperty(cssClass)) { format = formats[cssClass]; //this will try to convert alrady converted texts and cause an exception jQuery(\u0026quot;.\u0026quot; + cssClass).each(localise); } } }); It doesn\u0026rsquo;t work because jQuery(\u0026quot;.\u0026quot; + cssClass).each(localise) will try to apply localization to all elements, including these which already present in the page and converted. And localtime plugin starts throwing exceptions because it can not convert already converted data.\nSecond try - run local time conversion in $.ajaxComplete only for received data (doesn\u0026rsquo;t work) $('body').ajaxComplete(function(event, xhr, ajaxOptions) { //code from jquery.localtime.js, jQuery.ready handler var format; var localise = function () { if (jQuery(this).is(\u0026quot;:input\u0026quot;)) { jQuery(this).val(jQuery.localtime.toLocalTime(jQuery(this).val(), format)); } else { jQuery(this).text(jQuery.localtime.toLocalTime(jQuery(this).text(), format)); } }; var formats = jQuery.localtime.getFormat(); var cssClass; for (cssClass in formats) { if (formats.hasOwnProperty(cssClass)) { format = formats[cssClass]; var result = $(xhr.resultText); result.find(\u0026quot;.\u0026quot; + cssClass).each(localise); xhr.resultText = //convert result back to text } } }); It doesn\u0026rsquo;t work because ajaxComplete is invoked too late - when original $.ajax \u0026lsquo;success\u0026rsquo; handler already done its work.\nThird try - use $.ajaxSetup converters (doesn\u0026rsquo;t work) $.ajaxSetup({ converters: { \u0026quot;html html\u0026quot;: function( textValue ) { return localizeText(textValue); } } }); It doesn\u0026rsquo;t work because converter is invoked only when we have two different formats like \u0026ldquo;text html\u0026rdquo; and we got unexpected format in ajax result.\nFinal try - use $.ajaxSetup prefilters (works!) jQuery(function($) { $.ajaxPrefilter(function(options, originalOptions, jqXHR) { var success = options.success; //if reqested data type was text or html or * if (options.dataTypes.indexOf(\u0026quot;text\u0026quot;) !=-1 || options.dataTypes.indexOf(\u0026quot;html\u0026quot;) != -1 || options.dataTypes.indexOf(\u0026quot;*\u0026quot;) != -1 ) { options.success = function(data, textStatus, jqXHR) { // override success handling data = localizeText(data); if(typeof(success) === \u0026quot;function\u0026quot;) return success(data, textStatus, jqXHR); }; } }); function localizeText(textValue) { var format; var localise = function () { if (jQuery(this).is(\u0026quot;:input\u0026quot;)) { jQuery(this).val(jQuery.localtime.toLocalTime(jQuery(this).val(), format)); } else { jQuery(this).text(jQuery.localtime.toLocalTime(jQuery(this).text(), format)); } }; var formats = jQuery.localtime.getFormat(); var cssClass; //convert text to jQuery var var result = $(textValue); for (cssClass in formats) { if (formats.hasOwnProperty(cssClass)) { format = formats[cssClass]; result.find(\u0026quot;.\u0026quot; + cssClass).each(localise); } } var text = \u0026quot;\u0026quot;; //convert jQuery var back to text $(result).each(function(index){ if (this.nodeName.toLowerCase() == 'script') { text += '\u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt;'+$(this).html()+\u0026quot;\u0026lt;/\u0026quot;+\u0026quot;script\u0026gt;\u0026quot;; } else if (this.nodeType !== 1) { //skip non-elements (HTML comments, text, etc) return; } else { text += $(this).appendTo('\u0026lt;div\u0026gt;').parent().html(); } }); return text; } }); This approach finally works.\nThe tricky part here is also a text-to-jQuery-and-back conversion. While we can create a jQuery object (or array of objects) like this: $(textValue) and then process it, it is not so easy to convert it back to text.\nAfter the conversion we have result as an array of jQuery object and if we do result.html() than we get only inner HTML of the first item.\nAnd even if we wrap the result into a parent div like this $(result).appendTo('\u0026lt;div\u0026gt;').parent().html() than we get HTML but without javascript elements. So we need to iterate over the result and process HTML and javascript elements separately:\n$(result).each(function(index){ if (this.nodeName.toLowerCase() == 'script') { text += '\u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt;'+$(this).html()+\u0026quot;\u0026lt;/\u0026quot;+\u0026quot;script\u0026gt;\u0026quot;; } else if (this.nodeType !== 1) { //skip non-elements (HTML comments, text, etc) return; } else { text += $(this).appendTo('\u0026lt;div\u0026gt;').parent().html(); } }); Links Extending Ajax: Prefilters, Converters, and Transports\njQuery docs: $.ajaxComplete()\nStackowerflow: How can I intercept ajax responses in jQuery before the event handler?\n","href":"/html/2012-09-25-global-ajax-handler-and-localtime.html","title":"global ajax response handler and jquery.localtime plugin"},{"content":"Below are some notes regarding the innodb transactions and locks. Most of them are just a copy-paste from innodb docs with my notes. Also there are some examples of custom-maid deadlocks.\nIn the examples of MySQL operations different sessions are marked as (1) and (2), for example:\ncreate table child (id int(11) NOT NULL) ENGINE=InnoDB; common operation, do it from any session (1) start transaction; do it in session #1 (2) start transaction; do it in session #2 To get two (or more) sessions just launch several instances of mysql in different console windows. Most of information about deadlocks can be retrieved by SHOW ENGINE INNODB STATUS statement. Its output can be improved by creating special tables:\nCREATE TABLE innodb_monitor(a int) ENGINE=INNODB; CREATE TABLE innodb_lock_monitor(a int) ENGINE=INNODB; transaction isolation levels REPEATABLE READ (default) in transaction sequential selects will get the same data READ COMMITTED in transaction sequential selects can get different (new) data READ UNCOMMITTED dirty read, read even not commited data SERIALIZABLE all SELECTSs are done as SELECT \u0026hellip; LOCK IN SHARE MODE so no one can modify data we selected (all selected data becomes read-only) lock modes S (shared) row level, others can get S, IS (select data, but not modify it) X (exclusive) row level, others can do nothing IS (Intention shared) table-level, others can S, IS, IX (can not modify record, but can do IX) IX (Intention exclusive) table-level, others can IS, IX Note: information about IS/IX locks is (at least for me) a bit confusing. Especially following statement was very unclear:\n\u0026quot;For example, SELECT ... LOCK IN SHARE MODE sets an IS lock and SELECT ... FOR UPDATE sets an IX lock.\u0026quot; While in other chapter: \u0026ldquo;SELECT \u0026hellip; LOCK IN SHARE MODE sets a shared mode lock on any rows that are read\u0026hellip; (S-lock)\u0026rdquo; and \u0026ldquo;SELECT \u0026hellip; FOR UPDATE locks the rows and any associated index entries, the same as if you issued an UPDATE statement for those rows. \u0026hellip; (X-lock)\u0026rdquo;\nActually it seems that innodb set both IS and S (or IX and X) where IS is set on the table and S is set on the row, see wikipedia and blog post.\nSELECT ... FOR UPDATE and SELECT ... LOCK IN SHARE MODE Locking Reads - Usage Examples Example 1. Insert a new row into a table child, and make sure that the child row has a parent row in table parent:\nSELECT * FROM parent WHERE NAME = 'Jones' LOCK IN SHARE MODE; Now you can safely add the child record to the CHILD table and commit the transaction. Any transaction that tries to read or write to the applicable row in the PARENT table waits until you are finished, that is, the data in all tables is in a consistent state. Example 2. Consider an integer counter field in a table CHILD_CODES, used to assign a unique identifier to each child added to table CHILD:\nWe can not use here: SELECT - two users could see the same value for the counter, and a duplicate-key error occurs if two transactions attempt to add rows with the same identifier to the CHILD table. SELECT ... LOCK IN SHARE MODE - if wo users read the counter at the same time, at least one of them ends up in deadlock when it attempts to update the counter. Solution 1: First update the counter by incrementing it by 1, then read it and use the new value in the CHILD table. Any other transaction that tries to read the counter waits until your transaction commits. If another transaction is in the middle of this same sequence, your transaction waits until the other one commits. (update will set X-lock) Solution 2: First perform a locking read of the counter using FOR UPDATE, and then increment the counter: SELECT counter_field FROM child_codes FOR UPDATE; (read + set X-lock) UPDATE child_codes SET counter_field = counter_field + 1; Both solutions are only examples for SELECT ... FOR UPDATE. In MySQL, the specific task of generating a unique identifier actually can be accomplished using only a single access to the table: UPDATE child_codes SET counter_field = LAST_INSERT_ID(counter_field + 1); here \u0026ldquo;LAST_INSERT_ID(counter_field + 1)\u0026rdquo; makes the following \u0026ldquo;LAST_INSERT_ID()\u0026rdquo; to return an inserted value (counter_field + 1), so we can know it and use in other statements Record, Gap, and Next-Key Locks Record lock This is a lock on an index record. It lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden clustered index and uses this index for record locking. Gap lock This is a lock on a gap between index records, or a lock on the gap before the first or after the last index record. Next-key lock This is a combination of a record lock on the index record and a gap lock on the gap before the index record. This lock type is used by default in REPEATABLE READ isolation level and prevents selecting phantom rows. If one session has a shared or exclusive lock on record R in an index, another session cannot insert a new index record in the gap immediately before R in the index order. Suppose that an index contains the values 10, 15 and 20. The possible next-key locks for this index cover the following intervals, where ( or ) denote exclusion of the interval endpoint and [ or ] denote inclusion of the endpoint: (negative infinity, 10] (10, 15] (15, 20] (20, positive infinity) For the last interval, the next-key lock locks the gap above the largest value in the index and the supremum pseudo-record having a value higher than any value actually in the index. The \u0026lsquo;supremum\u0026rsquo; is not a real index record, so, in effect, this next-key lock locks only the gap following the largest index value. Note: there is no much information about “supremum”, but it acts as always existing record at the end of the table Example: create table child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB; insert into child (id) values (90),(102); (1) start transaction; (1) select * from child where id \u0026gt; 100 for update; (2) start transaction; (2) insert into child (id) values (101); wait The select above blocked record with id=102, gap before record 102 and also gap before record supremum, so other sessions can not insert something after the record 102 and will wait. Gap locking is not needed for statements that lock rows using a unique index to search for a unique row. This does not include the case that the search condition includes only some columns of a multiple-column unique index; in that case, gap locking does occur. For example, if the id column has a unique index, the following statement uses only an index-record lock for the row having id value 100 and it does not matter whether other sessions insert rows in the preceding gap: SELECT * FROM child WHERE id = 100; If id is not indexed or has a nonunique index, the statement does lock the preceding gap. INSERT operations set a special gap lock called isertion intention gap lock prior to row insertion. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to insert values of 5 and 6 each lock the gap between 4 and 7 with insert intention locks prior to obtaining the exclusive lock on the inserted row, but do not block each other because the rows are nonconflicting. Gap locking can be disabled explicitly. This occurs if you change the transaction isolation level to READ COMMITTED or enable the innodb_locks_unsafe_for_binlog system variable. Under these circumstances, gap locking is disabled for searches and index scans and is used only for foreign-key constraint checking and duplicate-key checking. Locks and statements A locking read (SELECT ... FOR UPDATE and SELECT ... LOCK IN SHARE MODE), an UPDATE, or a DELETE next-key locks (index record + gap before, take into account there is \u0026ldquo;supremum\u0026rdquo; record) Note: next-key locks are used by default is a default. This can be changed by setting a transaction isolation level or with innodb settings It does not matter whether there are WHERE conditions in the statement that would exclude the row. InnoDB does not remember the exact WHERE condition, but only knows which index ranges were scanned. If a secondary index is used in a search and index record locks to be set are exclusive (X), InnoDB also retrieves the corresponding clustered index records and sets locks on them. If you have no indexes suitable for your statement and MySQL must scan the entire table to process the statement, every row of the table becomes locked, which in turn blocks all inserts by other users to the table. It is important to create good indexes so that your queries do not unnecessarily scan many rows. while here: Record locks always lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden clustered index and uses this index for record locking. Does it mean that hidden index is used only for record locks, but not in the case of next-key locks? Check it: create table child (id int(11) NOT NULL) ENGINE=InnoDB; insert into child (id) values (90),(102); (1) start transaction; (1) select * from child where id \u0026gt; 100 for update; (2) start transaction; (2) insert into child (id) values (80); wait so it seems that whole table was locked SELECT ... FROM consistent read, no locks SELECT ... FROM ... LOCK IN SHARE MODE sets shared next-key locks on all index records the search encounters. SELECT ... FROM ... FOR UPDATE blocks other sessions from doing SELECT ... FROM ... LOCK IN SHARE MODE or from reading in certain transaction isolation levels. Consistent reads will ignore any locks set on the records that exist in the read view. Note: it blocks everything except plain SELECT UPDATE ... WHERE ... sets an exclusive next-key lock on every record the search encounters. DELETE FROM ... WHERE ... sets an exclusive next-key lock on every record the search encounters. INSERT sets an exclusive lock on the inserted row. This lock is an index-record lock, not a next-key lock (that is, there is no gap lock) and does not prevent other sessions from inserting into the gap before the inserted row. Prior to inserting the row, an insertion intention gap lock is set. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. Check it: create table child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB; insert into child (id) values (90),(102); (1) start transaction; (1) insert into child (id) values (93); insert intention lock on gap 90-102 X-lock on 93 (2) start transaction; (2) insert into child (id) values (97); insert intention lock on gap 90-102 X-lock on 97 no wait both inserted into the same gap whithout waiting \u0026ndash; (1) start transaction; (1) delete from child where id \u0026gt; 100; X `lock on gap 90-100 (gap before 100), 100, gap 100-supremum (\u0026ldquo;supremum\u0026rdquo; \u0026gt; 100 and fits to condition in \u0026lsquo;where\u0026rsquo;) (2) start transaction; (2) insert into child (id) values (105); insert intention on gap 100-supremum; wait (1) insert into child (id) values (107); OK (1) commit; (2) OK If a duplicate-key error occurs, a shared lock on the duplicate index record is set. no error returned now because because other transaction can be rolled back, so it will be no error for current transaction This use of a shared lock can result in deadlock should there be multiple sessions trying to insert the same row if another session already has an exclusive lock. see deadlock example1 and example2 INSERT ... ON DUPLICATE KEY UPDATE differs from a simple INSERT in that an exclusive next-key lock rather than a shared lock is placed on the row to be updated when a duplicate-key error occurs. REPLACE is done like an INSERT if there is no collision on a unique key. Otherwise, an exclusive next-key lock is placed on the row to be replaced. INSERT INTO T SELECT ... FROM S WHERE ... sets an exclusive index record without a gap lock on each row inserted into T, sets shared next-key locks on rows from S CREATE TABLE ... SELECT ... performs the SELECT with shared next-key locks or as a consistent read, as for INSERT ... SELECT. When a SELECT is used in the constructs REPLACE INTO t SELECT ... FROM s WHERE ... or UPDATE t ... WHERE col IN (SELECT ... FROM s ...), InnoDB sets shared next-key locks on rows from table s. While initializing a previously specified AUTO_INCREMENT column on a table, InnoDB sets an exclusive lock on the end of the index associated with the AUTO_INCREMENT column. In accessing the auto-increment counter, InnoDB uses a specific AUTO-INC table lock mode where the lock lasts only to the end of the current SQL statement, not to the end of the entire transaction. Other sessions cannot insert into the table while the AUTO-INC table lock is held; If a FOREIGN KEY constraint is defined on a table, any insert, update, or delete that requires the constraint condition to be checked sets shared record-level locks on the records that it looks at to check the constraint. InnoDB also sets these locks in the case where the constraint fails. LOCK TABLES sets table locks, but it is the higher MySQL layer above the InnoDB layer that sets these locks.InnoDB is aware of table locks if innodb_table_locks = 1 (the default) and autocommit = 0, and the MySQL layer above InnoDB knows about row-level locks. Otherwise, InnoDB\u0026rsquo;s automatic deadlock detection cannot detect deadlocks where such table locks are involved. Also, because in this case the higher MySQL layer does not know about row-level locks, it is possible to get a table lock on a table where another session currently has row-level locks. However, this does not endanger transaction integrity, see here and here. Deadlock Detection and Rollback InnoDB automatically detects transaction deadlocks and rolls back a transaction or transactions to break the deadlock. InnoDB tries to pick small transactions to roll back, where the size of a transaction is determined by the number of rows inserted, updated, or deleted. InnoDB is aware of table locks if innodb_table_locks = 1 (the default) and autocommit = 0, and the MySQL layer above it knows about row-level locks. Otherwise, InnoDB cannot detect deadlocks where a table lock set by a MySQL LOCK TABLES statement or a lock set by a storage engine other than InnoDB is involved. Resolve these situations by setting the value of the innodb_lock_wait_timeout system variable. When InnoDB performs a complete rollback of a transaction, all locks set by the transaction are released. However, if just a single SQL statement is rolled back as a result of an error, some of the locks set by the statement may be preserved. This happens because InnoDB stores row locks in a format such that it cannot know afterward which lock was set by which statement. Note: statement above looks a bit strange. If a SELECT calls a stored function in a transaction, and a statement within the function fails, that statement rolls back. Furthermore, if ROLLBACK is executed after that, the entire transaction rolls back. How to Cope with Deadlocks InnoDB uses automatic row-level locking. You can get deadlocks even in the case of transactions that just insert or delete a single row. That is because these operations are not really “atomic”; they automatically set locks on the (possibly several) index records of the row inserted or deleted. Techniques: Use SHOW ENGINE INNODB STATUS to determine the cause of the latest deadlock. That can help you to tune your application to avoid deadlocks. This is most important tool to resolve deadlocks. See also how to enable additional lock monitor output Always be prepared to re-issue a transaction if it fails due to deadlock. Deadlocks are not dangerous. Just try again. Note: Usually it is easy to say so, but not so easy to do. Commit your transactions often. Small transactions are less prone to collision. Note: Yes, take this into account, but often having a long transaction is necessary to keep the system in a consistent state. If you are using locking reads (SELECT ... FOR UPDATE or SELECT ... LOCK IN SHARE MODE), try using a lower isolation level such as READ COMMITTED. Note: this depends on situation. Take into account that READ COMMITED is less safe and can you can get phantom records because next-key locking is not used in this case. Access your tables and rows in a fixed order. Then transactions form well-defined queues and do not deadlock. Note: See deadlock example 3 and 4 - these are produced with data access in a different order Add well-chosen indexes to your tables. Then your queries need to scan fewer index records and consequently set fewer locks. Use EXPLAIN SELECT to determine which indexes the MySQL server regards as the most appropriate for your queries. Use less locking. If you can afford to permit a SELECT to return data from an old snapshot, do not add the clause FOR UPDATE or LOCK IN SHARE MODE to it. Using the READ COMMITTED isolation level is good here, because each consistent read within the same transaction reads from its own fresh snapshot. If nothing else helps, serialize your transactions with table-level locks. The correct way to use LOCK TABLES with transactional tables, such as InnoDB tables, is to begin a transaction with SET autocommit = 0 (not START TRANSACTION) followed by LOCK TABLES, and to not call UNLOCK TABLES until you commit the transaction explicitly. For example, if you need to write to table t1 and read from table t2, you can do this: SET autocommit=0; LOCK TABLES t1 WRITE, t2 READ, ...; \u0026hellip; do something with tables t1 and t2 here \u0026hellip; COMMIT; UNLOCK TABLES; Table-level locks make your transactions queue nicely and avoid deadlocks. Create an auxiliary “semaphore” table that contains just a single row. Have each transaction update that row before accessing other tables. In that way, all transactions happen in a serial fashion. Deadlock examples Example 1 (deadlock on insert): create table t1 (i int, primary key (i)) engine = InnoDB; (1) start transaction; (1) insert into t1 values(1); X lock (2) start transaction; (2) insert into t1 values(1); duplicate-key error -\u0026gt; S lock (3) start transaction; (3) insert into t1 values(1); duplicate-key error -\u0026gt; S lock (1) rollback; (2) \u0026amp; (3) - deadlock - both hold S-lock and can not get X-lock Example 2 (deadlock on delete / insert): create table t1 (i int, primary key (i)) engine = InnoDB; (1) start transaction; (1) delete from t1 where i = 1; X lock (2) start transaction; (2) insert into t1 values(1); duplicate-key error -\u0026gt; S lock (3) start transaction; (3) insert into t1 values(1); duplicate-key error -\u0026gt; S lock (1) rollback; (2) \u0026amp; (3) - deadlock - both hold S-lock and can not get X-lock Example 3 (deadlock on access in different order): Example is from this blog post. But I was unable to reproduce a deadlock with example from the post (maybe because mysql improved since then).\nHere is original case:\ncreate table vegetable (id bigint(10) NOT NULL auto_increment, name varchar(255) NOT NULL, PRIMARY KEY (id), UNIQUE KEY uk_name (name)) ENGINE=InnoDB; insert into vegetable (id, name) values (10, \u0026quot;ggg\u0026quot;), (5, \u0026quot;jjj\u0026quot;); (1) start transaction; (1) insert into vegetable values(null, \u0026quot;ppp\u0026quot;); expected: lock the gap between \u0026ldquo;jjj\u0026rdquo; and \u0026ldquo;ppp\u0026rdquo; actually: insert intention lock / X-row lock (2) `start transaction; (2) insert into vegetable values(null, \u0026quot;iii\u0026quot;); expected: locking the gap after \u0026ldquo;ggg\u0026rdquo;, upto \u0026ldquo;iii\u0026rdquo; actually: insert intention lock / X-row lock (2) insert into vegetable values (null, \u0026quot;mmm\u0026quot;); expected: lock the gap after \u0026ldquo;jjj\u0026rdquo; upto \u0026ldquo;mmm\u0026rdquo;. Since connection 1 has a lock between \u0026ldquo;jjj\u0026rdquo; and \u0026ldquo;ppp\u0026rdquo;, effectively spanning the lock connection 2 is attempting to take, this will block actually: no block, OK (1) insert into vegetable values (null, \u0026quot;hhh\u0026quot;); expected: This requires the gap lock between \u0026ldquo;ggg\u0026rdquo; and \u0026ldquo;hhh\u0026rdquo;. This will block as it spans the the lock [\u0026ldquo;ggg\u0026rdquo; to \u0026ldquo;iii\u0026rdquo;] held by connection 2. actually: OK OK because insert do not do next-key blocking, but sets insert intention lock, so such inserts work ok. Modified example with deadlock - inserts are combined with updates which actually do next-key locking:\ncreate table vegetable (id bigint(10) NOT NULL auto_increment, name varchar(255) NOT NULL, PRIMARY KEY (id), UNIQUE KEY uk_name (name)) ENGINE=InnoDB; insert into vegetable (id, name) values (10, \u0026quot;ggg\u0026quot;), (5, \u0026quot;jjj\u0026quot;); (1) start transaction; (1) update vegetable set name=\u0026quot;jjj1\u0026quot; where name \u0026gt; 'jjj'; lock the gap between \u0026ldquo;jjj\u0026rdquo; and \u0026ldquo;supremum\u0026rdquo; (pseudo record beyond the table end) (2) start transaction; (2) update vegetable set name=\u0026quot;ggg1\u0026quot; where name \u0026lt; 'jjj'; locking the gap after \u0026ldquo;ggg\u0026rdquo;, upto \u0026ldquo;jjj\u0026rdquo; (2) insert into vegetable values (null, \u0026quot;mmm\u0026quot;); insert intention lock - gap after \u0026ldquo;jjj\u0026rdquo;, wait for lock set by (1) wait (1) insert into vegetable values (null, \u0026quot;hhh\u0026quot;); insert intention lock - gap after \u0026ldquo;ggg\u0026rdquo;, wait for lock set by (2) deadlock Deadlock info:\nshow engine innodb status\\G; ------------------------ LATEST DETECTED DEADLOCK ------------------------ 120924 13:14:03 *** (1) TRANSACTION: TRANSACTION 3EDB62, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 5 lock struct(s), heap size 320, 6 row lock(s), undo log entries 2 MySQL thread id 50, OS thread handle 0xa6a9db40, query id 512 localhost root update insert into vegetable values (null, \u0026quot;mmm\u0026quot;) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1476 n bits 80 index `uk_name` of table `test_dl`.`vegetable` trx id 3EDB62 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 3EDB61, ACTIVE 22 sec inserting mysql tables in use 1, locked 1 3 lock struct(s), heap size 320, 2 row lock(s), undo log entries 1 MySQL thread id 51, OS thread handle 0xa6aceb40, query id 513 localhost root update insert into vegetable values (null, \u0026quot;hhh\u0026quot;) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 0 page no 1476 n bits 80 index `uk_name` of table `test_dl`.`vegetable` trx id 3EDB61 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1476 n bits 80 index `uk_name` of table `test_dl`.`vegetable` trx id 3EDB61 lock_mode X locks gap before rec insert intention waiting Record lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 3; hex 6a6a6a; asc jjj;; 1: len 8; hex 8000000000000005; asc ;; *** WE ROLL BACK TRANSACTION (2) Example 4 (deadlock on access in different order): CREATE TABLE ... SELECT conflicts with INSERT and causes a deadlock because of opposite order of data (1) INSERT a record at the end, got X-lock (2) CREATE TABLE ... SELECT scans the table from the beginning and sets S-locks on all records. When it reaches the last record it waits. (1) INSERT a record in the middle - try to get X-lock, but there is an S-lock set by (2) which waits for (1) deadlock Example 5 (deadlock on access in different order with complex primary key): create table complex (id1 int(11) not null, id2 int(11) not null, primary key (id1,id2)) engine=InnoDB; insert into complex (id1, id2) values (90, 5),(90,7),(90,9),(102, 5),(102,7),(102,9); (1) start transaction; (1) delete from complex where id1 = 102; Now (1) locks gap before 102/5 and records up to 102/9:\n+-----+-----+ | id1 | id2 | +-----+-----+ | 90 | 5 | | 90 | 7 | | 90 | 9 | ___ | | | | | 102 | 5 | | | 102 | 7 | | (1) | 102 | 9 | ____| +-----+-----+ (2) start transaction; (2) delete from complex where id1 = 90; Now (2) locks records until 102/5. Because key is complex it seems that gap after 90/9 is also locked when table was scanned by delete operation.\n+-----+-----+ | id1 | id2 | +-----+-----+ ___ | 90 | 5 | | | 90 | 7 | | (2) | 90 | 9 | | | | | ___| | 102 | 5 | | 102 | 7 | | 102 | 9 | +-----+-----+ So now the gap between 90/9 and 102/5 is not available for both sessions:\n(1) insert into complex (id1, id2) values (102,1); insert intension on gap between 90/9 and 102/5; wait because it is hold by (2) while insertion of (102,5) would be OK now (2) insert into complex (id1, id2) values (90,10); insert intension on gap between 90 and 102; conflicts with lock hold by (1) which starts with 90/9 now (2) continues and (1) aborted with deadlock: (1) ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction Deadlock info:\nshow engine innodb status/G; ------------------------ LATEST DETECTED DEADLOCK ------------------------ 120924 12:15:44 *** (1) TRANSACTION: TRANSACTION 3EDB4B, ACTIVE 25 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 320, 5 row lock(s), undo log entries 3 MySQL thread id 51, OS thread handle 0xa6aceb40, query id 440 localhost root update insert into complex (id1, id2) values (102,1) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1472 n bits 80 index `PRIMARY` of table `test_dl`.`complex` trx id 3EDB4B lock_mode X locks gap before rec insert intention waiting Record lock, heap no 5 PHYSICAL RECORD: n_fields 4; compact format; info bits 32 0: len 4; hex 80000066; asc f;; 1: len 4; hex 80000005; asc ;; 2: len 6; hex 0000003edb4b; asc \u0026gt; K;; 3: len 7; hex 3c000005e61ccd; asc \u0026lt; ;; *** (2) TRANSACTION: TRANSACTION 3EDB4C, ACTIVE 16 sec inserting mysql tables in use 1, locked 1 4 lock struct(s), heap size 320, 5 row lock(s), undo log entries 3 MySQL thread id 50, OS thread handle 0xa6a9db40, query id 441 localhost root update insert into complex (id1, id2) values (90,10) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 0 page no 1472 n bits 80 index `PRIMARY` of table `test_dl`.`complex` trx id 3EDB4C lock_mode X locks gap before rec Record lock, heap no 5 PHYSICAL RECORD: n_fields 4; compact format; info bits 32 0: len 4; hex 80000066; asc f;; 1: len 4; hex 80000005; asc ;; 2: len 6; hex 0000003edb4b; asc \u0026gt; K;; 3: len 7; hex 3c000005e61ccd; asc \u0026lt; ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1472 n bits 80 index `PRIMARY` of table `test_dl`.`complex` trx id 3EDB4C lock_mode X locks gap before rec insert intention waiting Record lock, heap no 5 PHYSICAL RECORD: n_fields 4; compact format; info bits 32 0: len 4; hex 80000066; asc f;; 1: len 4; hex 80000005; asc ;; 2: len 6; hex 0000003edb4b; asc \u0026gt; K;; 3: len 7; hex 3c000005e61ccd; asc \u0026lt; ;; *** WE ROLL BACK TRANSACTION (1) (1) failed to insert (102,1) while it could insert (102,5) without any problem. In this case can be not that easy to re-order statements to avoid a deadlock. Possible solution can be first insert new records and only then do delete:\n(1) start transaction; (1) insert into complex (id1, id2) values (102,1); (2) start transaction; (2) insert into complex (id1, id2) values (90,10); (1) delete from complex where id1 = 102 and id2 not in (1); X-lock on 102 and gap before it (2) delete from complex where id1 = 90 and id2 not in (10); wait (1) commit; (2) commit; Example 6 (deadlock in empty table): See initial post and explanation.\ncreate table T (C int not null primary key) engine=InnoDB; (1) start transaction; (1) select * from T where C = 42 for update; gap lock \u0026hellip; supremum (whole table) (2) start transaction; (2) select * from T where C = 42 for update; gap lock \u0026hellip; supremum (whole table) why it does not wait? It seems that it gets a lock for the same gap. (1) insert into T set C = 42; insert intention wait for gap lock (2) (2) insert into T set C = 42; insert intention wait for gap lock (1) deadlock Deadlock info:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 120924 14:51:18 *** (1) TRANSACTION: TRANSACTION 3EDB7A, ACTIVE 22 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 320, 2 row lock(s) MySQL thread id 51, OS thread handle 0xa6aceb40, query id 567 localhost root update insert into T set C = 42 *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1489 n bits 72 index `PRIMARY` of table `test_dl`.`T` trx id 3EDB7A lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 3EDB7B, ACTIVE 13 sec inserting mysql tables in use 1, locked 1 3 lock struct(s), heap size 320, 2 row lock(s) MySQL thread id 50, OS thread handle 0xa6a9db40, query id 568 localhost root update insert into T set C = 42 *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 0 page no 1489 n bits 72 index `PRIMARY` of table `test_dl`.`T` trx id 3EDB7B lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1489 n bits 72 index `PRIMARY` of table `test_dl`.`T` trx id 3EDB7B lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** WE ROLL BACK TRANSACTION (2) What seems to be not logical is that both sessions seems to get an X lock on the gap at the end of the table:\ncreate table innodb_monitor (id int); create table innodb_lock_monitor (id int); (1) start transaction; (1) select * from T where C = 42 for update; (2) start transaction; (2) select * from T where C = 42 for update; (3) show engine innodb status\\G; Transactions info:\n------------ TRANSACTIONS ------------ Trx id counter 3EDBDF Purge done for trx's n:o \u0026lt; 3EDBCD undo n:o \u0026lt; 0 History list length 675 LIST OF TRANSACTIONS FOR EACH SESSION: ---TRANSACTION 0, not started MySQL thread id 54, OS thread handle 0xa6affb40, query id 1067 localhost root show engine innodb status ---TRANSACTION 3EDBDE, ACTIVE 4 sec 2 lock struct(s), heap size 320, 1 row lock(s) MySQL thread id 50, OS thread handle 0xa6a9db40, query id 1066 localhost root TABLE LOCK table `test_dl`.`T` trx id 3EDBDE lock mode IX RECORD LOCKS space id 0 page no 1513 n bits 72 index `PRIMARY` of table `test_dl`.`T` trx id 3EDBDE lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; ---TRANSACTION 3EDBDD, ACTIVE 15 sec 2 lock struct(s), heap size 320, 1 row lock(s) MySQL thread id 51, OS thread handle 0xa6aceb40, query id 1064 localhost root TABLE LOCK table `test_dl`.`T` trx id 3EDBDD lock mode IX RECORD LOCKS space id 0 page no 1513 n bits 72 index `PRIMARY` of table `test_dl`.`T` trx id 3EDBDD lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; Both transactions hold the same X lock. See my post with question.\nExample 7 (deadlock after delete non-existing data), similar to example 6: create table d (id int not null primary key) engine=InnoDB; insert into d values (1),(2),(3); (1) start transaction; (1) delete from d where id = 4; lock on gap at the end of table (2) start transaction; (2) delete from d where id = 5; lock on gap at the end of table (1) insert into d values (4); wait (2) insert into d values (5); deadlock Deadlock info:\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 120924 17:44:05 *** (1) TRANSACTION: TRANSACTION 3EDBD2, ACTIVE 40 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 320, 2 row lock(s) MySQL thread id 51, OS thread handle 0xa6aceb40, query id 1001 localhost root update insert into d values (4) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1507 n bits 72 index `PRIMARY` of table `appceo`.`d` trx id 3EDBD2 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 3EDBD3, ACTIVE 18 sec inserting mysql tables in use 1, locked 1 3 lock struct(s), heap size 320, 2 row lock(s) MySQL thread id 50, OS thread handle 0xa6a9db40, query id 1002 localhost root update insert into d values (5) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 0 page no 1507 n bits 72 index `PRIMARY` of table `appceo`.`d` trx id 3EDBD3 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 0 page no 1507 n bits 72 index `PRIMARY` of table `appceo`.`d` trx id 3EDBD3 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** WE ROLL BACK TRANSACTION (2) With lock monitor enabled we can check transactions state before producing a deadlock:\ncreate table innodb_monitor (id int); create table innodb_lock_monitor (id int); (1) start transaction; (1) delete from d where id = 4; (2) start transaction; (2) delete from d where id = 5; (3) show engine innodb status\\G; Transactions info:\n------------ TRANSACTIONS ------------ Trx id counter 3EDBDC Purge done for trx's n:o \u0026lt; 3EDBCD undo n:o \u0026lt; 0 History list length 675 LIST OF TRANSACTIONS FOR EACH SESSION: ---TRANSACTION 0, not started MySQL thread id 54, OS thread handle 0xa6affb40, query id 1028 localhost root show engine innodb status ---TRANSACTION 3EDBDB, ACTIVE 11 sec 2 lock struct(s), heap size 320, 1 row lock(s) MySQL thread id 50, OS thread handle 0xa6a9db40, query id 1027 localhost root TABLE LOCK table `appceo`.`d` trx id 3EDBDB lock mode IX RECORD LOCKS space id 0 page no 1507 n bits 72 index `PRIMARY` of table `db`.`d` trx id 3EDBDB lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; ---TRANSACTION 3EDBDA, ACTIVE 17 sec 2 lock struct(s), heap size 320, 1 row lock(s) MySQL thread id 51, OS thread handle 0xa6aceb40, query id 1025 localhost root TABLE LOCK table `appceo`.`d` trx id 3EDBDA lock mode IX RECORD LOCKS space id 0 page no 1507 n bits 72 index `PRIMARY` of table `db`.`d` trx id 3EDBDA lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; ","href":"/html/2012-09-24-innodb.html","title":"innodb notes"},{"content":"","href":"/tags/mysql/","title":"mysql"},{"content":"Below are some git commands which can be useful to resolve problems with submodules.\nGet a list of commits inside a submodule Submodules are identified by SHA-1 hashes so you may need to get a list of them. Do the following inside the submodule folder (or inside the separate submodule repository):\ngit log --oneline 5bd722f commit 5 b5c1524 commit 4 2444bfa commit 3 e0eadd5 commit 2 c180c5a commit 1 702fc8a commit 0 View current submodule commit git submodule status 5bd722fa26dcdd64128392aa28e08849fe37f111 sub (heads/master) Compare a submodule state with another branch Assume we are on the \u0026ldquo;branch1\u0026rdquo; and want to compare \u0026lsquo;sub\u0026rsquo; submodule state with master:\ngit diff master -- sub diff --git a/sub b/sub index 702fc8a..5bd722f 160000 --- a/sub +++ b/sub @@ -1 +1 @@ -Subproject commit 702fc8a7edcdf5ceba9929958bd6cd7f000eb369 +Subproject commit 5bd722fa26dcdd64128392aa28e08849fe37f111 Where \u0026ldquo;-Subproject commit\u0026rdquo; is another (master) branch state and \u0026ldquo;+Subproject commit\u0026rdquo; is a current branch state. VIEW CHANGES TO SUBMODULE STATE ON THE CURRENT BRANCH What we do is show a git log with diffs for submodule (named \u0026lsquo;sub\u0026rsquo; in this case):\ngit log -p sub commit 261aa4bdb6944c77fc98e52748d656e56969ba6a Author: name Date: Thu Sep 13 15:20:48 2012 +0300 accidental sub switch to commit 5 diff --git a/sub b/sub index b5c1524..5bd722f 160000 --- a/sub +++ b/sub @@ -1 +1 @@ -Subproject commit b5c15241fe40f122d5225e5c76457802de3ad605 +Subproject commit 5bd722fa26dcdd64128392aa28e08849fe37f111 commit 073aaf160706d705c6c95ab21327388cd6683486 Author: name Date: Thu Sep 13 14:58:41 2012 +0300 sub commit 4 diff --git a/sub b/sub index 2444bfa..b5c1524 160000 --- a/sub +++ b/sub @@ -1 +1 @@ -Subproject commit 2444bfa760b5d3cc974658bd18482f9679f52b69 +Subproject commit b5c15241fe40f122d5225e5c76457802de3ad605 ... Where \u0026ldquo;-Subproject commit\u0026rdquo; is a previous submodule state and \u0026ldquo;+Subproject commit\u0026rdquo; is a new submodule state.\nLinks GitSubmoduleTutorial\nPro Git: 6.6 Git Tools - Submodules\nStackoverflow: git submodule update\nUnderstanding Git Submodules\ngit Submodules Explained\n","href":"/html/2012-06-15-android-mobile-network-problem.html","title":"git - submodule helpers"},{"content":"","href":"/tags/yii/","title":"yii"},{"content":"Assume we have an action \u0026ldquo;articles/get\u0026rdquo; which accepts optional parameters and we want to setup following URLs:\narticles/[article id or name] articles/[article id or name]/draft articles/[article id or name]/revisions/99 articles/[article id or name]/revisions/98/draft articles/revisions/[revision id] articles/revisions/[revision id]/draft We have a list of articles and each article has several revisions. Also each revision can have draft and published version.\nIn the code we have a single \u0026lsquo;article/get\u0026rsquo; action which allows us to get specific article (last revision) by name (\u0026lsquo;GET articles/my-article\u0026rsquo;) or id (\u0026lsquo;GET articles/33\u0026rsquo;).\nUsing additional parameters we can get last revision draft (\u0026lsquo;GET articles/33/draft\u0026rsquo;), get specific revision (\u0026lsquo;GET articles/33/revisions/99\u0026rsquo;) or get article using revision ID only (\u0026lsquo;GET articles/revisions/99\u0026rsquo;).\nNow we can setup URL rules for these routes like this:\n//- articles/revisions/98/draft array('article/get', 'pattern' =\u0026gt; 'articles/revisions/\u0026lt;revision:\\d+\u0026gt;/\u0026lt;version:published|draft\u0026gt;', 'verb' =\u0026gt; 'GET' ), //- articles/revisions/99 array('article/get', 'pattern' =\u0026gt; 'articles/revisions/\u0026lt;revision:\\d+\u0026gt;', 'verb' =\u0026gt; 'GET' ), //- articles/77/revisions/99/draft array('article/get', 'pattern' =\u0026gt; 'articles/\u0026lt;article:[\\w\\d\\.]+\u0026gt;/revisions/\u0026lt;revision:\\d+\u0026gt;/\u0026lt;version:published|draft\u0026gt;', 'verb' =\u0026gt; 'GET' ), //- articles/77/revisions/99 array('article/get', 'pattern' =\u0026gt; 'articles/\u0026lt;article:[\\w\\d\\.]+\u0026gt;/revisions/\u0026lt;revision:\\d+\u0026gt;', 'verb' =\u0026gt; 'GET' ), //- articles/77/draft array('article/get', 'pattern' =\u0026gt; 'articles/\u0026lt;revision:[\\w\\d\\.]+\u0026gt;/\u0026lt;version:published|draft\u0026gt;', 'verb' =\u0026gt; 'GET' ), // articles/77 (or articles/[name]) array('articles/get', 'pattern' =\u0026gt; 'article/\u0026lt;article:[\\w\\d\\.]+\u0026gt;', 'verb' =\u0026gt; 'GET' ), But there is much better option. We can describe this a single URL route with optional parameters (regexp groups with trailing question mark do the trick):\narray('app/get', 'pattern' =\u0026gt; 'apps(/\u0026lt;app:[\\w\\d\\.]+\u0026gt;)?(/updates/\u0026lt;update:\\d+\u0026gt;)?(/\u0026lt;revision:published|draft\u0026gt;)?', 'verb' =\u0026gt; 'GET' ), Optional parameters at the end The special case is when we need to allow any number of additional parameters at the end of the URL. This case is supported by URL manager:\nIf a pattern ends with '/*', it means additional GET parameters may be appended to the path info part of the URL; otherwise, the GET parameters can only appear in the query string part. ","href":"/html/2012-09-13-yii-url-rules-optional-parameters.html","title":"Yii url rules - optional parameters"},{"content":"","href":"/tags/ejs/","title":"ejs"},{"content":"","href":"/tags/express.js/","title":"express.js"},{"content":"ejs has a client-side support but documentation and examples do not describe how to reuse the same template on the server and on the client side.\nFor now I found two ways to do it. First way is to send a request from the client, get a template from a file and render it. And the second - put a template into the page when render it on the server and then just use the template on the client.\nDynamically get a template from the server and render it To be able to load a template from a file we need to put shared templates (used both on the sever and the client) under the \u0026ldquo;public\u0026rdquo; folder. Assume we have \u0026ldquo;public/tpl/template.ejs\u0026rdquo;.\nRender on the server (this code will be inside of a parent view which uses \u0026rsquo;template.ejs\u0026rsquo; as partial):\n\u0026lt;div class=\u0026#34;dataItems\u0026#34;\u0026gt; \u0026lt;% for (var i=0; i \u0026lt; dataItems.length; i++) { %\u0026gt; \u0026lt;%- partial(\u0026#39;../public/tpl/template\u0026#39;, {data: dataItems[i]}) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; Render on the client (you need to include client version of ejs.js into the page):\nfunction insertTemplate(data) { getTemplate(\u0026#39;/tpl/template.ejs\u0026#39;, function renderTemplate(err, tpl) { if (err) { throw err; } var tpl = require(\u0026#39;ejs\u0026#39;).render(tpl, {data: data}); $(tpl).prependTo(\u0026#39;div.dataItems\u0026#39;); }); } function getTemplate(file, callback) { $.ajax(file, { type: \u0026#39;GET\u0026#39;, success: function(data, textStatus, xhr) { return callback(null, data); }, error: function(xhr, textStatus, error) { return callback(error); } }); } Here the \u0026lsquo;getTemplate\u0026rsquo; function will load a template from the file on the server and return its contents. To reduce server load it could be enhanced to cache already loaded templates.\nPrepare a Template on the Server and Use it on the Client With this method we do not need to put the template into the public folder. Assume we have our template in views/_template.ejs. Before rendering the view we put this template into the variable:\nvar templates = { template: require(\u0026#39;fs\u0026#39;).readFileSync(\u0026#39;./views/_template.ejs\u0026#39;, \u0026#39;utf-8\u0026#39;) }; res.render(\u0026#39;myview\u0026#39;, { // …, templates: templates }); The server-side rendering will be usual use of partial. Another thing we need to do inside the view is to convert \u0026rsquo;templates\u0026rsquo; view variable into client-side javascript variable.\n\u0026lt;div class=\u0026#34;dataItems\u0026#34;\u0026gt; \u0026lt;% for (var i=0; i \u0026lt; dataItems.length; i++) { %\u0026gt; \u0026lt;%- partial(\u0026#39;_template\u0026#39;, {data: dataItems[i]}) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; jQuery(document).ready(function(){ //save templates to variable window.templates = \u0026lt;%-JSON.stringify(templates)%\u0026gt;; }); \u0026lt;/script\u0026gt; Now to render the template on the client:\nvar tplData = {...}; var html = require(\u0026#39;ejs\u0026#39;).render(templates.template, {data: tplData}); $(html).prependTo(\u0026#39;div.dataItems\u0026#39;); Links Discussion in the ejs issue tracker\n","href":"/html/2012-08-20-expressjs-ejs-reuse-templates.html","title":"express.js and ejs - reuse template on server and client"},{"content":"","href":"/tags/selenium/","title":"selenium"},{"content":"Problem Selenium 2.25.0, python 2.7, ubuntu 12.04, Firebox 4.0 (yes, old version but we need it) hangs when uploading a file larger then (about) 600KB.\nSolution For now I fixed this by commenting out local file upload (three first lines):\ndef send_keys(self, *value): \u0026#34;\u0026#34;\u0026#34;Simulates typing into the element.\u0026#34;\u0026#34;\u0026#34; #local_file = LocalFileDetector.is_local_file(*value) #if local_file is not None: #value = self._upload(local_file) typing = [] for val in value: if isinstance(val, Keys): typing.append(val) elif isinstance(val, int): val = str(val) for i in range(len(val)): typing.append(val[i]) else: for i in range(len(val)): typing.append(val[i]) self._execute(Command.SEND_KEYS_TO_ELEMENT, {\u0026#39;value\u0026#39;: typing}) Usually I do not change external libraries code, but in this case it is easiest way and it should not be necessary (I hope) after next selenium library update.\nLinks Bug report\n","href":"/html/2012-08-20-selenium-problem-with-big-file-upload.html","title":"selenium - problem with big file upload"},{"content":"This can be necessary for example for selector like #id \u0026gt; li:visible.\nIf you will try to do webdriver.find_element_by_css_selector you will get an error message \u0026ldquo;The given selector #id \u0026gt; li:visible is either invalid or does not result in a WebElement.\u0026rdquo;\nThe workaround is to use jQuery to find element. It can be done with this code (python):\nscript = \u0026quot;return $('\u0026quot;+selector+\u0026quot;').get(0);\u0026quot; element = webdriver.execute_script(script); ","href":"/html/2012-08-02-selenium-webdriver-element-by-jquery-selector.html","title":"selenium webdriver - get webelement by jQuery selector"},{"content":"To set the php session cookie we can use the addcookie (or python version add_cookie) method of the webdriver. But it accepts only name and value and does not allow to set additional cookie parameters like domain, path, etc.\nFortunately it is easy to do with javascript.\nHere is example of a JS code to set the cookie:\ndocument.cookie = \u0026quot;PHPSESSID=9ojofgkb21nujvhulvgq4drh06; domain=.myhost.com; path=/\u0026quot;; And here is python code version (assume you have set \u0026lsquo;cookie\u0026rsquo; and \u0026lsquo;domain\u0026rsquo; variables:\nwebdriver.execute_script(\u0026quot;document.cookie = 'PHPSESSID=\u0026quot;+cookie+\u0026quot;; domain=\u0026quot;+domain+\u0026quot;; path=/'\u0026quot;) ","href":"/html/2012-07-24-selenium-webdriver-set-php-session-cookie.html","title":"selenium webdriver - set php session cookie"},{"content":"The \u0026rsquo;executeScript\u0026rsquo; method of the webdriver receives additional \u0026lsquo;arguments\u0026rsquo; variable and we can pass WebElement instances to the script. So trigger an event on the elemen can be done like this (python):\nevent = 'click' #or 'hover' or any other script = \u0026quot;$(arguments[0]).trigger('\u0026quot;+event+\u0026quot;')\u0026quot; webdriver.execute_script(script, web_element) Links Stackoverflow question\n","href":"/html/2012-07-24-selenium-webdriver-trigger-event-via-jquery.html","title":"selenium webdriver - trigger event on element via jQuery"},{"content":"To use fugitive as a mergetool you can run the following commands:\ngit config --global mergetool.fugitive.cmd \u0026#39;vim -f -c \u0026#34;Gdiff\u0026#34; \u0026#34;$MERGED\u0026#34;\u0026#39; git config --global merge.tool fugitive Links stackoverflow fugitive screencast\n","href":"/html/2012-04-24-git-fugitive-to-resolve-conflicts.html","title":"git - use vim with fugitive to resolve merge conflicts"},{"content":"","href":"/tags/vim/","title":"vim"},{"content":"By default angular.js sends all data in json. So if you do a POST request to a PHP code then the $_POST superglobal will not be populated.\nThis can be solved in two ways - on the client side or on the server side.\nServer-side solution On the server you can parse input and then decode data from json:\n$data = file_get_contents(\u0026#34;php://input\u0026#34;); $postData = json_decode($data); Client-side solution On the client side the data can be sent in a way PHP expects it:\n$http({ url:url data : $.param(data), method : \u0026#39;POST\u0026#39;, headers : {\u0026#39;Content-Type\u0026#39;:\u0026#39;application/x-www-form-urlencoded; charset=UTF-8\u0026#39;} }).success(callback); Here we set the \u0026lsquo;Content-Type\u0026rsquo; header and encode data using jQuery\u0026rsquo;s $.param. This also can be done for all requests as described on stackoverflow.\n","href":"/html/2013-05-24-angular-post-to-php.html","title":"Angular.js POST data to PHP"},{"content":"GImport yii extension implements recursive import of directories with caching.\nImport is performed recursively for specified path alias. Classes found are cached, so the import process can be slow only first time.\nBasic usage example:\n$importer = new GImport; $importer-\u0026gt;add(\u0026#39;modules.myModule.*\u0026#39;); This code will import all clasees from modules/myModule/ directory.\nGImport can also be configured as application component. Add following code into the application config:\nreturn array( ... \u0026#39;preload\u0026#39; =\u0026gt; array(\u0026#39;log\u0026#39;, \u0026#39;import\u0026#39;), ... \u0026#39;components\u0026#39; =\u0026gt; array( \u0026#39;import\u0026#39; =\u0026gt; array( \u0026#39;class\u0026#39;=\u0026gt;\u0026#39;GImport\u0026#39;, \u0026#39;import\u0026#39; =\u0026gt; array( // add directories to import and // put \u0026#39;import\u0026#39; component to preload to trigger import // on application initialization \u0026#39;application.extensions.*\u0026#39;, ), ), ... Download at extension page.\nForum topic.\n","href":"/html/2012-10-10-gimport.html","title":"GImport yii extension"},{"content":"To remove files that were already deleted on the file system, run git add with -u flag:\ngit status D abc.c git add -u This tells git to automatically stage tracked files - including deleting the previously tracked files.\nStackoverflow: How do I commit all deleted files in Git?\n","href":"/html/2012-10-01-git-rm-deleted-files.html","title":"git - remove already deleted files"},{"content":"Rename local branch:\ngit branch -m old-branch-name new-branch-name Rename remote branch:\n#delete remote branch with old name git push origin :old-branch-name # create remote renamed branch git push origin new-branch-name Links stackoverflow\n","href":"/html/2012-03-15-oauth-1-0.html","title":"git - rename branch (local and remote)"},{"content":"C++ allows to declare one class as a friend of another one.\nThis can be useful if you want to keep some details of class protected, but available for another particular (friend) class.\nFor example this can be used in State pattern to keep setState method of context class protected.\nTo emulate this in PHP we can inherit state class from context class:\nclass AContext { private $_state; protected function setState(AState $state) { $this-\u0026gt;_state = $state; } public function request() { $this-\u0026gt;_state-\u0026gt;handle(); } } abstract class AState extends AContext { private $_owner; public function __construct(AContext $owner) { $this-\u0026gt;_owner = $owner; } protected function getOwner() { return $this-\u0026gt;_owner; } abstract function handle(); } class AConcreteState extends AState { public function handle() { ... $this-\u0026gt;getOwner()-\u0026gt;setState(new AnotherState($this-\u0026gt;getOwner()); } } class AnotherState extends AState { ... } ","href":"/html/2012-12-09-php-friend-via-extend.md","title":"PHP - friend a class via extend"},{"content":"Error when trying to access phpmyadmin (in Chrome):\nError 324 (net::ERR_EMPTY_RESPONSE): The server closed the connection without sending any data. The easiest way to fix I found is to disable eaccelerator in .htaccess (create it in the phpmyadmin root folder and add this line:\nphp_flag eaccelerator.enable 0 ","href":"/html/2012-10-09-phpmyadmin-and-eaccelerator-problem.html","title":"phpmyadmin and eaccelerator problem"},{"content":"There are several ways to speedup slow unit tests which interact with database:\nRefactor code and tests and do not interact with db in unit tests Use sqlite db in memory instead of MySql Use MySql MEMORY engine Move MySql data to memory It is better to try other listed approaches and I think of last method as of quick temporary hack, but here it is:\nstop mysql move /var/lib/mysql to /dev/shm/mysql link /var/lib/mysql to /dev/shm/mysql start mysql In Ubuntu there is also a problem with apparmor which will not allow mysql to read from /dev/shm. To fix this it is recommended to add following to the /etc/apparmor.d/usr.sbin.mysqld:\n/dev/shm/mysql/ r, /dev/shm/mysql/** rwk, But it doesn\u0026rsquo;t work for me and I disabled apparmor for mysql (not recommended):\nsudo mv /etc/apparmor.d/usr.sbin.mysqld /etc/apparmor.d/disable Below are shell scripts to move MySql data to /dev/shm and back, restore backed up data and check db state.\nMove db to memory script #!/bin/sh #Check if run as root if [ `whoami` != root ] then echo \u0026#34;You must be root to do that!\u0026#34; exit 1 fi service mysql stop if [ ! -s /var/lib/mysql.backup ] then cp -pRL /var/lib/mysql /var/lib/mysql.backup fi mv /var/lib/mysql /dev/shm/mysql chown -R mysql:mysql /dev/shm/mysql ln -s /dev/shm/mysql /var/lib/mysql chown -h mysql:mysql /var/lib/mysql service mysql start Move db to disk script #!/bin/sh #Check if run as root if [ `whoami` != root ] then echo \u0026#34;You must be root to do that!\u0026#34; exit 1 fi service mysql stop rm /var/lib/mysql if [ ! -s /dev/shm/mysql ] then cp -pRL /var/lib/mysql.backup /var/lib/mysql else mv /dev/shm/mysql /var/lib/mysql fi service mysql start Restore db backup script #!/bin/sh #Check if run as root if [ `whoami` != root ] then echo \u0026#34;You must be root to do that!\u0026#34; exit 1 fi service mysql stop if [ ! -s /var/lib/mysql.backup ] then exit -1 fi rm /var/lib/mysql cp -pRL /var/lib/mysql.backup /var/lib/mysql rm -rf /dev/shm/mysql service mysql start Check db state script #!/bin/sh #Check if run as root if [ `whoami` != root ] then echo \u0026#34;You must be root to do that!\u0026#34; exit 1 fi if [ -L /var/lib/mysql ] then echo \u0026#34;Mem db\u0026#34; exit 0 else echo \u0026#34;File db\u0026#34; exit 1 fi Links Unit test application including database is too slow\nForce an entire MySQL database to be in memory\nHow to run django\u0026rsquo;s test database only in memory?\nScript to put mysqld on a ram disk in ubuntu 10.04. Runs on every hudson slave boot\nMySQL tmpdir on /dev/shm\nHow can I get around MySQL Errcode 13 with SELECT INTO OUTFILE?\n","href":"/html/2012-12-17-unit-speed-mysql-to-mem.html","title":"Speedup unit tests by moving MySql data to memory [Ubuntu]"},{"content":"It is often useful to copy, let\u0026rsquo;s say new_name, to clipboard and then replace one or more occurrences of old_name with it. We can use :%s/new_name/old_name/g for that, but sometimes it is more convenient to go over the text and do replacements \u0026ldquo;manually\u0026rdquo;.\nA possible way to do that in vim is:\nYank inner word: yiw (the new_name is now in the default register \u0026quot;\u0026quot;) Move cursor to another word (the old_name) Select it and paste over: viwp (viw - visual inner word, p - paste) What happens next might be unexpected in this scenario: when we pasted over the old_name, it was deleted and put into the default Vim register instead of new_name, so next time we use p, it will paste the old_name (tldr: the xnoremap p pgvy mapping helps fix this).\nThis behavior is nice when we actually deleted something, for example, if we did dw to delete old_name, we expect to be able to paste it with p, but this is less expected in the case the deletion was implicit, as in the case of the pasting over slection.\nAt this point, we have a couple of choices to continue pasting new_name:\nOption 1: gvy (gv - reselect last selection, y - copy) to reselect new_name and copy it again and then do viwp on another word Option 2: use viw\u0026quot;0p on another word; here \u0026quot;0p will paste from 0 register (the default register now contains old_name and \u0026quot;0 register contains the previously yanked new_name) In more detail, what happens during this process is:\nDeleting, changing and yanking text copies the affected text to the unnamed register (\u0026quot;\u0026quot;). Yanking text also copies the text to register 0 (\u0026quot;0). We yank inner word: yiw (the new_name), it is now in \u0026quot;\u0026quot; and \u0026quot;0 We move cursor to another word (the old_name) We select the old_name and paste over: viwp, the deleted old_name is in \u0026quot;\u0026quot; and the new_name is still in \u0026quot;0p Next time we use p, it pastes old_name since p uses \u0026quot;\u0026quot; register by default (but we can instruct p to use the \u0026quot;0 register with \u0026quot;0p) Solution The solution that works quite well for me is the following mapping\nxnoremap p pgvy As mentioned above, the default Vim behavior is counter-intuitive when we paste over the selection: most of the time we do not need the text we pasted over (so do not want it to get into the default \u0026quot;\u0026quot; register), but the same behavior is useful when we explicitely delete something (dw a word and the paste it somewhere else).\nThe mapping above only redefines the first case:\nxnoremap - map only in visual mode, when we have something selected p - redefine p behavior pgvy - p past, gv reselect what we\u0026rsquo;ve just pasted, y copy back what we\u0026rsquo;ve just pasted With this mapping, we can still use Vim\u0026rsquo;s standard behavior if needed by using gp instead of p (gp is almost the same as p, with cursor left after the pasted text instead of the last char of it).\nAnother solution that seems simple is to have a xnoremap p \u0026quot;0p that would always use the \u0026quot;0p register when pasting in visual mode (in the example above, we saw that new_name stays in \u0026quot;0p during the whole process). But there is a negative side-effect: when we delete something explicitely, it gets into \u0026quot;\u0026quot;, but not into \u0026quot;0p, so we can not then paste it in visual mode. For example, select and copy irrelevant_name with yiw, delete new_name with dw, select old_name and paste with yiwp - the irrelevant_name will be pasted (whie new_name is expected).\nOne more solution is to use named registeres:\nYank the new_name into a specific register, for example, the a register: \u0026quot;ayiw Move the cursor to the old_name, select it with viw Paste the contents of the a register over the selected text: \u0026quot;ap This works, but I need to remember to use the named register before I start the operation and it also requires typing the \u0026quot;a prefix, which is not very convenient.\nI recommend the xnoremap p pgvy, it works well and I did not find any drawbacks while using it.\nLinks vim.wikia.com Stackoverflow: Paste multiple times\n","href":"/html/2012-04-03-vim-replace-word-with-yanked-text.html","title":"vim - replace a word with yanked text multiple times"},{"content":"This method allows to log InnoDB monitor output when deadlock error occured. This way we will have much more useful data to find and fix deadlock.\nExtend error handler class:\nclass AppErrorHandler extends CErrorHandler { protected function handleException($exception) { /* CDbCommand failed to execute the SQL statement: SQLSTATE[40001]: * Serialization failure: 1213 Deadlock found when trying to get lock; * try restarting transaction. The SQL statement executed was: * INSERT INTO `table_name` (`id`, `name`) VALUES (:yp0, :yp1) */ //can we check $exception-\u0026gt;getCode() ? if ($exception instanceof CDbException \u0026amp;\u0026amp; strpos($exception-\u0026gt;getMessage(), \u0026#39;Deadlock\u0026#39;) !== false ) { $data = Yii::app()-\u0026gt;db-\u0026gt;createCommand(\u0026#39;SHOW ENGINE INNODB STATUS\u0026#39;)-\u0026gt;query(); $info = $data-\u0026gt;read(); $info = serialize($info); Yii::log(\u0026#39;Deadlock error, innodb status: \u0026#39; . $info, CLogger::LEVEL_ERROR,\u0026#39;system.db.CDbCommand\u0026#39;); } return parent::handleException($exception); } } Put it in application/protected/components and set in the config/main.php:\nreturn array( ... \u0026#39;components\u0026#39; =\u0026gt; array( \u0026#39;errorHandler\u0026#39; =\u0026gt; array( \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;AppErrorHandler\u0026#39;, ), ), ... ); Links InnoDB Lock Modes\nInnoDB Record, Gap, and Next-Key Locks\nMySQL Forums :: Transactions :: Deadlock With DELETE/INSERT Queries\nMySQL Forums :: InnoDB :: Explain deadlock locking the same index\n","href":"/html/2012-03-28-yii-catch-and-log-deadlocks.html","title":"yii - catch and log MySQL deadlock errors"},{"content":"","href":"/tags/oauth/","title":"oauth"},{"content":" oAuth 1.0 flow A good explanation image from oauth.net:\nFlow description:\nConsumer has Consumer Key and Consumer Secret (shared secret) A) Consumer requests Request Token call get_request_token from Service Provider, send oauth_consumer_key oauth_signature_method oauth_signature \u0026hellip; here oauth_signature - is signature of the request created using Consumer Secret, simplified example: $signature = md5($request_text . $consumer_secret) both sides (Consumer and Service Provider) knows consumer_secret and able to perform this operation, so Service Provider can check whether signature is valid B) Service provider returns Request Token oauth_token oauth_token_secret C) Consumer redirects User to Service Provider oauth_token (request token from B) D) User confirms access and Service Provider redirects User to Consumer oauth_token (request token from B) oauth_verifier (request token verifier) E) Consumer requests Access Token call get_access_token, send oauth_consumer_key oauth_token (request token from B) oauth_signature_method oauth_signature \u0026hellip; oauth_verifier here oauth_signature - is signature of the request created using request token secret from B note, that on step A Consumer uses his Consumer Secret to sign the request and here he use request token secret F) Service provider grants Access Token oauth_token oauth_token_secret G) Consumer Accesses Protected Resources request includes oauth_consumer_key oauth_token (request token from F) oauth_signature_method oauth_signature \u0026hellip; here oauth_signature created using Access Token secret Links wikipedia\noauth.net\noauth guide\nyahoo\nintroducion to oauth\nhabrahabr, in russian\nyii implementations eauth extension\neoauth extension\napi module extension\nphp implementations list of libraries on oauth.net\nlist of libraries on twitter.com\nphp extension\noauth php project\noauth project\nzend\n","href":"/html/2012-02-28-git-default-color-diff.html","title":"oauth 1.0 notes"},{"content":"To have colored git commands output, use the following command:\ngit config color.ui true or (globally)\ngit config --global color.ui true Alternatively, you can set color for individual git commands.\ngit config color.branch auto git config color.diff auto git config color.interactive auto git config color.status auto See also color.* options in the git config docs.\nLinks git book\nColor highlighted diffs with git, svn and cvs\n","href":"/html/2012-02-13-git-branches-have-diverged.html","title":"git - colored diff, branch, etc output by default"},{"content":"Problem: selenium fails to start Firefox with following error:\n'The browser appears to have exited before we could connect. The output was: Failed to dlopen /usr/lib/libX11.so.6\\ndlerror says: /usr/lib/libX11.so.6: wrong ELF class: ELFCLASS32\\n' In my case it was reproduced on the 64 bit machine with Amazon Linux AMI. The problem itself is known and there is an issue in selenium tracker.\nIt is because x_ignore_nofocus library tries to load 32bit version of the libX11 instead of 64bit. In my system there are following versions of libX11:\n$ find / | grep libX11.so.6 /usr/lib64/libX11.so.6 \u0026lt;-- symbolic link to `libX11.so.6.3.0\u0026#39; /usr/lib64/libX11.so.6.3.0 \u0026lt;-- ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, stripped /usr/lib/libX11.so.6 \u0026lt;-- symbolic link to `libX11.so.6.3.0\u0026#39; /usr/lib/libX11.so.6.3.0 \u0026lt;-- ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, stripped At the right side (after \u0026ldquo;\u0026lt;\u0026ndash;\u0026rdquo;) is an output of file /usr/libX/libX11.so.6xxx.\nSolution Change symbolic link /usr/lib/libX11.so.6 to point to the 64-bit version /usr/lib64/libX11.so.6.3.0:\n$ mv /usr/lib/libX11.so.6 /usr/lib/libX11.so.6.bak $ ln -s /usr/lib64/libX11.so.6.3.0 /usr/lib/libX11.so.6 After that selenium started to work.\nLinks bug report.\n","href":"/html/2012-02-20-selenium-64bit-x_ignore_nofocus-problem.html","title":"selenium - problem with loading x_ignore_nofocus.so"},{"content":"Selenium tests require browser to run, so usually we run them on the X-server enabled machine. But in some cases, like CI system running on the headless EC2 instance, we want to run it on the virtual display. This can be done using xvfb (X virtual framebuffer).\nSet up Install xvfb\nsudo apt-get install xvfb Install x11vnc\nsudo apt-get install x11vnc Run tests on virtual display Start xvbf (virtual display number 99)\nXvfb -ac :99 Tell tests to run on virtual display\nexport DISPLAY=:99 Or inside the test code (python)\nos.environ[\u0026#39;DISPLAY\u0026#39;] = \u0026#39;:99\u0026#39; ... selenium = webdriver.Firefox(firefox_profile=ffp, firefox_binary=ffb) ... Watch tests running on virtual display Start x11vnc server on the same display:\nx11vnc -display :99 Use vnc client (for example, gtkvncviewer) to connect to the localhost and watch how tests are running.\nLinks Stackoverflow\nXvfb + Firefox\nHudson Ci Server Running Selenium/Webdriver Cucumber In Headless Mode Xvfb\n","href":"/html/2012-02-20-selenium-run-on-virtual-display.html","title":"selenium - run tests on a virtual display"},{"content":"Git error:\nYour branch and 'origin/xxx' have diverged, and have 1 and 1 different commit(s) each, respectively. The error is caused by two independent commits - one (or more) on the local branch copy and other - on the remote branch copy (for example, commit by another person to the same branch).\nAnother case for the error is git rebase (error is expected, see below).\nThe history looks like this:\n... o ---- o ---- A ---- B origin/branch_xxx (upstream work) \\ C branch_xxx (your work) The solution depends on what has actually happened, the reason why the upstream state has changed. If someone else is also working on the same branch, the good way to solve it is to rebase the commit C on top of the remote state:\n$ git rebase origin/branch_xxx The history after the rebase will look like this:\n... o ---- o ---- A ---- B origin/branch_xxx (upstream work) \\ C` branch_xxx (your work) Another way to fix the issue is to merge the upstream branch state to local:\n$ git merge origin/branch_xxx The history after will look like this:\n... o ---- o ---- A ---- C ---- B --- [merge commit] The same git error after rebase: This happens if you rebase the branch which was previously pushed to the origin repository, for example, we start with a state like this:\n... o ---- o ---- A ---- B master, origin/master \\ C branch_xxx, origin/branch_xxx Now we want to rebase branch_xxx against the master branch:\ngit checkout master git pull git checkout branch_xxx git rebase master Now local state of the mybranch and origin/mybranch state will diverge. This is expected as rebase rewrites history, so after it you\u0026rsquo;ll have different local and remote state:\n... o ---- o ---- A ---------------------- B master, origin/master \\ \\ C origin/branch_xxx C` branch_xxx $ git status Alias tip: g status On branch branch Your branch and \u0026#39;origin/branch\u0026#39; have diverged, and have 1 and 1 different commits each, respectively. (use \u0026#34;git pull\u0026#34; to merge the remote branch into yours) If you absolutely sure this is your case then you can force Git to push your changes:\ngit push -f origin branch_xxx Links Should I rebase master onto a branch that\u0026rsquo;s been pushed?\nStackoverflow - master branch and \u0026lsquo;origin/master\u0026rsquo; have diverged, how to \u0026lsquo;undiverge\u0026rsquo; branches\u0026rsquo;?\nStackoverflow - git rebase basics\n","href":"/html/2012-02-10-git-checkout-and-track-remote-branch.html","title":"git - your branch and 'origin/xxx' have diverged error"},{"content":"Note: in recent git versions, it is enough to do git pull and git checkout feature to checkout and start tracking the remote branch.\nBefore, it was necessary to use the -t (--track) flag to do that:\n#creates and checks out \u0026#34;feature\u0026#34; branch that tracks \u0026#34;origin/feature\u0026#34; $ git checkout -t origin/feature Relevant part from the checkout docs:\ngit checkout [] \u0026hellip; If is not found but there does exist a tracking branch in exactly one remote (call it ) with a matching name and \u0026ndash;no-guess is not specified, treat as equivalent to $ git checkout -b \u0026ndash;track /\n","href":"/html/2012-01-24-jquery-check-version.html","title":"git - checkout and track remote branch"},{"content":"It seems that we have no perfect solution for class table inheritance (or multiple table inheritance) in yii (comparing to the very good one for single table inheritance).\nPossible solutions are:\nAdd support for class table inheritance to the active record class. There are some implementations of this method (see here and here for examples). But I do not like this approach because it is too complex to implement it properly and to make it work for all possible active record usages. Use MySQL VIEWs. MySQL supports a VIEW which can be updated by INSERT and UPDATE statements. So we can hide two tables (for base and child classes) behind the view and use it as a usual single table. Use single table inheritance and keep an extended data in separate tables. I wanted to examine options #2 and #3 and made a simple yii application.\nMySQL VIEWs Using MySQL views we can join two or more tables on the database level and work with join result as with single table. This approach could be a perfect solution, but unfortunately MySQL has some limitations related to views which join several tables.\nI examined these limitations in my test application (see \u0026ldquo;view\u0026rdquo; module) and found following issues:\nSome additional code needed to make active record work properly (it does not detects primary key automatically and does not set record id after insert). This issue is not critical and can be handled in active record subclass. MySQL does not allow to delete records using view. This issue also can be handled in active record subclass - we can emulate delete method and do actual delete from joined tables. MySQL does not allow to update both joined tables at once. This is very disappointing and breaks the idea of hiding two tables behind the view. Of cause some workarounds are possible, for example in my sample app I show only fields from the base table when creating a record and only fields from the extended table when updating. But actually this limitation makes the whole idea of using VIEWS for class table inheritance no so good. A controller code in this case can remain simple and similar to the single table case, but you anyway should remember that you use the view and not a single table and do special handling in the UI on in the controller when you create / update records. So views can not provide a good abstract level when you want to create and update records.\nMySQL VIEWS are good solution for list or grid views. The code in this case will be exactly the same as for the single table.\nSingle table inheritance and additional data in external tables This approach is implemented in the \u0026ldquo;aggregation\u0026rdquo; module of my test application. A data structure is following:\n-- single table inheritance table CREATE TABLE `car` ( `id` INT NOT NULL AUTO_INCREMENT , `name` VARCHAR(45) NULL , `type` ENUM(\u0026#39;Car\u0026#39;,\u0026#39;SportCar\u0026#39;,\u0026#39;FamilyCar\u0026#39;) NULL DEFAULT \u0026#39;Car\u0026#39; , PRIMARY KEY (`id`) ) ENGINE = InnoDB; -- additional data for a SportCar class CREATE TABLE `sport_car_data` ( `car_id` INT NOT NULL , `power` INT NOT NULL , PRIMARY KEY (`car_id`) , CONSTRAINT `fk_sport_car_data_car` FOREIGN KEY (`car_id` ) REFERENCES `car` (`id` )) ENGINE = InnoDB; -- additional data for a FamilyCar class CREATE TABLE `family_car_data` ( `car_id` INT NOT NULL , `seats` INT NOT NULL , PRIMARY KEY (`car_id`) , CONSTRAINT `fk_family_car_data_car` FOREIGN KEY (`car_id` ) REFERENCES `car` (`id` )) ENGINE = InnoDB; Here the \u0026lsquo;car\u0026rsquo; table is a single inheritance table with type field (ENUM with class names). Base class is \u0026lsquo;Car\u0026rsquo;:\nclass Car extends CActiveRecord { public function tableName() { return \u0026#39;car\u0026#39;; } protected function instantiate($attributes) { $class=$attributes[\u0026#39;type\u0026#39;]; $model=new $class(null); return $model; } public function beforeSave() { if ($this-\u0026gt;isNewRecord) { $this-\u0026gt;type = get_class($this); } return parent::beforeSave(); } } class FamilyCar extends Car { public function tableName() { return \u0026#39;car\u0026#39;; } public function relations() { return array( \u0026#39;data\u0026#39; =\u0026gt; array(self::HAS_ONE, \u0026#39;FamilyCarData\u0026#39;, \u0026#39;car_id\u0026#39;), ); } function defaultScope() { return array( \u0026#39;condition\u0026#39;=\u0026gt;\u0026#34;type=\u0026#39;FamilyCar\u0026#39;\u0026#34;, ); } } class SportCar extends Car { public function tableName() { return \u0026#39;car\u0026#39;; } public function relations() { return array( \u0026#39;data\u0026#39; =\u0026gt; array(self::HAS_ONE, \u0026#39;SportCarData\u0026#39;, \u0026#39;car_id\u0026#39;), ); } function defaultScope() { return array( \u0026#39;condition\u0026#39;=\u0026gt;\u0026#34;type=\u0026#39;SportCar\u0026#39;\u0026#34;, ); } } Some notes:\nClass name is used as value for \u0026ldquo;type\u0026rdquo; field, so there is no need in switch in the Car::instantiante method (like in the original solution). The Car::defaultScope() method is defined in the child classes only. This way we can use base Car class to process all models regardless of type and child classes FamilyCar and SportCar to work with one model type only. Child classes have ‘data’ relation which points to the model with extended type data. In this case FamilyCarData and SportCarData are such models with extended data. With this approach you should handle two models in controllers and views related to subclasses (FamilyCar and SportCar).\nFor example, create action in the FamilyCar controller:\npublic function actionCreate() { //create base model and model with extended data $model=new FamilyCar; $model-\u0026gt;data=new FamilyCarData; if(isset($_POST[\u0026#39;FamilyCarData\u0026#39;])) { //get properties for both models and save them //of cause it is better to use transaction here $model-\u0026gt;attributes=$_POST[\u0026#39;FamilyCar\u0026#39;]; $model-\u0026gt;data-\u0026gt;attributes=$_POST[\u0026#39;FamilyCarData\u0026#39;]; if($model-\u0026gt;save()) { $model-\u0026gt;data-\u0026gt;car_id = $model-\u0026gt;id; if ($model-\u0026gt;data-\u0026gt;save()) { $this-\u0026gt;redirect(array(\u0026#39;view\u0026#39;,\u0026#39;id\u0026#39;=\u0026gt;$model-\u0026gt;id)); } } } $this-\u0026gt;render(\u0026#39;create\u0026#39;,array( \u0026#39;model\u0026#39;=\u0026gt;$model, )); } And the view code will be like this:\n\u0026lt;div class=\u0026#34;form\u0026#34;\u0026gt; \u0026lt;?php $form=$this-\u0026gt;beginWidget(\u0026#39;CActiveForm\u0026#39;, array( \u0026#39;id\u0026#39;=\u0026gt;\u0026#39;family-car-form\u0026#39;, \u0026#39;enableAjaxValidation\u0026#39;=\u0026gt;false, )); ?\u0026gt; \u0026lt;p class=\u0026#34;note\u0026#34;\u0026gt;Fields with \u0026lt;span class=\u0026#34;required\u0026#34;\u0026gt;*\u0026lt;/span\u0026gt; are required.\u0026lt;/p\u0026gt; \u0026lt;?php echo $form-\u0026gt;errorSummary($model); ?\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;?php echo $form-\u0026gt;labelEx($model,\u0026#39;name\u0026#39;); ?\u0026gt; \u0026lt;?php echo $form-\u0026gt;textField($model,\u0026#39;name\u0026#39;); ?\u0026gt; \u0026lt;?php echo $form-\u0026gt;error($model,\u0026#39;name\u0026#39;); ?\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;?php echo $form-\u0026gt;labelEx($model-\u0026gt;data,\u0026#39;seats\u0026#39;); ?\u0026gt; \u0026lt;?php echo $form-\u0026gt;textField($model-\u0026gt;data,\u0026#39;seats\u0026#39;); ?\u0026gt; \u0026lt;?php echo $form-\u0026gt;error($model-\u0026gt;data,\u0026#39;seats\u0026#39;); ?\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;row buttons\u0026#34;\u0026gt; \u0026lt;?php echo CHtml::submitButton($model-\u0026gt;isNewRecord ? \u0026#39;Create\u0026#39; : \u0026#39;Save\u0026#39;); ?\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;?php $this-\u0026gt;endWidget(); ?\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;!-- form --\u0026gt; Here we can access extended data through relation: $model-\u0026gt;data.\nSummary It is possible to use MySQL views to implement class table inheritance, but I would not recommend this, because of complexities with create / update data code. Views are convenient when you list records and simplify configuration of CListView and CGRidView widgets.\nIn my opinion the solution with single table inheritance and extended data in separate tables is the best choice here. Yes, you will have to handle two models in your controllers / views and should re-organize your classes hierarchy, but you have a clean view of what is going on and the fact you actually use two database tables is explicit.\nThis approach can be combined with MySQL views which are good for list and grid views (but not for create / update forms).\nLinks Class table inheritance pattern\nSingle table inheritance implementation in yii\nMySQL - updatable views\nYii forum topic - class table inheritance\nYii forum topic - multiple table inheritance approach\nYii forum topic - inheritance and dynamic attributes\nYii forum topic - another inheritance approach\n","href":"/html/2012-02-01-yii-class-table-inheritance.html","title":"yii - class table inheritance"},{"content":"I have LG p500 (Optimus One), rooted with original ROM.\nAfter update to BusyBox 1.20.1 my moblie GPRS network became broken. I got network indicator (E with two arrows), but can not access network.\nI suspected that this was someting with dns settings, but did not find any solution. I uninstalled busybox, but this didn\u0026rsquo;t help. I did a factory reset - also didn\u0026rsquo;t help. Actually I thought factory reset wil return my device to the original state, but then noticed that I still have root and busybox.\nSo I started examining system logs and found this error:\nFailed to replace default route com.android.server.NativeDaemonConnectorException 06-12 17:04:40.999 E/NetworkManagmentService(1560)com.android.server.NetworkManagementService 994 Failed to replace default route com.android.server.NativeDaemonConnectorException: Cmd {route replace def v4 rmnet0 31.144.159.49} failed with code 400 : {failed to replace default route for [rmnet0 -4] (I/O error)}: for iface [rmnet0] 06-12 17:04:40.999 E/NetworkStateTracker(1560)android.net.NetworkStateTracker 249 Unable to add default route. I googled this error and found XDA thread which mention this error, page where it mentioned first - XDA forum thread, see post 4174.\nNote that I use original LG ROM (not the rom discussed in that thread). Few pages later there is a more detailed examination - see post 4272 and 4275.\nDuring discussion they suggested to try this command (I modified it a bit because original forum version gave error about wrong arguments. I issued this command in the Script Manager via su):\nsu -c \u0026#34;ip -4 route replace default via 109.46.130.66 dev rmnet0\u0026#34; Where ip (109.46.130.66) is ip address from the com.android.server.NativeDaemonConnectorException error above - I think this is dynamic dns server I got from my mobile network provider (different on each connection). And this command makes network work! But only for curren session. When I reconnect GPRS the network doesn\u0026rsquo;t work again (because new dns server ip is issued every time).\nThe final solution is to take an \u0026ldquo;ip\u0026rdquo; binary from the original rom and replace a busybox\u0026rsquo;s version:\nGet the stock rom Extract system files from kdz format, how to After extracting \u0026ldquo;system.mbn\u0026rdquo;: copy system/bin/ip to the device (somewhere, for example, sd card root). Rename /system/bin/ip to /system/bin/ip_busybox (just a backup). To access system folder on the device I used ES File Explorer with root mode enabled. Copy original \u0026ldquo;ip\u0026rdquo; (from sdcard root) to /system/bin/ Finally I want to thank BusyBox installer developer, Stephen. I contacted him by email when I realized that problem was with busybox installer and he answered and put effort to help me find the solution. Also he posted about this issue in the mentioned above XDA thread (I could not do this because have no permission to post into developer topics on XDA).\n","href":"/html/2012-04-24-git-fugitive-to-resolve-conflicts.html","title":"Andriod - moblie network problem after BusyBox update"},{"content":"","href":"/tags/android/","title":"android"},{"content":"To check whether jQuery is loaded to the page and verify minimum version:\nif (typeof jQuery == 'undefined' || !/[1-9]\\.[3-9].[1-9]/.test($.fn.jquery) ) { throw('jQuery version 1.3.1 or above is required'); } Here a regular expression determines a required jQuery version -\n/[X-9]\\.[Y-9].[Z-9]/ For example, for 1.3.1 use\n/[1-9]\\.[3-9].[1-9]/ and for 1.2.3 use\n/[1-9]\\.[2-9].[3-9]/ ","href":"/html/2012-01-24-jquery-check-version.md","title":"jQuery - check minimal required version"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/search/","title":"Search"},{"content":"Problem: after setting the boolean flags for Firefox profile the webdriver fails with \u0026ldquo;Can\u0026rsquo;t load the profile\u0026rdquo; error.\nI tried to disable native events for Firefox webdriver in a following way:\nffp = webdriver.firefox.firefox_profile.FirefoxProfile(path) if (Config.ff_native_events_enabled == False): ffp.native_events_enabled = False ffb = webdriver.firefox.firefox_binary.FirefoxBinary(firefox_path=Config.browser_binary) selenium = webdriver.Firefox(firefox_profile=ffp, firefox_binary=ffb) After that Firefox starts, but python code can not connect to the webdriver extension.\nTest fails with error like this:\nTraceback (most recent call last): ... File \u0026quot;/usr/local/lib/python2.6/dist-packages/selenium/webdriver/firefox/webdriver.py\u0026quot;, line 45, in __init__ self.binary, timeout), File \u0026quot;/usr/local/lib/python2.6/dist-packages/selenium/webdriver/firefox/extension_connection.py\u0026quot;, line 46, in __init__ self.binary.launch_browser(self.profile) File \u0026quot;/usr/local/lib/python2.6/dist-packages/selenium/webdriver/firefox/firefox_binary.py\u0026quot;, line 44, in launch_browser self._wait_until_connectable() File \u0026quot;/usr/local/lib/python2.6/dist-packages/selenium/webdriver/firefox/firefox_binary.py\u0026quot;, line 86, in _wait_until_connectable self.profile.path, self._get_firefox_output())) WebDriverException: Message: \u0026quot;Can't load the profile. Profile Dir: /tmp/tmp2aEvmI/webdriver-py-profilecopy Firefox output: \u0026quot; Actually this error is not related to the profile loading and it fails in _wait_until_connectable method.\nIf I comment out the ffp.native_events_enabled = False string then everything works.\nSolution After some examination I found that webdriver (python code) passes parameters to the fireforx extension through the user.js file in the Firefox profile folder. And it looked like this:\nuser_pref(\u0026#34;security.warn_entering_weak\u0026#34;, false); user_pref(\u0026#34;security.fileuri.strict_origin_policy\u0026#34;, false); user_pref(\u0026#34;webdriver_enable_native_events\u0026#34;, False); ... Note that here we have lowercase \u0026lsquo;false\u0026rsquo; for all options except the webdriver_enable_native_events. The python code which is responsible for this:\n# my code ffp.native_events_enabled = False # webdriver code @native_events_enabled.setter def native_events_enabled(self, value): self.default_preferences[\u0026#39;webdriver_enable_native_events\u0026#39;] = str(value) Here python boolean value is converted into the \u0026ldquo;False\u0026rdquo; string. The solution is to use in my code lowercase string instead of boolean:\n# my code ffp.native_events_enabled = \u0026#34;false\u0026#34; I also added a bug report and hope this will be fixed in selenium code. The same problem exist for other boolean setters, like accept_untrusted_certs.\nLinks bug report\n","href":"/html/2012-02-20-selenium-python-webdriver-not-safe-setters.html","title":"selenium - python Firefox webdriver - unsafe setters in firefox_profile.py"},{"content":"","href":"/series/","title":"Series"}]