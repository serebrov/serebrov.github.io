<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on vim, git, aws and other three-letter words</title><link>https://serebrov.github.io/</link><description>Recent content in Home on vim, git, aws and other three-letter words</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 05 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://serebrov.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Profile Vue CLI Service Build (actually webpack build)</title><link>https://serebrov.github.io/html/2024-04-05-profile-vue-cli-service-build.html</link><pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2024-04-05-profile-vue-cli-service-build.html</guid><description>How to Profile Vue CLI Service Build (webpack build) The vue cli build is managed by the build command of the @vue/cli-service package. The command is located in node_modules/@vue/cli-service/lib/commands/build/index.js.
Looking into the code, there is not too much to debug or profile here, it boils down to this:
// Compose the webpackConfig object // ... // Run webpack: return new Promise((resolve, reject) =&amp;gt; { webpack(webpackConfig, (err, stats) =&amp;gt; { ... }) }) So the actual problem is webpack build profiling.</description></item><item><title>Vue CLI Service Build Out of Memory Error</title><link>https://serebrov.github.io/html/2024-04-05-profile-vue-cli-service-build-out-of-memory.html</link><pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2024-04-05-profile-vue-cli-service-build-out-of-memory.html</guid><description>Solving the Vue app build out-of-memory error If you encounter an out-of-memory error during the frontend build, you can increase the memory limit for the node process by setting the NODE_OPTIONS environment variable:
export NODE_OPTIONS=--max_old_space_size=8192 npm run build -- --mode production Note that it should be enough to build the frontend application with the default memory limit. Increasing the limit is a temporary measure to be able to profile and debug the build process.</description></item><item><title>Tmux: how to run the same command in multiple panes</title><link>https://serebrov.github.io/html/2024-01-30-tmux-run-same-command-in-multiple-panes.html</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2024-01-30-tmux-run-same-command-in-multiple-panes.html</guid><description>Why run the same command in multiple tmux panes? Sometimes it can be convenient as a quick way to run some command in parallel on the server. We can still see the output, detach from tmux and disconnect and reconnect later and attach to the tmux session to see the result.
How to run the same command in multiple panes? To run the same command in multiple panes, we can use a script like this:</description></item><item><title>Touch Typing: Spacebar and Thumbs</title><link>https://serebrov.github.io/html/2024-01-18-touch-typing-spacebar-thumbs.html</link><pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2024-01-18-touch-typing-spacebar-thumbs.html</guid><description>Traditional keyboards have a large spacebar that is available for both thumbs. On split keyboards, I often see the spacebar key only on one side - left or right.
But what is better? Should the spacebar be pressed by the left or right thumb? Or is it better to alternate thumbs?
A collection of links to touch typing tutors and games.
I was reviewing my layout and I usually have two symmetrical spacebar keys:</description></item><item><title>Testing FastAPI CORS settings</title><link>https://serebrov.github.io/html/2023-12-16-testing-fastapi-cors-settings.html</link><pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-12-16-testing-fastapi-cors-settings.html</guid><description>Testing FastAPI CORS Settings This is an example of testing FastAPI app CORS settings to see how allow_origins (the Access-Control-Allow-Origin) and allow_credentials (the Access-Control-Allow-Credentials header) parameters work practically.
We wanted to make sure that we do not need the allow_credentials=True. From the docs it looks like this is the case when we want to send cookies or auth headers from the server (which we do not do), although in our case we send auth headers from the client (the API auth token).</description></item><item><title>Google Colab notebook - input and output, OpenAI TTS API</title><link>https://serebrov.github.io/html/2023-11-09-google-colab-input-and-output-openai-tts.html</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-11-09-google-colab-input-and-output-openai-tts.html</guid><description>Running OpenAI TTS API in Google Colab The example below runs OpenAI TTS API in a Google Colab notebook. The process involves some input and output:
Prompt the user for API key Run the API method Save result into a file I have a notebook with two cells.
First: Setup cell # @title Notebook Setup # @markdown Please, run this cell first. You&amp;#39;ll be prompted for your OpenAI API Keys. # @markdown https://platform.</description></item><item><title>Elastic Beanstalk - how to configure access to the external RDS database</title><link>https://serebrov.github.io/html/2023-10-28-elastic-beanstalk-rds-access.html</link><pubDate>Sat, 28 Oct 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-10-28-elastic-beanstalk-rds-access.html</guid><description>I used to configure ElasticBeanstalk access to the external RDS database by editing inbound rules for the security group attached to the database. This is inconvenient because there is always a risk of breaking something, especially if there are several environments accessing the database and we have multiple inbound rules.
A more convenient method is to use a &amp;ldquo;proxy&amp;rdquo; security group:
Create a new security group named rds-{database name}-access Add this group to the RDS security group inbound rules, allowing access to the DB port (such as 5432 for PostgreSQL) Add proxy group to the ElasticBeanstalk security groups The convenience is that we do not have to edit security groups anymore, we just add the &amp;ldquo;proxy&amp;rdquo; group in environment settings.</description></item><item><title>node - localhost connection error ECONNREFUSED ::1:4723 in node 17 and node 18</title><link>https://serebrov.github.io/html/2023-07-28-node-econnrefused-localhost.html</link><pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-07-28-node-econnrefused-localhost.html</guid><description>The upgrade to node 18 broke some things on CI that looked strange at first:
The npx wait-on checks started showing connection errors. The webdriver.io tests for native apps failed, also with connection errors. The errors look like this:
Unable to connect to &amp;ldquo;http://localhost:4723/&amp;rdquo;, make sure browser driver is running on that address. .. ERROR webdriver: RequestError: connect ECONNREFUSED ::1:4723
The localhost resolves to IPv6 address ::1 and the connection fails as the server (Appium) only runs on IPv4 address (127.</description></item><item><title>bash - how to run command in a loop until it fails</title><link>https://serebrov.github.io/html/2023-07-20-bash-how-to-run-command-until-it-fails.html</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-07-20-bash-how-to-run-command-until-it-fails.html</guid><description>I want to debug a flaky test and run it multiple times until the test runner returns non-zero code.
Here is how to do that with a one-liner:
# Put the command to run into a variable, for convenience CMD=&amp;#34;npm run test:unit -- tests/unit/MyTest.spec.ts -t \&amp;#34;&amp;#39;my test case&amp;#39;\&amp;#34;&amp;#34; # Run it multiple times until it fails cnt=1; while eval $CMD; do echo &amp;#34;Command succeeded, attempt $cnt&amp;#34;; ((cnt++)); done Alternatively, create a script to run the command, it might be more useful if there is a sequence of commands to run:</description></item><item><title>docker - check image size and see what takes space</title><link>https://serebrov.github.io/html/2023-06-19-docker-check-image-size.html</link><pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-06-19-docker-check-image-size.html</guid><description>There are three useful tools to check the Docker image size and see what takes space:
docker image ls - show images and sizes docker image history image:tag - show image layers and size for each layer dive - a tool to inspect the image and see what each layer adds to the image Check the image size with docker image ls:
docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE backend-ecs test-master 1b433074c182 59 minutes ago 2.</description></item><item><title>How to inspect extension `chrome.storage` in Chrome DevTools</title><link>https://serebrov.github.io/html/2023-06-13-chrome-extension-inspect-storage.html</link><pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-06-13-chrome-extension-inspect-storage.html</guid><description>The extension storage is not displayed under the &amp;ldquo;Application&amp;rdquo; tab in Chrome DevTools, but it is possible to access the extension storage using a javascript console, here is how:
it is possible to access it via the javascript console:
Open some web page, open Chrome DevTools In the javascript console, select the extension context (the drop-down with &amp;ldquo;top&amp;rdquo; in it) Use chrome.storage.local to access the local storage The chrome.storage.local is a StorageArea object and it has methods to get and set values.</description></item><item><title>git - use kdiff3 as a diff/merge tool</title><link>https://serebrov.github.io/html/2023-06-11-git-use-kdiff3-as-difftool.html</link><pubDate>Sun, 11 Jun 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-06-11-git-use-kdiff3-as-difftool.html</guid><description>To use kdiff3 as your diff tool and merge tool in git, run the following commands:
git config --global mergetool.kdiff3.cmd &amp;#39;kdiff3 &amp;#34;$BASE&amp;#34; &amp;#34;$LOCAL&amp;#34; &amp;#34;$REMOTE&amp;#34; -o &amp;#34;$MERGED&amp;#34;&amp;#39; git config --global merge.tool kdiff3 git config --global difftool.kdiff3.cmd &amp;#39;kdiff3 &amp;#34;$LOCAL&amp;#34; &amp;#34;$REMOTE&amp;#34;&amp;#39; git config --global diff.tool kdiff3 Alternatively, edit the ~/.gitconfig and add settings there:
[mergetool &amp;#34;kdiff3&amp;#34;] cmd = kdiff3 $LOCAL $REMOTE $BASE -o $MERGED [diff &amp;#34;kdiff3&amp;#34;] cmd = kdiff3 $LOCAL $REMOTE [merge] tool = kdiff3 [diff] tool = kdiff3</description></item><item><title>How to format large JSON file in command line</title><link>https://serebrov.github.io/html/2023-06-11-format-json-command-line.html</link><pubDate>Sun, 11 Jun 2023 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2023-06-11-format-json-command-line.html</guid><description>There are several tools that can be used to format the large json file.
Prettier (if you have node.js and npx installed):
npx run prettier input_json.json &amp;gt; formatted_json.json With python:
cat input_json.json | python -m json.tool &amp;gt; formatted_json.json # json.tool uses 4 spaces as indent by default, we can change it: cat input_json.json | python -m json.tool --indent 2 &amp;gt; formatted_json.json With jq:
jq &amp;#39;.&amp;#39; input_json.json &amp;gt; formatted_json.json Reminder: to open a large file in vim / nvim it is better to run it with -u NONE: nvim -u NONE formatted_json.</description></item><item><title>git - cherry-pick a range of commits</title><link>https://serebrov.github.io/html/2021-09-13-git-cherry-pick-a-range-of-commits.html</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2021-09-13-git-cherry-pick-a-range-of-commits.html</guid><description>To cherry pick a range of commits to another branch, we can use the START^..END commit range syntax, where START is the first commit in the range and END is the last commit:
git cherry-pick START^..END References How to cherry-pick a range of commits and merge them into another branch?</description></item><item><title>git - how to move files with history to another repository</title><link>https://serebrov.github.io/html/2021-09-13-git-move-history-to-another-repository.html</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2021-09-13-git-move-history-to-another-repository.html</guid><description>Git allows joining unrelated repositories via remotes which, in turn, allows moving files and change history between them.
Some cases when this might be needed:
Extract part of a big repository into a separate repository, preserving change history Splitting big repository to a set of smaller repositories Merge smaller repository into a bigger one (merge in library from the external repository) Preserving the history has an important effect: after we do the change, for example, extract a part of a bigger repository into a separate repository, we can continue moving changes between them (merge updates from the big repository to the small one and back).</description></item><item><title>git - show number of commits by author</title><link>https://serebrov.github.io/html/2021-09-13-git-number-of-commits-by-author.html</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2021-09-13-git-number-of-commits-by-author.html</guid><description>The shortlog -ns will show number of commits by author:
git shortlog -ns 280 Author One 46 Author Two 25 authorthree 14 au 4 x 3 Autor One 1 ide user x</description></item><item><title>git - update commit message</title><link>https://serebrov.github.io/html/2021-09-13-git-update-commit-message.html</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2021-09-13-git-update-commit-message.html</guid><description>Simple: Change Last Commit Message To change the last commit message, use commit with --amend flag:
Careful: `commit --amend` will rewirte history, do not use on public branches. $ git commit --amend It will open an editor and change the commit message, changes will be applied after saving the file and closing the editor.
Advanced: Change Any Commit Message or Multiple Commit Messages Besides other things, interactive rebase allows editing commit messages:</description></item><item><title>Mitosis Keyboard First Impressions</title><link>https://serebrov.github.io/html/2020-11-22-mitosis-keyboard-first-impressions.html</link><pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2020-11-22-mitosis-keyboard-first-impressions.html</guid><description>Mitosis Keyboard First Impressions
I&amp;rsquo;ve received my Mitosis a few days ago and I like it a lot so far:
The size: perfect for the 36 keys layout I use The shape: very comfortable, lowered outer columns feel great for pinkies I wished the inner column, for the index finger would not be shifted up as it is, it causes a bit of extension to use Y and T (not a big deal though) I also think the bottom thumb row is too far, so I am using the top row, having the bottom one without keycaps, so it doesn&amp;rsquo;t get in a way Wireless: probably this was one of main reasons to get it, it is really nice to freely move both halves without having to care about wires An interesting decision here is to use the radio channel instead of bluetooth which gives much longer battery lifetime It requires a receiver to be connected to the computer, but that is not a big deal, also it is cool to realize that your keyboard (QMK) is actually running inside that small board Layout: not sure which layout was there initially, it was QWERTY while the default layout in QMK configuratior is Maltron (I think), anyway, I changed it to my layout, similar to what I have on Moonlander Open source: here is the announcement post, hardware, receiver firmware and keyboard firmware, QMK How it looks:</description></item><item><title>Oculus Quest for Work: First Impressions</title><link>https://serebrov.github.io/html/2020-05-03-oculus-quest-for-work.html</link><pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2020-05-03-oculus-quest-for-work.html</guid><description>After reading this, this and this and this, I&amp;rsquo;ve got Oculus Quest willing to try it for work. I am a software developer and spend 8-10 hours per day before the laptop. My main machine at the moment is Mac Book Pro, I also own Thinkpad with Linux.
This way, in terms of software, ImmersedVR seems to be the only contender and only compatible headsets are Oculus Go and Oculus Quest.</description></item><item><title>Vue.js Cli: How to Use Multiple vue.config.js Configs</title><link>https://serebrov.github.io/html/2020-05-03-vue-cli-multiple-configs.html</link><pubDate>Sat, 02 May 2020 21:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2020-05-03-vue-cli-multiple-configs.html</guid><description>It can be useful to have more than one configuration file, for example, to build several code bundles.
The config file to use can be set with VUE_CLI_SERVICE_CONFIG_PATH environment variable:
# Build using vue.config.public.js CONF=`realpath vue.config.public.js` VUE_CLI_SERVICE_CONFIG_PATH=$CONF npm run build -- --mode production # build using vue.config.js npm run build -- --mode production Where npm run build is defined in package.json as vue-cli-service build.
It is also possible to create several bundles using vue cli multi-page mode, but in this case we will have big common js and css &amp;ldquo;vendors&amp;rdquo; package.</description></item><item><title>Touch Typing Tutors and Games</title><link>https://serebrov.github.io/html/2020-01-21-touch-typing.html</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2020-01-21-touch-typing.html</guid><description>A collection of links to touch typing tutors and games.
Online Typing Tutors keybr.com - Typing practice, supports Qwerty, Dvorak, Colemak and Workman layouts typing.io - Typing Practice for Programmers monkeytype.com - a minimalistic, customizable typing test, featuring many test modes, open source Programmer&amp;rsquo;s typing practice - no timers, no WPMs—just a never-ending stream of gibberish, open source speedtyper.dev - Typing competitions for programmers, code snippets from open source projects in different languages, open source typelit.</description></item><item><title>AWS Config - Unexpected Charges and Data Analysis</title><link>https://serebrov.github.io/html/2019-10-08-aws-config-charges.html</link><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2019-10-08-aws-config-charges.html</guid><description>I started seeing an increased charge in billing for AWS Config service in one of the accounts, it increased from around $5 to $100 per month. And I didn&amp;rsquo;t even remember if I enabled and configured it.
I could not get any details from AWS Cost Explorer besides that charges are in the same region where our app is running.
The confusing part was a note in the AWS Config management console:</description></item><item><title>Git Hook to Add Issue Number to Commit Message</title><link>https://serebrov.github.io/html/2019-06-16-git-hook-to-add-issue-number-to-commit-message.html</link><pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2019-06-16-git-hook-to-add-issue-number-to-commit-message.html</guid><description>When using project management system (Jira, Redmine, Github issues, etc) it is useful to add the issue number into commit message that makes is easier to understand which issue the commit belongs to and often allows the project management system to display related commits.
For same reasons, it is also useful to include the issue number into branch name, such as 123-branch-description or feature/PROJECT-123-branch-description.
The process of adding the issue number into commit message can be automated with git prepare-commit-msg hook (shell script).</description></item><item><title>Multi-Origin CloudFront Setup to Route Requests to Services Based on Request Path</title><link>https://serebrov.github.io/html/2019-06-16-multi-origin-cloudfront-setup.html</link><pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2019-06-16-multi-origin-cloudfront-setup.html</guid><description>AWS CloudFront allows to have multiple origins for the distribution and, along with lambda@edge functions, that makes it possible to use CloudFront as an entry point to route the requests to different services based on the request path.
For example:
www.myapp.com -&amp;gt; unbounce.com (landing pages) www.myapp.com/app -&amp;gt; single page app hosted on S3 www.myapp.com/blog -&amp;gt; wordpress blog CloudFront Setup Structure CloudFront distribution is an entry point which we assign a root domain name (www.</description></item><item><title>Managing NPM packages on github</title><link>https://serebrov.github.io/html/2019-02-16-manage-npm-packages-on-github.html</link><pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2019-02-16-manage-npm-packages-on-github.html</guid><description>Sometimes it is simpler to keep the package on github, for example, if you have a fork of a published package with some private changes. So you can avoid cluttering npm registry with similar packages, creating confusing for other people.
NPM supports installing dependencies from github, but it is also good to have versioning for your package so you can use it exactly as other packages, develop it independently and upgrade the dependency for the main project in a controlled way.</description></item><item><title>Recording Linux Terminal Session to GIF with asciinema</title><link>https://serebrov.github.io/html/2018-11-29-linux-terminal-screencast.html</link><pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-11-29-linux-terminal-screencast.html</guid><description>The asciinema is a good and simple to use tool to record a screencast from the terminal session.
And asciicast2gif allows to convert the recording to gif animation.
virtualenv -p python3 venv source venv/bin/activate pip install asciinema $ asciinema asciinema: recording asciicast to demo.cast asciinema: press &amp;lt;ctrl-d&amp;gt; or type &amp;#34;exit&amp;#34; when you&amp;#39;re done ... $ &amp;lt;ctrl-d&amp;gt; asciinema: recording finished asciinema: asciicast saved to demo.cast And convert it to gif:
docker run --rm -v $PWD:/data asciinema/asciicast2gif -t solarized-dark demo.</description></item><item><title>Debugging Python with ipdb and pdbpp</title><link>https://serebrov.github.io/html/2018-11-28-python-debugging-with-ipdb-pdbpp.html</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-11-28-python-debugging-with-ipdb-pdbpp.html</guid><description>To get a very convenient full-screen console debugger for python, install ipdb and pdbpp packages.
Then use __import__('ipdb').set_trace() to start the debugger and enter sticky to switch to the full-screen mode.
Both packages can be installed with pip:
virtualenv -p python3 venv source venv/bin/activate pip install ipdb pip install pdbpp The ipdb package improves the standard (pdb) debugger by adding syntax highlight and code completion. And the pdbpp adds the &amp;ldquo;sticky&amp;rdquo; mode, so the debugger can be run in a full-screen mode, in terminal:</description></item><item><title>Formatting Parameter Blocks in Python</title><link>https://serebrov.github.io/html/2018-07-27-python-parameter-blocks-formatting.html</link><pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-07-27-python-parameter-blocks-formatting.html</guid><description>There are two ways recommended in pep-8 to format the blocks with long parameter lists in Python:
# Arguments start on the next line foo = long_function_name( var_one, var_two, var_three, var_four) Another way is:
# Arguments start on the same line foo = long_function_name(var_one, var_two, var_three, var_four) I always prefer the first option and the other one is problematic for a few reasons.
For example, if you have two blocks with such indentation, the indent will be jumping:</description></item><item><title>Disqus - code formatting and highlighting in comments</title><link>https://serebrov.github.io/html/2018-01-19-disqus-code-in-comments.html</link><pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-01-19-disqus-code-in-comments.html</guid><description>It is possible to format and have syntax highlighting for code in Disqus comments. To do that, wrap the code into &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt; tags (see the example comment to this post).
I didn&amp;rsquo;t know about this feature and, acutally, I think this is an UI flaw.
It would be great to see the formatting help link or popup when you are editing the comment (and also the preview feature would be really nice to have).</description></item><item><title>There Is No Callback Hell In JavaScript</title><link>https://serebrov.github.io/html/2018-01-19-there-is-no-callback-hell.html</link><pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-01-19-there-is-no-callback-hell.html</guid><description>There is no &amp;ldquo;callback hell&amp;rdquo; in Javascript, it is just a bad programming style. The infamous JavaScript &amp;ldquo;callback hell&amp;rdquo; can easily be fixed by un-nesting all the callbacks into separate functions.
Here is an example:
const verifyUser = function(username, password, callback) { dataBase.verifyUser(username, password, function(error, userInfo) { if (error) { callback(error); } else { dataBase.getRoles(username, function(error, roles) { if (error) { callback(error); } else { dataBase.logAccess(username, function(error) { if (error) { callback(error); } else { callback(null, userInfo, roles); } }) } }) } }) }; The same code with separate functions instead of inline callbacks:</description></item><item><title>SSH Tunnels (How to Access AWS RDS Locally Without Exposing it to Internet)</title><link>https://serebrov.github.io/html/2018-01-11-aws-ec2-ssh-tunnel.html</link><pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2018-01-11-aws-ec2-ssh-tunnel.html</guid><description>Using SSH tunnels, it is possible to access remote resources that are not exposed to the Internet through the intermediate hosts or expose your local services to the Internet.
Setup To make SSH commands shorter and easier to use, edit the ~/.ssh/config and add the configuration for the hosts you are going to connect.
The configuration defines default ssh options, so instead of the command like this ssh ec2-user@ec2-55-222-55-55.compute-1.amazonaws.com -i ~/.</description></item><item><title>AWS error - Default subnet in us-east-1f not found</title><link>https://serebrov.github.io/html/2017-06-28-aws-default-subnet-in-us-east-1f-not-found.html</link><pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2017-06-28-aws-default-subnet-in-us-east-1f-not-found.html</guid><description>I suddenly started getting the Default subnet in us-east-1f not found error during the ElasticBeanstalk environment update.
Failed to deploy application. Updating load balancer named: awseb-e-t-AWSEBLoa-XXXXXXXXXXXXX failed Reason: Default subnet not found in us-east-1f Service:AmazonCloudFormation, Message:Stack named &amp;#39;awseb-e-xxxxxxxxxx-stack&amp;#39; aborted operation. Current state: &amp;#39;UPDATE_ROLLBACK_IN_PROGRESS&amp;#39; Reason: The following resource(s) failed to create: [AWSEBUpdateWaitConditionHandleralanC]. The following resource(s) failed to update: [AWSEBLoadBalancer]. And the similar one when trying to create the new environment:
Creating load balancer failed Reason: Default subnet in us-east-1f not found Created CloudWatch alarm named: awseb-e-tet63me2mx-stack-AWSEBCWLAllErrorsCountAlarm-3XCPMJ1ZGJ18 Stack named &amp;#39;awseb-e-tet63me2mx-stack&amp;#39; aborted operation.</description></item><item><title>Simple Git Workflow</title><link>https://serebrov.github.io/html/2016-07-03-simple-git-workflow.html</link><pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2016-07-03-simple-git-workflow.html</guid><description>The main purpose of this workflow is to have a reliable, but simple to use git workflow. It is simple enough to be used by git beginners and minimizes possibility of mistakes (comparing to advanced flows which use rebase and related git features to achieve clean history).
The main idea of this workflow is that we create a new branch for every task and one developer works on this branch until the task is finished.</description></item><item><title>Setup Automatic Deployment, Updates and Backups of Multiple Web Applications with Docker on the Scaleway Server</title><link>https://serebrov.github.io/html/2016-06-10-scaleway-docker-deployment.html</link><pubDate>Fri, 10 Jun 2016 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2016-06-10-scaleway-docker-deployment.html</guid><description>The purpose of this setup is:
Setup multiple web apps with different dependencies on the same server Link all apps to the same MySQL server Manage uploaded files for web apps in the single place (so it is easy to backup them) Automatically deploy and update apps on the remote server Run the same setup locally, so development environment is very close to production Setup backups for MySQL databases and for uploaded files In this case I deploy to Scaleway, but same approach can be used for almost any cloud service.</description></item><item><title>OOP SOLID Principles "L" - Liskov Substitution Principle</title><link>https://serebrov.github.io/html/2016-02-18-oop-solid-l-liskov-substitution-principle.html</link><pubDate>Thu, 18 Feb 2016 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2016-02-18-oop-solid-l-liskov-substitution-principle.html</guid><description>According to the Wikipedia the Liskov Substitution Principle (LSP) is defined as:
Subtype Requirement: Let f(x) be a property provable about objects x of type T. Then f(y) should be true for objects y of type S where S is a subtype of T. The basic idea - if you have an object of type T then you can also use objects of its subclasses instead of it.
Or, in other words: the subclass should behave the same way as the base class.</description></item><item><title>AWS PostgreSQL RDS - remaining connection slots are reserved error</title><link>https://serebrov.github.io/html/2015-09-22-aws-postgresql-max-connections.html</link><pubDate>Tue, 22 Sep 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-09-22-aws-postgresql-max-connections.html</guid><description>Today I had a problem with PostgreSQL connection, both my application and psql tool returned an error:
FATAL: remaining connection slots are reserved for non-replication superuser connections The PostgreSQL server was running on the db.t1.micro RDS instance and the &amp;lsquo;Current activity&amp;rsquo; column showed &amp;lsquo;22 connections&amp;rsquo; and a red line which should represent a connection limit was far away from the 22 value.
Here is how it looked:
.
And this connection information is actually misleading - it shows 22 connections and it looks like around 30% consumed.</description></item><item><title>How to set up Drone CI on EC2 instance via Elastic Beanstalk</title><link>https://serebrov.github.io/html/2015-07-05-elastic-beanstalk-drone-ci-setup.html</link><pubDate>Sun, 05 Jul 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-07-05-elastic-beanstalk-drone-ci-setup.html</guid><description>Drone CI is a Continuous Integration platform. It uses Docker containers to run tests for your application hosted on github.
It not complex to set up the automatic testing for your application and run Drone CI on EC2 instance using Elastic Beanstalk. It is even not necessary to have a dedicated EC2 instance for CI system, for example, I run it on the staging server.
Drone CI setup First you&amp;rsquo;ll need to create a drone configuration file, .</description></item><item><title>CloudWatch Logs - how to log data from multiple instances to the single stream</title><link>https://serebrov.github.io/html/2015-06-17-cloudwatch-logs-single-stream.html</link><pubDate>Wed, 20 May 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-06-17-cloudwatch-logs-single-stream.html</guid><description>After using CloudWatch Logs for some time I found that it is very inconvenient to have one stream per instance. The Logs UI is really complex to use - I need to remember instance names, open the log group I need and then go into each instance logs one-by-one to check them.
A more convenient alternative is to use one stream like error_log for all instances.
Update: logging to the same stream from multiple sources is not recommended and may cause duplicate records (although in my case this is fine).</description></item><item><title>Elastic Beanstalk - how to setup CloudWatch Logs</title><link>https://serebrov.github.io/html/2015-05-20-cloudwatch-setup.html</link><pubDate>Wed, 20 May 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-05-20-cloudwatch-setup.html</guid><description>CloudWatch Logs is an AWS service to collect and monitor system and application logs. On the top level setup is this:
install CloudWatch agent to collect logs data and send to CloudWatch Logs service define log metric filters to extract useful data, like number of all errors or information about some specific events create alarms for metrics to get notifications about logs make sure that the instance role has permissions to push logs to CloudWatch (see comments for details about this issue) All the configuration can be done using the Elastic Beanstalk config.</description></item><item><title>Elastic Beanstalk - python application server structure and celery installation</title><link>https://serebrov.github.io/html/2015-04-02-elastic-beanstalk-python.html</link><pubDate>Thu, 02 Apr 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-04-02-elastic-beanstalk-python.html</guid><description>Elastic beanstalk python application is deployed under /opt/python/. The application is running under Apache web server.
Source folder structure is this:
bin httpdlaunch - a tool script to set environment variables and launch httpd bundle - dir with app source code, used during updates current - symlink to the recent source code version under bundle app - application sources env - shell script with environment variables (passed from EB environment settings) etc - supervisord config log - supervisord logs run - virtual environments Apache logs, deployment logs and system messages log are under /var/log.</description></item><item><title>Amazon DynamoDB - how to add global secondary index</title><link>https://serebrov.github.io/html/2015-01-25-aws-add-secondary-index.html</link><pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-01-25-aws-add-secondary-index.html</guid><description>Note: this post is outdated, because it is already possible to add a secondary index to the existing table (it was not possible in earlier DynamoDB versions).
At the moment it is not possible to add a secondary index into the existing table. This feature is announced but not yet available.
So the only way is to create a new table and migrate the existing data to it. This can be done using Amazon EMR.</description></item><item><title>Local Amazon DynamoDB - tools, dump/restore and testing</title><link>https://serebrov.github.io/html/2015-02-01-dynamodb-local.html</link><pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-02-01-dynamodb-local.html</guid><description>Setup Download and extract dynamodb local to some folder.
Launch it (-sharedDb allows us to connect to the same database with other tools):
$ java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb By default it will be running on the port 8000 and will create the db file in the same directory where it was launched.
Without the -sharedDB parameter the DB file name depends on connection parameters, the name is {aws_access_key_id}_{region_name}.db. So different clients can use different databases.</description></item><item><title>Amazon DynamoDB, EMR and Hive notes</title><link>https://serebrov.github.io/html/2015-01-24-aws-dynamodb-emr-hive.html</link><pubDate>Sat, 24 Jan 2015 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2015-01-24-aws-dynamodb-emr-hive.html</guid><description>First you need the EMR cluster running and you should have ssh connection to the master instance like described in the getting started tutorial.
Now it is possible to run Hive commands in few following ways:
Connect via ssh, launch hive and run commands interactively Create a script file with commands, upload it to S3 and launch as a ERM &amp;lsquo;Hive program&amp;rsquo; step Run it from Hue web-interface (see below) Connect to Hue Hue is a Hadoop web interface.</description></item><item><title>AWS - Deployment via OpsWorks from the command line</title><link>https://serebrov.github.io/html/2014-12-30-aws-opsworks-cmd-deployment.html</link><pubDate>Tue, 30 Dec 2014 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2014-12-30-aws-opsworks-cmd-deployment.html</guid><description>Below is a simple python script which performs application deployment using OpsWorks API library (boto). Script performs following steps
Execute &amp;lsquo;update_custom_cookbooks&amp;rsquo; deployment command and wait for successful completion (or stop with an error) Execute &amp;lsquo;deploy&amp;rsquo; command and wait for completion At the top there are aws configuration parameters (aws_access_key, aws_secret_key) - these can be left empty if the script is launched on the AWS instance which has IAM role assigned.</description></item><item><title>AWS OpsWorks - setup mongodb ebs volume backups</title><link>https://serebrov.github.io/html/2014-12-30-aws-ebs-mongo-backups.html</link><pubDate>Tue, 30 Dec 2014 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2014-12-30-aws-ebs-mongo-backups.html</guid><description>I described how to setup mongodb on EC2 using OpsWorks and here is how to setup mongo data backups.
In my case all mongo data is stored on the same EBS volume so I just need to make a volume snapshot.
The relevant part from the mongodb docs:
Backup with --journal The journal file allows for roll forward recovery. The journal files are located in the dbpath directory so will be snapshotted at the same time as the database files.</description></item><item><title>Amazon OpsWorks - node.js app with MongoDB setup</title><link>https://serebrov.github.io/html/2014-12-19-aws-opsworks-mongo-and-nodejs.html</link><pubDate>Fri, 19 Dec 2014 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2014-12-19-aws-opsworks-mongo-and-nodejs.html</guid><description>Amazon OpsWorks provides a way to manage AWS resources using Chef recipes.
Here I describe a simple setup of the single-instance node.js app with single-node MongoDB server. It is similar to the php application + mysql setup described in the OpsWorks Getting Started guide.
The OpsWorks setup includes:
Stack - a container for the deployment process we will setup Two Layers - node.js app and MongoDB Two Instances - one EC2 instance for node app and another for mongo One Application - this is the code we will deploy MongoDb setup is based on this blog post.</description></item><item><title>git - how to revert multiple recent commits</title><link>https://serebrov.github.io/html/2014-01-04-git-revert-multiple-recent-comments.html</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2014-01-04-git-revert-multiple-recent-comments.html</guid><description>Let&amp;rsquo;s assume we have a history like this:
G1 - G2 - G3 - B1 - B2 - B3 Where G1-G3 are &amp;lsquo;good&amp;rsquo; commits and B1-B3 are &amp;lsquo;bad&amp;rsquo; commits and we want to revert them.
Here is the shell script to create the revision history like above, you can use it to try and see the effect of different commands.
git reset The first method is a simple way to throw away few recent commits.</description></item><item><title>How to keep `git log` and `git diff` output on the screen after exit</title><link>https://serebrov.github.io/html/2014-01-04-git-log-and-less-keep-output.html</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2014-01-04-git-log-and-less-keep-output.html</guid><description>When git uses less as pager the output of commands like git log disappears from the console screen when you exit from less.
This is not convenient in many cases so here is how to fix this.
Just for git commands:
git config --global --replace-all core.pager &amp;#34;less -iXFR&amp;#34; For less command, globally (including git) - add to your shell profile (.bashrc, .zshrc, etc):
export LESS=-iXFR Options we set for less:</description></item><item><title>Node.js - how to debug mocha test with node inspector</title><link>https://serebrov.github.io/html/2013-12-02-node-debug-mocha.html</link><pubDate>Mon, 02 Dec 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-12-02-node-debug-mocha.html</guid><description>To debug mocha test with node inspector use the delay before test:
beforeEach(function(done) { //start mocha as //mocha -t 10000 --debug setTimeout(function() { done(); }, 5000); }); This way there are 5 seconds to start the node inspector and set a breakpoint. Mocha should be lauched as this:
$ mocha -t 10000 --debug Same approach can be used not only for tests but for any short-living node app - just wrap the startup code into the setTimeout() call.</description></item><item><title>Node.js - how to get core module source</title><link>https://serebrov.github.io/html/2013-12-02-node-core-module-source.html</link><pubDate>Mon, 02 Dec 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-12-02-node-core-module-source.html</guid><description>It is possible to ask node to show its core module source.
For example, we want to check the source of the readFileSunc() method:
$ node &amp;gt; fs = require('fs'); &amp;gt; fs.writeFileSync('fs.js', fs.toString()) &amp;gt; fs.writeFileSync('fs.readFileSync.js', fs.readFileSync.toString()) [Ctrl-C][Ctrl-C] Now check the fs.readFileSync.js file in the current folder.
Also on some systems source code of core node modules is in the /usr/lib/nodejs/.
And another (less interesting) way to get core module source - is to look for it in the node github repository:</description></item><item><title>Elastic Beanstalk - cron command and RDS DB access</title><link>https://serebrov.github.io/html/2013-10-22-elastic-beanstalk-cron-and-db.html</link><pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-10-22-elastic-beanstalk-cron-and-db.html</guid><description>Problem I have a console command in php which needs an access to DB. The command need to be launched via cron.
The DB connection string looks like like this
'connectionString' =&amp;gt; 'mysql:host='.$_SERVER['RDS_HOSTNAME'].';port='.$_SERVER['RDS_PORT'].';dbname='.$_SERVER['RDS_DB_NAME'], where RDS_xxx parameters come from environment variables.
The problem is that cron launches the command with a clean environment (there are no RDS_xx variables). So the command fails to access the database.
Solution Solution is to set the required environment variables before launching the command and this can be done with &amp;lsquo;/opt/elasticbeanstal/support/envvars&amp;rsquo; script:</description></item><item><title>Elastic Beanstalk - deploy from different machines / by different users (or how to get rid of absolute paths in configs)</title><link>https://serebrov.github.io/html/2013-09-11-elastic-beanstalk-configs.html</link><pubDate>Wed, 11 Sep 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-09-11-elastic-beanstalk-configs.html</guid><description>By default Elastic Beanstalk console tool (eb) adds config files to .gitignore. If there are manual changes to EB configs it can be complex to manually sync these changes between different machines / different users. Of cause it is possible to add config files to git repository but there are also several parameters in the main config which are absolute paths to local files. This way it makes configs not useful for other users (except for the case when different users have exactly the same files layout).</description></item><item><title>How to setup git server on ubuntu with push email notifications</title><link>https://serebrov.github.io/html/2013-06-14-git-server-setup.html</link><pubDate>Fri, 14 Jun 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-06-14-git-server-setup.html</guid><description>Git Server Prerequisites are git and ssh-server (apt-get install openssh-server).
The installation process is described in the Pro Git book. Below is the setup process with some comments and updates.
Add git user, set some password (you will be asked for it):
$ sudo adduser git Log in as git user and setup authorized ssh keys:
$ su git git@localname$ cd ~ git@localname$ mkdir .ssh For each user who need an access to the server add user&amp;rsquo;s public key into ~/.</description></item><item><title>JS libraties for charts (links)</title><link>https://serebrov.github.io/html/2013-06-14-js-libraries-for-charts.html</link><pubDate>Fri, 14 Jun 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-06-14-js-libraries-for-charts.html</guid><description>D3.js D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.
Browser support: IE9+, For IE8 compatibility it is recommended to use Aight. See also tutorials and re-usable charts for d3.</description></item><item><title>Amazon NoSQL Solutions</title><link>https://serebrov.github.io/html/2013-06-11-amazon-nosql-review.html</link><pubDate>Tue, 11 Jun 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-06-11-amazon-nosql-review.html</guid><description>Amazon provides following NoSQL storage options:
[SimpleDB] (http://aws.amazon.com/simpledb/) - Amazon SimpleDB is a highly available and flexible non-relational data store that offloads the work of database administration. Developers simply store and query data items via web services requests and Amazon SimpleDB does the rest. [DynamoDB] (http://aws.amazon.com/dynamodb/) - Amazon DynamoDB is a fully-managed, high performance, NoSQL database service that is easy to set up, operate, and scale. Amazon&amp;rsquo;s review of big data solutions Amazon&amp;rsquo;s review Big Data on AWS mentions only DynamoDB and Elastic Map Reduce (based on Hadoop) as tools for big data management.</description></item><item><title>Angular.js and SEO - pre-render content on the server</title><link>https://serebrov.github.io/html/2013-05-24-angular-seo.html</link><pubDate>Fri, 24 May 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-05-24-angular-seo.html</guid><description>With angular.js you have an HTML which looks like this:
&amp;lt;span&amp;gt;{{variableValue}}&amp;lt;/span&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li ng-repeat=&amp;#34;item in items&amp;#34; ng-bind=&amp;#34;item.name&amp;#34;&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; The simple way to make this content SEO-friendly is to pre-render data on the server and then allow angular to do it&amp;rsquo;s job on the client.
For simple variables there is ng-bind. And for lists there is ng-include. Here is the example from above with pre-rendered content:
&amp;lt;span ng-bind=&amp;#34;variableValue&amp;#34;&amp;gt;Static indexed value&amp;lt;/span&amp;gt; &amp;lt;ul ng-include=&amp;#34;&amp;#39;your/dynamic/list&amp;#39;&amp;#34;&amp;gt; &amp;lt;li&amp;gt;seo-friendly item1&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;seo-friendly item2&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;script type=&amp;#34;text/ng-template&amp;#34; id=&amp;#34;your/dynamic/list&amp;#34;&amp;gt; &amp;lt;li ng-repeat=&amp;#34;item in items&amp;#34; ng-bind=&amp;#34;item.</description></item><item><title>PHP - utf-8 strings handling</title><link>https://serebrov.github.io/html/2013-03-23-php-utf-8-strings.html</link><pubDate>Sat, 23 Mar 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-03-23-php-utf-8-strings.html</guid><description>Enable mbstring function overloading mode and set default encoding for string functions to utf-8 in php.ini:
mbstring.internal_encoding = UTF-8 mbstring.func_overload = 7 These settings allow us to use &amp;ldquo;usual&amp;rdquo; php string functions like substr() for utf-8 strings. It is not recommended to set function overloading in per-directory context (via Apache config or in the .htaccess).
Default encoding can also be set using mb_internal_encoding function:
mb_internal_encoding('UTF-8'); Or encoding can be set explicitly as argument in mbstring function:</description></item><item><title>Want Scalable Application Architecture? Check AngularJS.</title><link>https://serebrov.github.io/html/2013-03-18-js-scalable-architecture.html</link><pubDate>Mon, 18 Mar 2013 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-03-18-js-scalable-architecture.html</guid><description>The Scalable JavaScript Application Architecture is a presentation by Nicholas Zakas where he suggests a flexible and scalable architecture for JavaScript applications. Here are other related resources:
Presentation summary Presentation slides Patterns For Large-Scale JavaScript Application Architecture by Addy Osmani The presentation is interesting but it also leaves many open questions. In short, the architecture contains following application layers:
base library (jquery, etc) application core: manages modules (register modules, tell when to start and when to stop) handle errors (like wrap all modules&amp;rsquo; methods into try/catch and log errors) enable inter-module communication should be extensible (error handling, ajax wrapper, general utilites, anything!</description></item><item><title>Window managers for Google Chrome</title><link>https://serebrov.github.io/html/2012-12-09-google-chrome-window-manager.html</link><pubDate>Sun, 09 Dec 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-12-09-google-chrome-window-manager.html</guid><description>A list of extensions to manage position of chrome tabs / windows and to create splits. Descriptions are original texts from extension pages.
Tabs Outliner Next Generation Session/Windows/Tabs Manager and Too_Many_Open_Tabs Solution That Really Works.
Ultimate overview for all of your open, and not so open (read further), browser windows and tabs, in a resizable vertical side view, with a Tree Style Tab feature. A KILLER FEATURE – Has the unique ability to close and preserve “in place” any tab or window, without removing them from the original context in the tree.</description></item><item><title>git - find not merged branches</title><link>https://serebrov.github.io/html/2012-10-01-git-find-not-merged-branches.html</link><pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-10-01-git-find-not-merged-branches.html</guid><description>Branches that were not merged with master:
git branch -a --no-merged (or git branch -a --no-merged master) Branches that were merged with feature:
git branch -a --merged feature Branches that were merged not with feature:
git branch -a --no-merged feature Reference The git-branch(1) command:
with --contains flag shows only the branches that contain the named commit (in other words, the branches whose tip commits are descendants of the named commit). with --merged shows only branches merged into the named commit (i.</description></item><item><title>git - viewing changes - diff and log</title><link>https://serebrov.github.io/html/2012-10-01-git-diff-and-log.html</link><pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-10-01-git-diff-and-log.html</guid><description>Diff and log between two branches (all changes on both branches) To see all changes between two branches, use diff with two or three dots and log with three dots between branch names.
Note: the confusing part is that log with two dots only shows changes on one branch, while diff with two dots includes changes on both branches (showing changes from one branch as removed and from another branch - as added).</description></item><item><title>git - find all branches where file was changed</title><link>https://serebrov.github.io/html/2012-09-30-git-all-branches-with-file.html</link><pubDate>Sun, 30 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-30-git-all-branches-with-file.html</guid><description>Solution from stackoverflow.
Find all branches which contain a change to FILENAME (even if before the (non-recorded) branch point):
git log --all --format=%H FILENAME | while read f; do git branch --contains $f; done | sort -u Manually inspect:
gitk --all --date-order -- FILENAME Find all changes to FILENAME not merged to master:
git for-each-ref --format=&amp;#34;%(refname:short)&amp;#34; refs/heads | grep -v master | while read br; do git cherry master $br | while read x h; do if [ &amp;#34;`git log -n 1 --format=%H $h -- FILENAME`&amp;#34; = &amp;#34;$h&amp;#34; ]; then echo $br; fi; done; done | sort -u</description></item><item><title>git - Your branch is ahead of 'origin/master' by 1 commit after pull</title><link>https://serebrov.github.io/html/2012-09-30-git-branch-ahead-after-pull.html</link><pubDate>Sun, 30 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-30-git-branch-ahead-after-pull.html</guid><description>Your branch is ahead of &amp;lsquo;origin/master&amp;rsquo; by 1 commit (or X commits) after git pull origin master.
The sequence:
Have up-to-date repository There is a change in the origin/master Do git pull origin master Change is received and merged git status shows “Your branch is ahead of &amp;lsquo;origin/master&amp;rsquo; by 1 commit.” The reason is because during pull origin master reference to the remote origin/master is not changed (still points to older version).</description></item><item><title>Yii and jquery.localtime.js - display dates in user local timezone</title><link>https://serebrov.github.io/html/2012-09-29-yii-jquery-localtime.html</link><pubDate>Sat, 29 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-29-yii-jquery-localtime.html</guid><description>With this method we work on the server with UTC timezone dates and convert them to a user local timezone on client.
Use ‘TIMESTAMP’ type for date/datetime DB fields Setup MySQL and PHP timezone Set UTC timezone for both MySQL and PHP.
Yii db config (MySQL):
‘db’ =&amp;gt; array( &amp;#39;connectionString&amp;#39; =&amp;gt; &amp;#39;...’, &amp;#39;initSQLs&amp;#39;=&amp;gt;&amp;#34;set time_zone=&amp;#39;+00:00’;&amp;#34;, ); And PHP timezone:
date_default_timezone_set(‘UTC’); Generate HTML with UTC dates in ISO 8601 See date formats here Helper functions to convert dates:</description></item><item><title>global ajax response handler and jquery.localtime plugin</title><link>https://serebrov.github.io/html/2012-09-25-global-ajax-handler-and-localtime.html</link><pubDate>Tue, 25 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-25-global-ajax-handler-and-localtime.html</guid><description>The jquery.localtime plugin allows to convert date/time strings to a local user time on a client site. By default it works when the page is loaded initially, but if some elements are updated via ajax then they do not converted and left in an UTC format.
Possible solution is to add some special handling to $.ajax &amp;lsquo;success&amp;rsquo; handlers, but it can require a lot of modifications. Better way is to set some global handler for all ajax requests and apply conversion to local time there.</description></item><item><title>innodb notes</title><link>https://serebrov.github.io/html/2012-09-24-innodb.html</link><pubDate>Mon, 24 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-24-innodb.html</guid><description>Below are some notes regarding the innodb transactions and locks. Most of them are just a copy-paste from innodb docs with my notes. Also there are some examples of custom-maid deadlocks.
In the examples of MySQL operations different sessions are marked as (1) and (2), for example:
create table child (id int(11) NOT NULL) ENGINE=InnoDB; common operation, do it from any session (1) start transaction; do it in session #1 (2) start transaction; do it in session #2 To get two (or more) sessions just launch several instances of mysql in different console windows.</description></item><item><title>git - submodule helpers</title><link>https://serebrov.github.io/html/2012-06-15-android-mobile-network-problem.html</link><pubDate>Sun, 23 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-06-15-android-mobile-network-problem.html</guid><description>Below are some git commands which can be useful to resolve problems with submodules.
Get a list of commits inside a submodule Submodules are identified by SHA-1 hashes so you may need to get a list of them. Do the following inside the submodule folder (or inside the separate submodule repository):
git log --oneline 5bd722f commit 5 b5c1524 commit 4 2444bfa commit 3 e0eadd5 commit 2 c180c5a commit 1 702fc8a commit 0 View current submodule commit git submodule status 5bd722fa26dcdd64128392aa28e08849fe37f111 sub (heads/master) Compare a submodule state with another branch Assume we are on the &amp;ldquo;branch1&amp;rdquo; and want to compare &amp;lsquo;sub&amp;rsquo; submodule state with master:</description></item><item><title>Yii url rules - optional parameters</title><link>https://serebrov.github.io/html/2012-09-13-yii-url-rules-optional-parameters.html</link><pubDate>Thu, 13 Sep 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-09-13-yii-url-rules-optional-parameters.html</guid><description>Assume we have an action &amp;ldquo;articles/get&amp;rdquo; which accepts optional parameters and we want to setup following URLs:
articles/[article id or name] articles/[article id or name]/draft articles/[article id or name]/revisions/99 articles/[article id or name]/revisions/98/draft articles/revisions/[revision id] articles/revisions/[revision id]/draft We have a list of articles and each article has several revisions. Also each revision can have draft and published version.
In the code we have a single &amp;lsquo;article/get&amp;rsquo; action which allows us to get specific article (last revision) by name (&amp;lsquo;GET articles/my-article&amp;rsquo;) or id (&amp;lsquo;GET articles/33&amp;rsquo;).</description></item><item><title>express.js and ejs - reuse template on server and client</title><link>https://serebrov.github.io/html/2012-08-20-expressjs-ejs-reuse-templates.html</link><pubDate>Mon, 20 Aug 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-08-20-expressjs-ejs-reuse-templates.html</guid><description>ejs has a client-side support but documentation and examples do not describe how to reuse the same template on the server and on the client side.
For now I found two ways to do it. First way is to send a request from the client, get a template from a file and render it. And the second - put a template into the page when render it on the server and then just use the template on the client.</description></item><item><title>selenium - problem with big file upload</title><link>https://serebrov.github.io/html/2012-08-20-selenium-problem-with-big-file-upload.html</link><pubDate>Mon, 20 Aug 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-08-20-selenium-problem-with-big-file-upload.html</guid><description>Problem Selenium 2.25.0, python 2.7, ubuntu 12.04, Firebox 4.0 (yes, old version but we need it) hangs when uploading a file larger then (about) 600KB.
Solution For now I fixed this by commenting out local file upload (three first lines):
def send_keys(self, *value): &amp;#34;&amp;#34;&amp;#34;Simulates typing into the element.&amp;#34;&amp;#34;&amp;#34; #local_file = LocalFileDetector.is_local_file(*value) #if local_file is not None: #value = self._upload(local_file) typing = [] for val in value: if isinstance(val, Keys): typing.append(val) elif isinstance(val, int): val = str(val) for i in range(len(val)): typing.</description></item><item><title>selenium webdriver - get webelement by jQuery selector</title><link>https://serebrov.github.io/html/2012-08-02-selenium-webdriver-element-by-jquery-selector.html</link><pubDate>Thu, 02 Aug 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-08-02-selenium-webdriver-element-by-jquery-selector.html</guid><description>This can be necessary for example for selector like #id &amp;gt; li:visible.
If you will try to do webdriver.find_element_by_css_selector you will get an error message &amp;ldquo;The given selector #id &amp;gt; li:visible is either invalid or does not result in a WebElement.&amp;rdquo;
The workaround is to use jQuery to find element. It can be done with this code (python):
script = &amp;quot;return $('&amp;quot;+selector+&amp;quot;').get(0);&amp;quot; element = webdriver.execute_script(script);</description></item><item><title>selenium webdriver - set php session cookie</title><link>https://serebrov.github.io/html/2012-07-24-selenium-webdriver-set-php-session-cookie.html</link><pubDate>Tue, 24 Jul 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-07-24-selenium-webdriver-set-php-session-cookie.html</guid><description>To set the php session cookie we can use the addcookie (or python version add_cookie) method of the webdriver. But it accepts only name and value and does not allow to set additional cookie parameters like domain, path, etc.
Fortunately it is easy to do with javascript.
Here is example of a JS code to set the cookie:
document.cookie = &amp;quot;PHPSESSID=9ojofgkb21nujvhulvgq4drh06; domain=.myhost.com; path=/&amp;quot;; And here is python code version (assume you have set &amp;lsquo;cookie&amp;rsquo; and &amp;lsquo;domain&amp;rsquo; variables:</description></item><item><title>selenium webdriver - trigger event on element via jQuery</title><link>https://serebrov.github.io/html/2012-07-24-selenium-webdriver-trigger-event-via-jquery.html</link><pubDate>Tue, 24 Jul 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-07-24-selenium-webdriver-trigger-event-via-jquery.html</guid><description>The &amp;rsquo;executeScript&amp;rsquo; method of the webdriver receives additional &amp;lsquo;arguments&amp;rsquo; variable and we can pass WebElement instances to the script. So trigger an event on the elemen can be done like this (python):
event = 'click' #or 'hover' or any other script = &amp;quot;$(arguments[0]).trigger('&amp;quot;+event+&amp;quot;')&amp;quot; webdriver.execute_script(script, web_element) Links Stackoverflow question</description></item><item><title>git - use vim with fugitive to resolve merge conflicts</title><link>https://serebrov.github.io/html/2012-04-24-git-fugitive-to-resolve-conflicts.html</link><pubDate>Tue, 24 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-04-24-git-fugitive-to-resolve-conflicts.html</guid><description>To use fugitive as a mergetool you can run the following commands:
git config --global mergetool.fugitive.cmd &amp;#39;vim -f -c &amp;#34;Gdiff&amp;#34; &amp;#34;$MERGED&amp;#34;&amp;#39; git config --global merge.tool fugitive Links stackoverflow fugitive screencast</description></item><item><title>Angular.js POST data to PHP</title><link>https://serebrov.github.io/html/2013-05-24-angular-post-to-php.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2013-05-24-angular-post-to-php.html</guid><description>By default angular.js sends all data in json. So if you do a POST request to a PHP code then the $_POST superglobal will not be populated.
This can be solved in two ways - on the client side or on the server side.
Server-side solution On the server you can parse input and then decode data from json:
$data = file_get_contents(&amp;#34;php://input&amp;#34;); $postData = json_decode($data); Client-side solution On the client side the data can be sent in a way PHP expects it:</description></item><item><title>GImport yii extension</title><link>https://serebrov.github.io/html/2012-10-10-gimport.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-10-10-gimport.html</guid><description>GImport yii extension implements recursive import of directories with caching.
Import is performed recursively for specified path alias. Classes found are cached, so the import process can be slow only first time.
Basic usage example:
$importer = new GImport; $importer-&amp;gt;add(&amp;#39;modules.myModule.*&amp;#39;); This code will import all clasees from modules/myModule/ directory.
GImport can also be configured as application component. Add following code into the application config:
return array( ... &amp;#39;preload&amp;#39; =&amp;gt; array(&amp;#39;log&amp;#39;, &amp;#39;import&amp;#39;), .</description></item><item><title>git - remove already deleted files</title><link>https://serebrov.github.io/html/2012-10-01-git-rm-deleted-files.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-10-01-git-rm-deleted-files.html</guid><description>To remove files that were already deleted on the file system, run git add with -u flag:
git status D abc.c git add -u This tells git to automatically stage tracked files - including deleting the previously tracked files.
Stackoverflow: How do I commit all deleted files in Git?</description></item><item><title>git - rename branch (local and remote)</title><link>https://serebrov.github.io/html/2012-03-15-oauth-1-0.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-03-15-oauth-1-0.html</guid><description>Rename local branch:
git branch -m old-branch-name new-branch-name Rename remote branch:
#delete remote branch with old name git push origin :old-branch-name # create remote renamed branch git push origin new-branch-name Links stackoverflow</description></item><item><title>PHP - friend a class via extend</title><link>https://serebrov.github.io/html/2012-12-09-php-friend-via-extend.md</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-12-09-php-friend-via-extend.md</guid><description>C++ allows to declare one class as a friend of another one.
This can be useful if you want to keep some details of class protected, but available for another particular (friend) class.
For example this can be used in State pattern to keep setState method of context class protected.
To emulate this in PHP we can inherit state class from context class:
class AContext { private $_state; protected function setState(AState $state) { $this-&amp;gt;_state = $state; } public function request() { $this-&amp;gt;_state-&amp;gt;handle(); } } abstract class AState extends AContext { private $_owner; public function __construct(AContext $owner) { $this-&amp;gt;_owner = $owner; } protected function getOwner() { return $this-&amp;gt;_owner; } abstract function handle(); } class AConcreteState extends AState { public function handle() { .</description></item><item><title>phpmyadmin and eaccelerator problem</title><link>https://serebrov.github.io/html/2012-10-09-phpmyadmin-and-eaccelerator-problem.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-10-09-phpmyadmin-and-eaccelerator-problem.html</guid><description>Error when trying to access phpmyadmin (in Chrome):
Error 324 (net::ERR_EMPTY_RESPONSE): The server closed the connection without sending any data. The easiest way to fix I found is to disable eaccelerator in .htaccess (create it in the phpmyadmin root folder and add this line:
php_flag eaccelerator.enable 0</description></item><item><title>Speedup unit tests by moving MySql data to memory [Ubuntu]</title><link>https://serebrov.github.io/html/2012-12-17-unit-speed-mysql-to-mem.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-12-17-unit-speed-mysql-to-mem.html</guid><description>There are several ways to speedup slow unit tests which interact with database:
Refactor code and tests and do not interact with db in unit tests Use sqlite db in memory instead of MySql Use MySql MEMORY engine Move MySql data to memory It is better to try other listed approaches and I think of last method as of quick temporary hack, but here it is:
stop mysql move /var/lib/mysql to /dev/shm/mysql link /var/lib/mysql to /dev/shm/mysql start mysql In Ubuntu there is also a problem with apparmor which will not allow mysql to read from /dev/shm.</description></item><item><title>vim - replace a word with yanked text multiple times</title><link>https://serebrov.github.io/html/2012-04-03-vim-replace-word-with-yanked-text.html</link><pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-04-03-vim-replace-word-with-yanked-text.html</guid><description>It is often useful to copy, let&amp;rsquo;s say new_name, to clipboard and then replace one or more occurrences of old_name with it. We can use :%s/new_name/old_name/g for that, but sometimes it is more convenient to go over the text and do replacements &amp;ldquo;manually&amp;rdquo;.
A possible way to do that in vim is:
Yank inner word: yiw (the new_name is now in the default register &amp;quot;&amp;quot;) Move cursor to another word (the old_name) Select it and paste over: viwp (viw - visual inner word, p - paste) What happens next might be unexpected in this scenario: when we pasted over the old_name, it was deleted and put into the default Vim register instead of new_name, so next time we use p, it will paste the old_name (tldr: the xnoremap p pgvy mapping helps fix this).</description></item><item><title>yii - catch and log MySQL deadlock errors</title><link>https://serebrov.github.io/html/2012-03-28-yii-catch-and-log-deadlocks.html</link><pubDate>Wed, 28 Mar 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-03-28-yii-catch-and-log-deadlocks.html</guid><description>This method allows to log InnoDB monitor output when deadlock error occured. This way we will have much more useful data to find and fix deadlock.
Extend error handler class:
class AppErrorHandler extends CErrorHandler { protected function handleException($exception) { /* CDbCommand failed to execute the SQL statement: SQLSTATE[40001]: * Serialization failure: 1213 Deadlock found when trying to get lock; * try restarting transaction. The SQL statement executed was: * INSERT INTO `table_name` (`id`, `name`) VALUES (:yp0, :yp1) */ //can we check $exception-&amp;gt;getCode() ?</description></item><item><title>oauth 1.0 notes</title><link>https://serebrov.github.io/html/2012-02-28-git-default-color-diff.html</link><pubDate>Thu, 15 Mar 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-28-git-default-color-diff.html</guid><description>oAuth 1.0 flow A good explanation image from oauth.net:
Flow description:
Consumer has Consumer Key and Consumer Secret (shared secret) A) Consumer requests Request Token call get_request_token from Service Provider, send oauth_consumer_key oauth_signature_method oauth_signature &amp;hellip; here oauth_signature - is signature of the request created using Consumer Secret, simplified example: $signature = md5($request_text . $consumer_secret) both sides (Consumer and Service Provider) knows consumer_secret and able to perform this operation, so Service Provider can check whether signature is valid B) Service provider returns Request Token oauth_token oauth_token_secret C) Consumer redirects User to Service Provider oauth_token (request token from B) D) User confirms access and Service Provider redirects User to Consumer oauth_token (request token from B) oauth_verifier (request token verifier) E) Consumer requests Access Token call get_access_token, send oauth_consumer_key oauth_token (request token from B) oauth_signature_method oauth_signature &amp;hellip; oauth_verifier here oauth_signature - is signature of the request created using request token secret from B note, that on step A Consumer uses his Consumer Secret to sign the request and here he use request token secret F) Service provider grants Access Token oauth_token oauth_token_secret G) Consumer Accesses Protected Resources request includes oauth_consumer_key oauth_token (request token from F) oauth_signature_method oauth_signature &amp;hellip; here oauth_signature created using Access Token secret Links wikipedia</description></item><item><title>git - colored diff, branch, etc output by default</title><link>https://serebrov.github.io/html/2012-02-13-git-branches-have-diverged.html</link><pubDate>Tue, 28 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-13-git-branches-have-diverged.html</guid><description>To have colored git commands output, use the following command:
git config color.ui true or (globally)
git config --global color.ui true Alternatively, you can set color for individual git commands.
git config color.branch auto git config color.diff auto git config color.interactive auto git config color.status auto See also color.* options in the git config docs.
Links git book
Color highlighted diffs with git, svn and cvs</description></item><item><title>selenium - problem with loading x_ignore_nofocus.so</title><link>https://serebrov.github.io/html/2012-02-20-selenium-64bit-x_ignore_nofocus-problem.html</link><pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-20-selenium-64bit-x_ignore_nofocus-problem.html</guid><description>Problem: selenium fails to start Firefox with following error:
'The browser appears to have exited before we could connect. The output was: Failed to dlopen /usr/lib/libX11.so.6\ndlerror says: /usr/lib/libX11.so.6: wrong ELF class: ELFCLASS32\n' In my case it was reproduced on the 64 bit machine with Amazon Linux AMI. The problem itself is known and there is an issue in selenium tracker.
It is because x_ignore_nofocus library tries to load 32bit version of the libX11 instead of 64bit.</description></item><item><title>selenium - run tests on a virtual display</title><link>https://serebrov.github.io/html/2012-02-20-selenium-run-on-virtual-display.html</link><pubDate>Mon, 20 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-20-selenium-run-on-virtual-display.html</guid><description>Selenium tests require browser to run, so usually we run them on the X-server enabled machine. But in some cases, like CI system running on the headless EC2 instance, we want to run it on the virtual display. This can be done using xvfb (X virtual framebuffer).
Set up Install xvfb
sudo apt-get install xvfb Install x11vnc
sudo apt-get install x11vnc Run tests on virtual display Start xvbf (virtual display number 99)</description></item><item><title>git - your branch and 'origin/xxx' have diverged error</title><link>https://serebrov.github.io/html/2012-02-10-git-checkout-and-track-remote-branch.html</link><pubDate>Mon, 13 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-10-git-checkout-and-track-remote-branch.html</guid><description>Git error:
Your branch and 'origin/xxx' have diverged, and have 1 and 1 different commit(s) each, respectively. The error is caused by two independent commits - one (or more) on the local branch copy and other - on the remote branch copy (for example, commit by another person to the same branch).
Another case for the error is git rebase (error is expected, see below).
The history looks like this:</description></item><item><title>git - checkout and track remote branch</title><link>https://serebrov.github.io/html/2012-01-24-jquery-check-version.html</link><pubDate>Wed, 01 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-01-24-jquery-check-version.html</guid><description>Note: in recent git versions, it is enough to do git pull and git checkout feature to checkout and start tracking the remote branch.
Before, it was necessary to use the -t (--track) flag to do that:
#creates and checks out &amp;#34;feature&amp;#34; branch that tracks &amp;#34;origin/feature&amp;#34; $ git checkout -t origin/feature Relevant part from the checkout docs:
git checkout [] &amp;hellip; If is not found but there does exist a tracking branch in exactly one remote (call it ) with a matching name and &amp;ndash;no-guess is not specified, treat as equivalent to $ git checkout -b &amp;ndash;track /</description></item><item><title>yii - class table inheritance</title><link>https://serebrov.github.io/html/2012-02-01-yii-class-table-inheritance.html</link><pubDate>Wed, 01 Feb 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-01-yii-class-table-inheritance.html</guid><description>It seems that we have no perfect solution for class table inheritance (or multiple table inheritance) in yii (comparing to the very good one for single table inheritance).
Possible solutions are:
Add support for class table inheritance to the active record class. There are some implementations of this method (see here and here for examples). But I do not like this approach because it is too complex to implement it properly and to make it work for all possible active record usages.</description></item><item><title>Andriod - moblie network problem after BusyBox update</title><link>https://serebrov.github.io/html/2012-04-24-git-fugitive-to-resolve-conflicts.html</link><pubDate>Tue, 24 Jan 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-04-24-git-fugitive-to-resolve-conflicts.html</guid><description>I have LG p500 (Optimus One), rooted with original ROM.
After update to BusyBox 1.20.1 my moblie GPRS network became broken. I got network indicator (E with two arrows), but can not access network.
I suspected that this was someting with dns settings, but did not find any solution. I uninstalled busybox, but this didn&amp;rsquo;t help. I did a factory reset - also didn&amp;rsquo;t help. Actually I thought factory reset wil return my device to the original state, but then noticed that I still have root and busybox.</description></item><item><title>jQuery - check minimal required version</title><link>https://serebrov.github.io/html/2012-01-24-jquery-check-version.md</link><pubDate>Tue, 24 Jan 2012 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-01-24-jquery-check-version.md</guid><description>To check whether jQuery is loaded to the page and verify minimum version:
if (typeof jQuery == 'undefined' || !/[1-9]\.[3-9].[1-9]/.test($.fn.jquery) ) { throw('jQuery version 1.3.1 or above is required'); } Here a regular expression determines a required jQuery version -
/[X-9]\.[Y-9].[Z-9]/ For example, for 1.3.1 use
/[1-9]\.[3-9].[1-9]/ and for 1.2.3 use
/[1-9]\.[2-9].[3-9]/</description></item><item><title>Search</title><link>https://serebrov.github.io/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/search/</guid><description/></item><item><title>selenium - python Firefox webdriver - unsafe setters in firefox_profile.py</title><link>https://serebrov.github.io/html/2012-02-20-selenium-python-webdriver-not-safe-setters.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://serebrov.github.io/html/2012-02-20-selenium-python-webdriver-not-safe-setters.html</guid><description>Problem: after setting the boolean flags for Firefox profile the webdriver fails with &amp;ldquo;Can&amp;rsquo;t load the profile&amp;rdquo; error.
I tried to disable native events for Firefox webdriver in a following way:
ffp = webdriver.firefox.firefox_profile.FirefoxProfile(path) if (Config.ff_native_events_enabled == False): ffp.native_events_enabled = False ffb = webdriver.firefox.firefox_binary.FirefoxBinary(firefox_path=Config.browser_binary) selenium = webdriver.Firefox(firefox_profile=ffp, firefox_binary=ffb) After that Firefox starts, but python code can not connect to the webdriver extension.
Test fails with error like this:
Traceback (most recent call last): .</description></item></channel></rss>